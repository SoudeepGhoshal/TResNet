{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SoudeepGhoshal/TResNet/blob/main/TResNet_SVHN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Street View House Numbers (SVHN)"
      ],
      "metadata": {
        "id": "sNRQSZpDoyuU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ResNet-18"
      ],
      "metadata": {
        "id": "NyhudVhDo1q-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UZeC9wM6oflE",
        "outputId": "f9e5504a-2f80-472c-90dd-ea9cb746319a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Reproducibility seed: 42\n",
            "Downloading train data from http://ufldl.stanford.edu/housenumbers/train_32x32.mat...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "SVHN train: 100%|██████████| 182M/182M [00:17<00:00, 10.6MiB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train data downloaded successfully.\n",
            "Downloading test data from http://ufldl.stanford.edu/housenumbers/test_32x32.mat...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "SVHN test: 100%|██████████| 64.3M/64.3M [00:11<00:00, 5.71MiB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test data downloaded successfully.\n",
            "Downloading extra data from http://ufldl.stanford.edu/housenumbers/extra_32x32.mat...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "SVHN extra: 100%|██████████| 1.33G/1.33G [01:56<00:00, 11.4MiB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "extra data downloaded successfully.\n",
            "Loaded 73257 samples\n",
            "Data shape: (73257, 32, 32, 3)\n",
            "Labels range: 0 to 9\n",
            "Label distribution: [ 4948 13861 10585  8497  7458  6882  5727  5595  5045  4659]\n",
            "Loaded 26032 samples\n",
            "Data shape: (26032, 32, 32, 3)\n",
            "Labels range: 0 to 9\n",
            "Label distribution: [1744 5099 4149 2882 2523 2384 1977 2019 1660 1595]\n",
            "Loaded 531131 samples\n",
            "Data shape: (531131, 32, 32, 3)\n",
            "Labels range: 0 to 9\n",
            "Label distribution: [45550 90560 74740 60765 50633 53490 41582 43997 35358 34456]\n",
            "Total merged dataset size: 630420\n",
            "  - Train: 73257 samples\n",
            "  - Test: 26032 samples\n",
            "  - Extra: 531131 samples\n",
            "Dataset splits - Train: 441294, Val: 94563, Test: 94563, Total: 630420\n",
            "\n",
            "--- Starting Training ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100 | Time: 172.97s | Train Loss: 0.2288, Train Acc: 93.05% | Val Loss: 0.1659, Val Acc: 95.15%\n",
            "\n",
            "Model saved to ./best_svhn_model.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/100 | Time: 172.71s | Train Loss: 0.1314, Train Acc: 96.18% | Val Loss: 0.1353, Val Acc: 96.13%\n",
            "\n",
            "Model saved to ./best_svhn_model.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/100 | Time: 171.57s | Train Loss: 0.1141, Train Acc: 96.74% | Val Loss: 0.1158, Val Acc: 96.67%\n",
            "\n",
            "Model saved to ./best_svhn_model.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4/100 | Time: 172.03s | Train Loss: 0.1028, Train Acc: 97.09% | Val Loss: 0.1133, Val Acc: 96.81%\n",
            "\n",
            "Model saved to ./best_svhn_model.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5/100 | Time: 171.13s | Train Loss: 0.0972, Train Acc: 97.25% | Val Loss: 0.1023, Val Acc: 97.11%\n",
            "\n",
            "Model saved to ./best_svhn_model.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6/100 | Time: 170.02s | Train Loss: 0.0921, Train Acc: 97.40% | Val Loss: 0.0959, Val Acc: 97.31%\n",
            "\n",
            "Model saved to ./best_svhn_model.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7/100 | Time: 171.49s | Train Loss: 0.0882, Train Acc: 97.50% | Val Loss: 0.0960, Val Acc: 97.31%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8/100 | Time: 171.16s | Train Loss: 0.0847, Train Acc: 97.64% | Val Loss: 0.0950, Val Acc: 97.35%\n",
            "\n",
            "Model saved to ./best_svhn_model.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9/100 | Time: 178.10s | Train Loss: 0.0833, Train Acc: 97.67% | Val Loss: 0.1015, Val Acc: 97.16%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10/100 | Time: 174.80s | Train Loss: 0.0818, Train Acc: 97.70% | Val Loss: 0.0937, Val Acc: 97.37%\n",
            "\n",
            "Model saved to ./best_svhn_model.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 11/100 | Time: 174.80s | Train Loss: 0.0800, Train Acc: 97.77% | Val Loss: 0.0924, Val Acc: 97.40%\n",
            "\n",
            "Model saved to ./best_svhn_model.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 12/100 | Time: 175.49s | Train Loss: 0.0783, Train Acc: 97.82% | Val Loss: 0.0929, Val Acc: 97.46%\n",
            "\n",
            "Model saved to ./best_svhn_model.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 13/100 | Time: 175.60s | Train Loss: 0.0775, Train Acc: 97.85% | Val Loss: 0.0957, Val Acc: 97.37%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 14/100 | Time: 174.58s | Train Loss: 0.0765, Train Acc: 97.86% | Val Loss: 0.0962, Val Acc: 97.38%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 15/100 | Time: 174.35s | Train Loss: 0.0756, Train Acc: 97.88% | Val Loss: 0.0927, Val Acc: 97.45%\n",
            "\n",
            "Reducing learning rate from 1.00e-03 to 2.00e-04\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 16/100 | Time: 173.00s | Train Loss: 0.0473, Train Acc: 98.78% | Val Loss: 0.0739, Val Acc: 98.08%\n",
            "\n",
            "Model saved to ./best_svhn_model.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 17/100 | Time: 173.56s | Train Loss: 0.0378, Train Acc: 99.09% | Val Loss: 0.0775, Val Acc: 98.01%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 18/100 | Time: 173.51s | Train Loss: 0.0325, Train Acc: 99.23% | Val Loss: 0.0816, Val Acc: 97.97%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 19/100 | Time: 173.29s | Train Loss: 0.0287, Train Acc: 99.32% | Val Loss: 0.0839, Val Acc: 98.01%\n",
            "\n",
            "Reducing learning rate from 2.00e-04 to 4.00e-05\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 20/100 | Time: 172.81s | Train Loss: 0.0192, Train Acc: 99.62% | Val Loss: 0.0859, Val Acc: 98.05%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 21/100 | Time: 173.24s | Train Loss: 0.0161, Train Acc: 99.70% | Val Loss: 0.0908, Val Acc: 98.02%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 22/100 | Time: 173.78s | Train Loss: 0.0140, Train Acc: 99.75% | Val Loss: 0.0945, Val Acc: 98.00%\n",
            "\n",
            "Reducing learning rate from 4.00e-05 to 8.00e-06\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 23/100 | Time: 172.52s | Train Loss: 0.0115, Train Acc: 99.81% | Val Loss: 0.0947, Val Acc: 98.02%\n",
            "\n",
            "Early stopping at epoch 23\n",
            "Restored model weights from the end of the best epoch: 16\n",
            "--- Training Finished ---\n",
            "\n",
            "--- Starting Comprehensive Evaluation on Test Set ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating: 100%|██████████| 739/739 [00:25<00:00, 29.45it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Top-K Accuracy Results ---\n",
            "Top-1 Accuracy: 97.98%\n",
            "Top-3 Accuracy: 99.42%\n",
            "Top-5 Accuracy: 99.70%\n",
            "\n",
            "--- Additional Performance Metrics ---\n",
            "Macro Average Precision: 0.9791\n",
            "Macro Average Recall: 0.9787\n",
            "Macro Average F1-Score: 0.9789\n",
            "Weighted Average Precision: 0.9798\n",
            "Weighted Average Recall: 0.9798\n",
            "Weighted Average F1-Score: 0.9798\n",
            "\n",
            "--- Model Confidence & Calibration Metrics ---\n",
            "Average Prediction Confidence: 0.9911\n",
            "Confidence Standard Deviation: 0.0520\n",
            "Average Prediction Entropy: 0.0306\n",
            "Expected Calibration Error: 0.0113\n",
            "\n",
            "Final Test Loss: 0.0937\n",
            "===============================================\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "import torchvision.transforms as transforms\n",
        "import scipy.io\n",
        "from PIL import Image\n",
        "import os\n",
        "import requests\n",
        "import zipfile\n",
        "import shutil\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "import sys\n",
        "import numpy as np\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "\n",
        "# --- Configuration Parameters ---\n",
        "CONFIG = {\n",
        "    \"data_path\": \"./svhn-data/\",\n",
        "    \"batch_size\": 128,\n",
        "    \"num_epochs\": 100,\n",
        "    \"learning_rate\": 0.001,\n",
        "    \"weight_decay\": 1e-4,\n",
        "    \"split_seed\": 42,\n",
        "    \"train_split\": 0.70,\n",
        "    \"val_split\": 0.15,\n",
        "    \"test_split\": 0.15,\n",
        "    \"num_classes\": 10,    # SVHN has 10 classes (digits 0-9)\n",
        "    \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
        "    \"checkpoint_path\": \"./best_svhn_model.pth\",\n",
        "}\n",
        "\n",
        "# ==========================================\n",
        "# Callback Classes\n",
        "# ==========================================\n",
        "\n",
        "class Callback:\n",
        "    \"\"\"Base callback class\"\"\"\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        pass\n",
        "\n",
        "    def on_training_end(self):\n",
        "        pass\n",
        "\n",
        "class ReduceLROnPlateau(Callback):\n",
        "    \"\"\"Reduce learning rate when a metric has stopped improving\"\"\"\n",
        "    def __init__(self, optimizer, monitor='val_accuracy', factor=0.2, patience=3, min_lr=1e-7, verbose=1):\n",
        "        self.optimizer = optimizer\n",
        "        self.monitor = monitor\n",
        "        self.factor = factor\n",
        "        self.patience = patience\n",
        "        self.min_lr = min_lr\n",
        "        self.verbose = verbose\n",
        "        self.wait = 0\n",
        "        self.best = None\n",
        "        self.mode = 'min' if 'loss' in monitor else 'max'\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        if logs is None:\n",
        "            return\n",
        "\n",
        "        current = logs.get(self.monitor)\n",
        "        if current is None:\n",
        "            return\n",
        "\n",
        "        if self.best is None:\n",
        "            self.best = current\n",
        "            return\n",
        "\n",
        "        if self.mode == 'min':\n",
        "            improved = current < self.best\n",
        "        else:\n",
        "            improved = current > self.best\n",
        "\n",
        "        if improved:\n",
        "            self.best = current\n",
        "            self.wait = 0\n",
        "        else:\n",
        "            self.wait += 1\n",
        "            if self.wait >= self.patience:\n",
        "                old_lr = self.optimizer.param_groups[0]['lr']\n",
        "                new_lr = max(old_lr * self.factor, self.min_lr)\n",
        "                if new_lr != old_lr:\n",
        "                    self.optimizer.param_groups[0]['lr'] = new_lr\n",
        "                    if self.verbose:\n",
        "                        print(f\"\\nReducing learning rate from {old_lr:.2e} to {new_lr:.2e}\")\n",
        "                    self.wait = 0\n",
        "\n",
        "class EarlyStopping(Callback):\n",
        "    \"\"\"Stop training when a monitored metric has stopped improving\"\"\"\n",
        "    def __init__(self, monitor='val_accuracy', patience=7, restore_best_weights=True, verbose=1):\n",
        "        self.monitor = monitor\n",
        "        self.patience = patience\n",
        "        self.restore_best_weights = restore_best_weights\n",
        "        self.verbose = verbose\n",
        "        self.wait = 0\n",
        "        self.best = None\n",
        "        self.best_weights = None\n",
        "        self.mode = 'min' if 'loss' in monitor else 'max'\n",
        "        self.stopped_epoch = 0\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None, model=None):\n",
        "        if logs is None:\n",
        "            return False\n",
        "\n",
        "        current = logs.get(self.monitor)\n",
        "        if current is None:\n",
        "            return False\n",
        "\n",
        "        if self.best is None:\n",
        "            self.best = current\n",
        "            if model is not None and self.restore_best_weights:\n",
        "                self.best_weights = model.state_dict().copy()\n",
        "            return False\n",
        "\n",
        "        if self.mode == 'min':\n",
        "            improved = current < self.best\n",
        "        else:\n",
        "            improved = current > self.best\n",
        "\n",
        "        if improved:\n",
        "            self.best = current\n",
        "            self.wait = 0\n",
        "            if model is not None and self.restore_best_weights:\n",
        "                self.best_weights = model.state_dict().copy()\n",
        "        else:\n",
        "            self.wait += 1\n",
        "            if self.wait >= self.patience:\n",
        "                self.stopped_epoch = epoch\n",
        "                if self.verbose:\n",
        "                    print(f\"\\nEarly stopping at epoch {epoch + 1}\")\n",
        "                return True\n",
        "        return False\n",
        "\n",
        "    def on_training_end(self, model=None):\n",
        "        if self.stopped_epoch > 0 and self.verbose:\n",
        "            print(f\"Restored model weights from the end of the best epoch: {self.stopped_epoch + 1 - self.patience}\")\n",
        "        if model is not None and self.restore_best_weights and self.best_weights is not None:\n",
        "            model.load_state_dict(self.best_weights)\n",
        "\n",
        "class ModelCheckpoint(Callback):\n",
        "    \"\"\"Save the model after every epoch\"\"\"\n",
        "    def __init__(self, filepath, save_best_only=True, monitor='val_accuracy', verbose=1):\n",
        "        self.filepath = filepath\n",
        "        self.save_best_only = save_best_only\n",
        "        self.monitor = monitor\n",
        "        self.verbose = verbose\n",
        "        self.best = None\n",
        "        self.mode = 'min' if 'loss' in monitor else 'max'\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None, model=None):\n",
        "        if logs is None or model is None:\n",
        "            return\n",
        "\n",
        "        current = logs.get(self.monitor)\n",
        "        if current is None:\n",
        "            return\n",
        "\n",
        "        if not self.save_best_only:\n",
        "            filepath = self.filepath.replace('.pth', f'_epoch_{epoch + 1}.pth')\n",
        "            torch.save(model.state_dict(), filepath)\n",
        "            if self.verbose:\n",
        "                print(f\"\\nModel saved to {filepath}\")\n",
        "            return\n",
        "\n",
        "        if self.best is None:\n",
        "            self.best = current\n",
        "            torch.save(model.state_dict(), self.filepath)\n",
        "            if self.verbose:\n",
        "                print(f\"\\nModel saved to {self.filepath}\")\n",
        "            return\n",
        "\n",
        "        if self.mode == 'min':\n",
        "            improved = current < self.best\n",
        "        else:\n",
        "            improved = current > self.best\n",
        "\n",
        "        if improved:\n",
        "            self.best = current\n",
        "            torch.save(model.state_dict(), self.filepath)\n",
        "            if self.verbose:\n",
        "                print(f\"\\nModel saved to {self.filepath}\")\n",
        "\n",
        "# ==========================================\n",
        "# Data Preprocessing\n",
        "# ==========================================\n",
        "\n",
        "def download_svhn_data(data_path):\n",
        "    \"\"\"Downloads SVHN dataset files.\"\"\"\n",
        "    os.makedirs(data_path, exist_ok=True)\n",
        "\n",
        "    urls = {\n",
        "        'train': 'http://ufldl.stanford.edu/housenumbers/train_32x32.mat',\n",
        "        'test': 'http://ufldl.stanford.edu/housenumbers/test_32x32.mat',\n",
        "        'extra': 'http://ufldl.stanford.edu/housenumbers/extra_32x32.mat'\n",
        "    }\n",
        "\n",
        "    downloaded_files = {}\n",
        "\n",
        "    for split, url in urls.items():\n",
        "        file_path = os.path.join(data_path, f'{split}_32x32.mat')\n",
        "\n",
        "        if os.path.exists(file_path):\n",
        "            print(f\"{split} data already exists.\")\n",
        "            downloaded_files[split] = file_path\n",
        "            continue\n",
        "\n",
        "        print(f\"Downloading {split} data from {url}...\")\n",
        "        try:\n",
        "            with requests.get(url, stream=True, timeout=60) as r:\n",
        "                r.raise_for_status()\n",
        "                total_size = int(r.headers.get('content-length', 0))\n",
        "                with open(file_path, 'wb') as f, tqdm(\n",
        "                    total=total_size, unit='iB', unit_scale=True, desc=f\"SVHN {split}\"\n",
        "                ) as progress_bar:\n",
        "                    for chunk in r.iter_content(chunk_size=8192):\n",
        "                        f.write(chunk)\n",
        "                        progress_bar.update(len(chunk))\n",
        "            downloaded_files[split] = file_path\n",
        "            print(f\"{split} data downloaded successfully.\")\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            print(f\"Error downloading {split} data: {e}\", file=sys.stderr)\n",
        "            return None\n",
        "\n",
        "    return downloaded_files\n",
        "\n",
        "class SVHNDataset(Dataset):\n",
        "    \"\"\"Custom SVHN Dataset class.\"\"\"\n",
        "    def __init__(self, mat_file_path, transform=None):\n",
        "        self.transform = transform\n",
        "\n",
        "        # Load .mat file\n",
        "        mat_data = scipy.io.loadmat(mat_file_path)\n",
        "\n",
        "        # Extract data and labels\n",
        "        # SVHN format: X is (32, 32, 3, N), y is (N, 1)\n",
        "        self.data = mat_data['X'].transpose((3, 0, 1, 2))  # Convert to (N, 32, 32, 3)\n",
        "        self.labels = mat_data['y'].flatten()\n",
        "\n",
        "        # Convert labels: SVHN uses 1-10, we need 0-9\n",
        "        # Label 10 represents digit '0', labels 1-9 represent digits '1'-'9'\n",
        "        self.labels = self.labels % 10  # Convert 10 -> 0, keep 1-9 as is\n",
        "\n",
        "        # Ensure labels are in correct range\n",
        "        assert self.labels.min() >= 0 and self.labels.max() <= 9, f\"Labels should be 0-9, got {self.labels.min()}-{self.labels.max()}\"\n",
        "\n",
        "        print(f\"Loaded {len(self.data)} samples\")\n",
        "        print(f\"Data shape: {self.data.shape}\")\n",
        "        print(f\"Labels range: {self.labels.min()} to {self.labels.max()}\")\n",
        "        print(f\"Label distribution: {np.bincount(self.labels)}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Get image and label\n",
        "        image = self.data[idx]\n",
        "        label = int(self.labels[idx])  # Ensure label is int\n",
        "\n",
        "        # Convert numpy array to PIL Image\n",
        "        image = Image.fromarray(image.astype('uint8'))\n",
        "\n",
        "        # Apply transforms\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, label\n",
        "\n",
        "def get_dataloaders(config):\n",
        "    \"\"\"Downloads, prepares, and splits the SVHN data, returning DataLoaders.\"\"\"\n",
        "\n",
        "    # Download SVHN data\n",
        "    downloaded_files = download_svhn_data(config[\"data_path\"])\n",
        "    if downloaded_files is None:\n",
        "        return None\n",
        "\n",
        "    # Define transforms\n",
        "    transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.4377, 0.4438, 0.4728], std=[0.1980, 0.2010, 0.1970]),  # SVHN stats\n",
        "    ])\n",
        "\n",
        "    # Load all datasets\n",
        "    train_dataset = SVHNDataset(downloaded_files['train'], transform=transform)\n",
        "    test_dataset = SVHNDataset(downloaded_files['test'], transform=transform)\n",
        "    extra_dataset = SVHNDataset(downloaded_files['extra'], transform=transform)\n",
        "\n",
        "    # Merge all datasets together\n",
        "    full_dataset = torch.utils.data.ConcatDataset([train_dataset, test_dataset, extra_dataset])\n",
        "    print(f\"Total merged dataset size: {len(full_dataset)}\")\n",
        "    print(f\"  - Train: {len(train_dataset)} samples\")\n",
        "    print(f\"  - Test: {len(test_dataset)} samples\")\n",
        "    print(f\"  - Extra: {len(extra_dataset)} samples\")\n",
        "\n",
        "    # Ensure reproducible splits\n",
        "    torch.manual_seed(config[\"split_seed\"])\n",
        "    np.random.seed(config[\"split_seed\"])\n",
        "\n",
        "    total_size = len(full_dataset)\n",
        "    train_size = int(config[\"train_split\"] * total_size)\n",
        "    val_size = int(config[\"val_split\"] * total_size)\n",
        "    test_size = int(config[\"test_split\"] * total_size)\n",
        "\n",
        "    # Adjust sizes to ensure they sum to total_size (handle rounding issues)\n",
        "    actual_total = train_size + val_size + test_size\n",
        "    if actual_total != total_size:\n",
        "        # Add/subtract the difference to test_size\n",
        "        test_size += (total_size - actual_total)\n",
        "\n",
        "    print(f\"Dataset splits - Train: {train_size}, Val: {val_size}, Test: {test_size}, Total: {total_size}\")\n",
        "\n",
        "    # Create reproducible generator for splitting\n",
        "    generator = torch.Generator().manual_seed(config[\"split_seed\"])\n",
        "    train_subset, val_subset, test_subset = random_split(\n",
        "        full_dataset, [train_size, val_size, test_size], generator=generator\n",
        "    )\n",
        "\n",
        "    # Set worker init function for reproducible DataLoader behavior\n",
        "    def worker_init_fn(worker_id):\n",
        "        np.random.seed(config[\"split_seed\"] + worker_id)\n",
        "\n",
        "    # Create data loaders\n",
        "    train_loader = DataLoader(\n",
        "        train_subset,\n",
        "        batch_size=config[\"batch_size\"],\n",
        "        shuffle=True,\n",
        "        num_workers=2,\n",
        "        pin_memory=True,\n",
        "        worker_init_fn=worker_init_fn,\n",
        "        generator=torch.Generator().manual_seed(config[\"split_seed\"])\n",
        "    )\n",
        "\n",
        "    val_loader = DataLoader(\n",
        "        val_subset,\n",
        "        batch_size=config[\"batch_size\"],\n",
        "        shuffle=False,\n",
        "        num_workers=2,\n",
        "        pin_memory=True,\n",
        "        worker_init_fn=worker_init_fn\n",
        "    )\n",
        "\n",
        "    test_loader = DataLoader(\n",
        "        test_subset,\n",
        "        batch_size=config[\"batch_size\"],\n",
        "        shuffle=False,\n",
        "        num_workers=2,\n",
        "        pin_memory=True,\n",
        "        worker_init_fn=worker_init_fn\n",
        "    )\n",
        "\n",
        "    return train_loader, val_loader, test_loader\n",
        "\n",
        "# ==========================================\n",
        "# Model Architecture and Training\n",
        "# ==========================================\n",
        "\n",
        "class ResidualBlock(nn.Module):\n",
        "    \"\"\"A residual block, the fundamental building block of ResNet.\"\"\"\n",
        "    def __init__(self, in_channels, out_channels, stride=1):\n",
        "        super(ResidualBlock, self).__init__()\n",
        "        # Main path\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "\n",
        "        # Shortcut path (for matching dimensions)\n",
        "        self.downsample = nn.Sequential()\n",
        "        if stride != 1 or in_channels != out_channels:\n",
        "            self.downsample = nn.Sequential(\n",
        "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(out_channels)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = self.downsample(x)\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        out += identity\n",
        "        out = self.relu(out)\n",
        "        return out\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "    \"\"\"A modular ResNet implementation.\"\"\"\n",
        "    def __init__(self, block, layers, num_classes=200):\n",
        "        super(ResNet, self).__init__()\n",
        "        self.in_channels = 64\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "        self.layer1 = self._make_layer(block, 64, layers[0], stride=1)\n",
        "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n",
        "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Linear(512, num_classes)\n",
        "\n",
        "    def _make_layer(self, block, out_channels, num_blocks, stride):\n",
        "        strides = [stride] + [1] * (num_blocks - 1)\n",
        "        layers = []\n",
        "        for s in strides:\n",
        "            layers.append(block(self.in_channels, out_channels, s))\n",
        "            self.in_channels = out_channels\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "        x = self.avgpool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "def ResNet18(num_classes=200):\n",
        "    return ResNet(ResidualBlock, [2, 2, 2, 2], num_classes)\n",
        "\n",
        "def evaluate_model(model, data_loader, criterion, device):\n",
        "    \"\"\"Evaluates the model on a given dataset.\"\"\"\n",
        "    model.eval()\n",
        "    total_loss, correct, total = 0.0, 0, 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in data_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            total_loss += loss.item() * images.size(0)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    avg_loss = total_loss / total\n",
        "    accuracy = 100 * correct / total\n",
        "    return avg_loss, accuracy\n",
        "\n",
        "def train_and_validate(model, train_loader, val_loader, config):\n",
        "    \"\"\"Main training loop with callbacks.\"\"\"\n",
        "    device = config[\"device\"]\n",
        "    model.to(device)\n",
        "\n",
        "    # Set seeds for reproducible training\n",
        "    torch.manual_seed(config[\"split_seed\"])\n",
        "    np.random.seed(config[\"split_seed\"])\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(config[\"split_seed\"])\n",
        "        torch.cuda.manual_seed_all(config[\"split_seed\"])\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=config[\"learning_rate\"], weight_decay=config[\"weight_decay\"])\n",
        "\n",
        "    # Initialize callbacks\n",
        "    callbacks = [\n",
        "        ReduceLROnPlateau(\n",
        "            optimizer=optimizer,\n",
        "            monitor='val_accuracy',\n",
        "            factor=0.2,\n",
        "            patience=3,\n",
        "            min_lr=1e-7,\n",
        "            verbose=1\n",
        "        ),\n",
        "        EarlyStopping(\n",
        "            monitor='val_accuracy',\n",
        "            patience=7,\n",
        "            restore_best_weights=True,\n",
        "            verbose=1\n",
        "        ),\n",
        "        ModelCheckpoint(\n",
        "            filepath=config[\"checkpoint_path\"],\n",
        "            save_best_only=True,\n",
        "            monitor='val_accuracy',\n",
        "            verbose=1\n",
        "        )\n",
        "    ]\n",
        "\n",
        "    print(\"\\n--- Starting Training ---\")\n",
        "    early_stop = False\n",
        "\n",
        "    for epoch in range(config[\"num_epochs\"]):\n",
        "        if early_stop:\n",
        "            break\n",
        "\n",
        "        model.train()\n",
        "        start_time = time.time()\n",
        "        running_loss, train_correct, train_total = 0.0, 0, 0\n",
        "\n",
        "        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{config['num_epochs']} [Train]\", leave=False)\n",
        "        for images, labels in progress_bar:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item() * images.size(0)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            train_total += labels.size(0)\n",
        "            train_correct += (predicted == labels).sum().item()\n",
        "            progress_bar.set_postfix(loss=loss.item())\n",
        "\n",
        "        train_loss = running_loss / train_total\n",
        "        train_acc = 100 * train_correct / train_total\n",
        "        val_loss, val_acc = evaluate_model(model, val_loader, criterion, device)\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{config['num_epochs']} | Time: {time.time() - start_time:.2f}s | \"\n",
        "              f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}% | \"\n",
        "              f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%\")\n",
        "\n",
        "        # Prepare logs for callbacks\n",
        "        logs = {\n",
        "            'train_loss': train_loss,\n",
        "            'train_accuracy': train_acc,\n",
        "            'val_loss': val_loss,\n",
        "            'val_accuracy': val_acc\n",
        "        }\n",
        "\n",
        "        # Execute callbacks\n",
        "        for callback in callbacks:\n",
        "            if isinstance(callback, EarlyStopping):\n",
        "                if callback.on_epoch_end(epoch, logs, model):\n",
        "                    early_stop = True\n",
        "                    break\n",
        "            elif isinstance(callback, ModelCheckpoint):\n",
        "                callback.on_epoch_end(epoch, logs, model)\n",
        "            else:\n",
        "                callback.on_epoch_end(epoch, logs)\n",
        "\n",
        "    # Execute callback cleanup\n",
        "    for callback in callbacks:\n",
        "        if isinstance(callback, EarlyStopping):\n",
        "            callback.on_training_end(model)\n",
        "        else:\n",
        "            callback.on_training_end()\n",
        "\n",
        "    print(\"--- Training Finished ---\\n\")\n",
        "    return model\n",
        "\n",
        "# ==========================================\n",
        "# Evaluation\n",
        "# ==========================================\n",
        "\n",
        "def calculate_top_k_accuracy(outputs, labels, k_values=[1, 3, 5]):\n",
        "    \"\"\"Calculate top-k accuracy for given k values.\"\"\"\n",
        "    batch_size = labels.size(0)\n",
        "    _, pred = outputs.topk(max(k_values), 1, True, True)\n",
        "    pred = pred.t()\n",
        "    correct = pred.eq(labels.view(1, -1).expand_as(pred))\n",
        "\n",
        "    top_k_accuracies = {}\n",
        "    for k in k_values:\n",
        "        correct_k = correct[:k].reshape(-1).float().sum(0, keepdim=True)\n",
        "        top_k_accuracies[k] = correct_k.item() / batch_size\n",
        "\n",
        "    return top_k_accuracies\n",
        "\n",
        "def calculate_entropy(probs):\n",
        "    \"\"\"Calculate entropy of probability distributions.\"\"\"\n",
        "    # Add small epsilon to avoid log(0)\n",
        "    epsilon = 1e-8\n",
        "    probs = probs + epsilon\n",
        "    entropy = -torch.sum(probs * torch.log(probs), dim=1)\n",
        "    return entropy\n",
        "\n",
        "def calculate_ece(confidences, accuracies, n_bins=10):\n",
        "    \"\"\"Calculate Expected Calibration Error.\"\"\"\n",
        "    bin_boundaries = torch.linspace(0, 1, n_bins + 1)\n",
        "    bin_lowers = bin_boundaries[:-1]\n",
        "    bin_uppers = bin_boundaries[1:]\n",
        "\n",
        "    ece = 0\n",
        "    for bin_lower, bin_upper in zip(bin_lowers, bin_uppers):\n",
        "        in_bin = (confidences > bin_lower.item()) & (confidences <= bin_upper.item())\n",
        "        prop_in_bin = in_bin.float().mean()\n",
        "\n",
        "        if prop_in_bin.item() > 0:\n",
        "            accuracy_in_bin = accuracies[in_bin].float().mean()\n",
        "            avg_confidence_in_bin = confidences[in_bin].mean()\n",
        "            ece += torch.abs(avg_confidence_in_bin - accuracy_in_bin) * prop_in_bin\n",
        "\n",
        "    return ece.item()\n",
        "\n",
        "def comprehensive_evaluation(model, test_loader, criterion, device):\n",
        "    \"\"\"Comprehensive evaluation with all requested metrics.\"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    all_predictions = []\n",
        "    all_labels = []\n",
        "    all_confidences = []\n",
        "    all_entropies = []\n",
        "    all_top_k_results = {1: [], 3: [], 5: []}\n",
        "    total_loss = 0.0\n",
        "    total_samples = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in tqdm(test_loader, desc=\"Evaluating\"):\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            # Convert outputs to probabilities\n",
        "            probs = torch.softmax(outputs, dim=1)\n",
        "\n",
        "            # Get predictions and confidences\n",
        "            confidences, predictions = torch.max(probs, dim=1)\n",
        "\n",
        "            # Calculate entropy\n",
        "            entropies = calculate_entropy(probs)\n",
        "\n",
        "            # Calculate top-k accuracies\n",
        "            top_k_accs = calculate_top_k_accuracy(outputs, labels, [1, 3, 5])\n",
        "\n",
        "            # Store results\n",
        "            all_predictions.extend(predictions.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "            all_confidences.extend(confidences.cpu().numpy())\n",
        "            all_entropies.extend(entropies.cpu().numpy())\n",
        "\n",
        "            for k in [1, 3, 5]:\n",
        "                all_top_k_results[k].extend([top_k_accs[k]] * labels.size(0))\n",
        "\n",
        "            total_loss += loss.item() * images.size(0)\n",
        "            total_samples += images.size(0)\n",
        "\n",
        "    # Convert to numpy arrays\n",
        "    all_predictions = np.array(all_predictions)\n",
        "    all_labels = np.array(all_labels)\n",
        "    all_confidences = np.array(all_confidences)\n",
        "    all_entropies = np.array(all_entropies)\n",
        "\n",
        "    # Calculate basic metrics\n",
        "    avg_loss = total_loss / total_samples\n",
        "\n",
        "    # Calculate top-k accuracies\n",
        "    top1 = np.mean([pred == label for pred, label in zip(all_predictions, all_labels)])\n",
        "    top3 = np.mean(all_top_k_results[3])\n",
        "    top5 = np.mean(all_top_k_results[5])\n",
        "\n",
        "    # Calculate precision, recall, f1-score\n",
        "    precision_macro, recall_macro, f1_macro, _ = precision_recall_fscore_support(\n",
        "        all_labels, all_predictions, average='macro', zero_division=0\n",
        "    )\n",
        "    precision_weighted, recall_weighted, f1_weighted, _ = precision_recall_fscore_support(\n",
        "        all_labels, all_predictions, average='weighted', zero_division=0\n",
        "    )\n",
        "\n",
        "    # Calculate confidence and calibration metrics\n",
        "    avg_confidence = np.mean(all_confidences)\n",
        "    std_confidence = np.std(all_confidences)\n",
        "    avg_entropy = np.mean(all_entropies)\n",
        "\n",
        "    # Calculate ECE\n",
        "    accuracies = (all_predictions == all_labels).astype(float)\n",
        "    ece = calculate_ece(torch.tensor(all_confidences), torch.tensor(accuracies))\n",
        "\n",
        "    return {\n",
        "        'test_loss': avg_loss,\n",
        "        'top1': top1,\n",
        "        'top3': top3,\n",
        "        'top5': top5,\n",
        "        'precision_macro': precision_macro,\n",
        "        'recall_macro': recall_macro,\n",
        "        'f1_macro': f1_macro,\n",
        "        'precision_weighted': precision_weighted,\n",
        "        'recall_weighted': recall_weighted,\n",
        "        'f1_weighted': f1_weighted,\n",
        "        'avg_confidence': avg_confidence,\n",
        "        'std_confidence': std_confidence,\n",
        "        'avg_entropy': avg_entropy,\n",
        "        'ece': ece\n",
        "    }\n",
        "\n",
        "# ==========================================\n",
        "# Main Execution\n",
        "# ==========================================\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # Set global seeds for full reproducibility\n",
        "    torch.manual_seed(CONFIG[\"split_seed\"])\n",
        "    np.random.seed(CONFIG[\"split_seed\"])\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(CONFIG[\"split_seed\"])\n",
        "        torch.cuda.manual_seed_all(CONFIG[\"split_seed\"])\n",
        "        # Ensure deterministic behavior on CUDA\n",
        "        torch.backends.cudnn.deterministic = True\n",
        "        torch.backends.cudnn.benchmark = False\n",
        "\n",
        "    print(f\"Using device: {CONFIG['device']}\")\n",
        "    print(f\"Reproducibility seed: {CONFIG['split_seed']}\")\n",
        "\n",
        "    # Get data loaders\n",
        "    dataloaders = get_dataloaders(CONFIG)\n",
        "    if dataloaders is None:\n",
        "        print(\"Could not prepare data. Halting execution.\", file=sys.stderr)\n",
        "        sys.exit(1) # Exit if data preparation failed\n",
        "\n",
        "    train_loader, val_loader, test_loader = dataloaders\n",
        "\n",
        "    # Initialize and train model\n",
        "    model = ResNet18(num_classes=CONFIG[\"num_classes\"])\n",
        "    trained_model = train_and_validate(model, train_loader, val_loader, CONFIG)\n",
        "\n",
        "    # Comprehensive evaluation on the test set\n",
        "    print(\"--- Starting Comprehensive Evaluation on Test Set ---\")\n",
        "    results = comprehensive_evaluation(trained_model, test_loader, nn.CrossEntropyLoss(), CONFIG[\"device\"])\n",
        "\n",
        "    # Print all requested metrics\n",
        "    print(\"\\n--- Top-K Accuracy Results ---\")\n",
        "    print(f\"Top-1 Accuracy: {results['top1'] * 100:.2f}%\")\n",
        "    print(f\"Top-3 Accuracy: {results['top3'] * 100:.2f}%\")\n",
        "    print(f\"Top-5 Accuracy: {results['top5'] * 100:.2f}%\")\n",
        "\n",
        "    print(\"\\n--- Additional Performance Metrics ---\")\n",
        "    print(f\"Macro Average Precision: {results['precision_macro']:.4f}\")\n",
        "    print(f\"Macro Average Recall: {results['recall_macro']:.4f}\")\n",
        "    print(f\"Macro Average F1-Score: {results['f1_macro']:.4f}\")\n",
        "    print(f\"Weighted Average Precision: {results['precision_weighted']:.4f}\")\n",
        "    print(f\"Weighted Average Recall: {results['recall_weighted']:.4f}\")\n",
        "    print(f\"Weighted Average F1-Score: {results['f1_weighted']:.4f}\")\n",
        "\n",
        "    print(\"\\n--- Model Confidence & Calibration Metrics ---\")\n",
        "    print(f\"Average Prediction Confidence: {results['avg_confidence']:.4f}\")\n",
        "    print(f\"Confidence Standard Deviation: {results['std_confidence']:.4f}\")\n",
        "    print(f\"Average Prediction Entropy: {results['avg_entropy']:.4f}\")\n",
        "    print(f\"Expected Calibration Error: {results['ece']:.4f}\")\n",
        "\n",
        "    print(f\"\\nFinal Test Loss: {results['test_loss']:.4f}\")\n",
        "    print(\"===============================================\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hybrid Transformer on ResNet-18 (No PE)"
      ],
      "metadata": {
        "id": "df6CSddHp8OJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "import torchvision.transforms as transforms\n",
        "import scipy.io\n",
        "from PIL import Image\n",
        "import os\n",
        "import requests\n",
        "import zipfile\n",
        "import shutil\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "import sys\n",
        "import numpy as np\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "\n",
        "# --- Configuration Parameters ---\n",
        "CONFIG = {\n",
        "    \"data_path\": \"./svhn-data/\",\n",
        "    \"batch_size\": 128,\n",
        "    \"num_epochs\": 100,\n",
        "    \"learning_rate\": 0.001,\n",
        "    \"weight_decay\": 1e-4,\n",
        "    \"split_seed\": 42,\n",
        "    \"train_split\": 0.70,\n",
        "    \"val_split\": 0.15,\n",
        "    \"test_split\": 0.15,\n",
        "    \"num_classes\": 10,    # SVHN has 10 classes (digits 0-9)\n",
        "    \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
        "    \"checkpoint_path\": \"./best_svhn_model.pth\",\n",
        "}\n",
        "\n",
        "# --- Transformer Configuration ---\n",
        "TRANSFORMER_CONFIG = {\n",
        "    \"embedding_dim\": 32,\n",
        "    \"nhead\": 16,\n",
        "    \"num_encoder_layers\": 3,\n",
        "    \"dim_feedforward\": 2,\n",
        "    \"dropout\": 0.1,\n",
        "}\n",
        "\n",
        "# ==========================================\n",
        "# Callback Classes\n",
        "# ==========================================\n",
        "\n",
        "class Callback:\n",
        "    \"\"\"Base callback class\"\"\"\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        pass\n",
        "\n",
        "    def on_training_end(self):\n",
        "        pass\n",
        "\n",
        "class ReduceLROnPlateau(Callback):\n",
        "    \"\"\"Reduce learning rate when a metric has stopped improving\"\"\"\n",
        "    def __init__(self, optimizer, monitor='val_loss', factor=0.2, patience=3, min_lr=1e-7, verbose=1):\n",
        "        self.optimizer = optimizer\n",
        "        self.monitor = monitor\n",
        "        self.factor = factor\n",
        "        self.patience = patience\n",
        "        self.min_lr = min_lr\n",
        "        self.verbose = verbose\n",
        "        self.wait = 0\n",
        "        self.best = None\n",
        "        self.mode = 'min' if 'loss' in monitor else 'max'\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        if logs is None:\n",
        "            return\n",
        "\n",
        "        current = logs.get(self.monitor)\n",
        "        if current is None:\n",
        "            return\n",
        "\n",
        "        if self.best is None:\n",
        "            self.best = current\n",
        "            return\n",
        "\n",
        "        if self.mode == 'min':\n",
        "            improved = current < self.best\n",
        "        else:\n",
        "            improved = current > self.best\n",
        "\n",
        "        if improved:\n",
        "            self.best = current\n",
        "            self.wait = 0\n",
        "        else:\n",
        "            self.wait += 1\n",
        "            if self.wait >= self.patience:\n",
        "                old_lr = self.optimizer.param_groups[0]['lr']\n",
        "                new_lr = max(old_lr * self.factor, self.min_lr)\n",
        "                if new_lr != old_lr:\n",
        "                    self.optimizer.param_groups[0]['lr'] = new_lr\n",
        "                    if self.verbose:\n",
        "                        print(f\"\\nReducing learning rate from {old_lr:.2e} to {new_lr:.2e}\")\n",
        "                    self.wait = 0\n",
        "\n",
        "class EarlyStopping(Callback):\n",
        "    \"\"\"Stop training when a monitored metric has stopped improving\"\"\"\n",
        "    def __init__(self, monitor='val_loss', patience=7, restore_best_weights=True, verbose=1):\n",
        "        self.monitor = monitor\n",
        "        self.patience = patience\n",
        "        self.restore_best_weights = restore_best_weights\n",
        "        self.verbose = verbose\n",
        "        self.wait = 0\n",
        "        self.best = None\n",
        "        self.best_weights = None\n",
        "        self.mode = 'min' if 'loss' in monitor else 'max'\n",
        "        self.stopped_epoch = 0\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None, model=None):\n",
        "        if logs is None:\n",
        "            return False\n",
        "\n",
        "        current = logs.get(self.monitor)\n",
        "        if current is None:\n",
        "            return False\n",
        "\n",
        "        if self.best is None:\n",
        "            self.best = current\n",
        "            if model is not None and self.restore_best_weights:\n",
        "                self.best_weights = model.state_dict().copy()\n",
        "            return False\n",
        "\n",
        "        if self.mode == 'min':\n",
        "            improved = current < self.best\n",
        "        else:\n",
        "            improved = current > self.best\n",
        "\n",
        "        if improved:\n",
        "            self.best = current\n",
        "            self.wait = 0\n",
        "            if model is not None and self.restore_best_weights:\n",
        "                self.best_weights = model.state_dict().copy()\n",
        "        else:\n",
        "            self.wait += 1\n",
        "            if self.wait >= self.patience:\n",
        "                self.stopped_epoch = epoch\n",
        "                if self.verbose:\n",
        "                    print(f\"\\nEarly stopping at epoch {epoch + 1}\")\n",
        "                return True\n",
        "        return False\n",
        "\n",
        "    def on_training_end(self, model=None):\n",
        "        if self.stopped_epoch > 0 and self.verbose:\n",
        "            print(f\"Restored model weights from the end of the best epoch: {self.stopped_epoch + 1 - self.patience}\")\n",
        "        if model is not None and self.restore_best_weights and self.best_weights is not None:\n",
        "            model.load_state_dict(self.best_weights)\n",
        "\n",
        "class ModelCheckpoint(Callback):\n",
        "    \"\"\"Save the model after every epoch\"\"\"\n",
        "    def __init__(self, filepath, save_best_only=True, monitor='val_accuracy', verbose=1):\n",
        "        self.filepath = filepath\n",
        "        self.save_best_only = save_best_only\n",
        "        self.monitor = monitor\n",
        "        self.verbose = verbose\n",
        "        self.best = None\n",
        "        self.mode = 'min' if 'loss' in monitor else 'max'\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None, model=None):\n",
        "        if logs is None or model is None:\n",
        "            return\n",
        "\n",
        "        current = logs.get(self.monitor)\n",
        "        if current is None:\n",
        "            return\n",
        "\n",
        "        if not self.save_best_only:\n",
        "            filepath = self.filepath.replace('.pth', f'_epoch_{epoch + 1}.pth')\n",
        "            torch.save(model.state_dict(), filepath)\n",
        "            if self.verbose:\n",
        "                print(f\"\\nModel saved to {filepath}\")\n",
        "            return\n",
        "\n",
        "        if self.best is None:\n",
        "            self.best = current\n",
        "            torch.save(model.state_dict(), self.filepath)\n",
        "            if self.verbose:\n",
        "                print(f\"\\nModel saved to {self.filepath}\")\n",
        "            return\n",
        "\n",
        "        if self.mode == 'min':\n",
        "            improved = current < self.best\n",
        "        else:\n",
        "            improved = current > self.best\n",
        "\n",
        "        if improved:\n",
        "            self.best = current\n",
        "            torch.save(model.state_dict(), self.filepath)\n",
        "            if self.verbose:\n",
        "                print(f\"\\nModel saved to {self.filepath}\")\n",
        "\n",
        "# ==========================================\n",
        "# Data Preprocessing\n",
        "# ==========================================\n",
        "\n",
        "def download_svhn_data(data_path):\n",
        "    \"\"\"Downloads SVHN dataset files.\"\"\"\n",
        "    os.makedirs(data_path, exist_ok=True)\n",
        "\n",
        "    urls = {\n",
        "        'train': 'http://ufldl.stanford.edu/housenumbers/train_32x32.mat',\n",
        "        'test': 'http://ufldl.stanford.edu/housenumbers/test_32x32.mat',\n",
        "        'extra': 'http://ufldl.stanford.edu/housenumbers/extra_32x32.mat'\n",
        "    }\n",
        "\n",
        "    downloaded_files = {}\n",
        "\n",
        "    for split, url in urls.items():\n",
        "        file_path = os.path.join(data_path, f'{split}_32x32.mat')\n",
        "\n",
        "        if os.path.exists(file_path):\n",
        "            print(f\"{split} data already exists.\")\n",
        "            downloaded_files[split] = file_path\n",
        "            continue\n",
        "\n",
        "        print(f\"Downloading {split} data from {url}...\")\n",
        "        try:\n",
        "            with requests.get(url, stream=True, timeout=60) as r:\n",
        "                r.raise_for_status()\n",
        "                total_size = int(r.headers.get('content-length', 0))\n",
        "                with open(file_path, 'wb') as f, tqdm(\n",
        "                    total=total_size, unit='iB', unit_scale=True, desc=f\"SVHN {split}\"\n",
        "                ) as progress_bar:\n",
        "                    for chunk in r.iter_content(chunk_size=8192):\n",
        "                        f.write(chunk)\n",
        "                        progress_bar.update(len(chunk))\n",
        "            downloaded_files[split] = file_path\n",
        "            print(f\"{split} data downloaded successfully.\")\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            print(f\"Error downloading {split} data: {e}\", file=sys.stderr)\n",
        "            return None\n",
        "\n",
        "    return downloaded_files\n",
        "\n",
        "class SVHNDataset(Dataset):\n",
        "    \"\"\"Custom SVHN Dataset class.\"\"\"\n",
        "    def __init__(self, mat_file_path, transform=None):\n",
        "        self.transform = transform\n",
        "\n",
        "        # Load .mat file\n",
        "        mat_data = scipy.io.loadmat(mat_file_path)\n",
        "\n",
        "        # Extract data and labels\n",
        "        # SVHN format: X is (32, 32, 3, N), y is (N, 1)\n",
        "        self.data = mat_data['X'].transpose((3, 0, 1, 2))  # Convert to (N, 32, 32, 3)\n",
        "        self.labels = mat_data['y'].flatten()\n",
        "\n",
        "        # Convert labels: SVHN uses 1-10, we need 0-9\n",
        "        # Label 10 represents digit '0', labels 1-9 represent digits '1'-'9'\n",
        "        self.labels = self.labels % 10  # Convert 10 -> 0, keep 1-9 as is\n",
        "\n",
        "        # Ensure labels are in correct range\n",
        "        assert self.labels.min() >= 0 and self.labels.max() <= 9, f\"Labels should be 0-9, got {self.labels.min()}-{self.labels.max()}\"\n",
        "\n",
        "        print(f\"Loaded {len(self.data)} samples\")\n",
        "        print(f\"Data shape: {self.data.shape}\")\n",
        "        print(f\"Labels range: {self.labels.min()} to {self.labels.max()}\")\n",
        "        print(f\"Label distribution: {np.bincount(self.labels)}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Get image and label\n",
        "        image = self.data[idx]\n",
        "        label = int(self.labels[idx])  # Ensure label is int\n",
        "\n",
        "        # Convert numpy array to PIL Image\n",
        "        image = Image.fromarray(image.astype('uint8'))\n",
        "\n",
        "        # Apply transforms\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, label\n",
        "\n",
        "def get_dataloaders(config):\n",
        "    \"\"\"Downloads, prepares, and splits the SVHN data, returning DataLoaders.\"\"\"\n",
        "\n",
        "    # Download SVHN data\n",
        "    downloaded_files = download_svhn_data(config[\"data_path\"])\n",
        "    if downloaded_files is None:\n",
        "        return None\n",
        "\n",
        "    # Define transforms\n",
        "    transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.4377, 0.4438, 0.4728], std=[0.1980, 0.2010, 0.1970]),  # SVHN stats\n",
        "    ])\n",
        "\n",
        "    # Load all datasets\n",
        "    train_dataset = SVHNDataset(downloaded_files['train'], transform=transform)\n",
        "    test_dataset = SVHNDataset(downloaded_files['test'], transform=transform)\n",
        "    extra_dataset = SVHNDataset(downloaded_files['extra'], transform=transform)\n",
        "\n",
        "    # Merge all datasets together\n",
        "    full_dataset = torch.utils.data.ConcatDataset([train_dataset, test_dataset, extra_dataset])\n",
        "    print(f\"Total merged dataset size: {len(full_dataset)}\")\n",
        "    print(f\"  - Train: {len(train_dataset)} samples\")\n",
        "    print(f\"  - Test: {len(test_dataset)} samples\")\n",
        "    print(f\"  - Extra: {len(extra_dataset)} samples\")\n",
        "\n",
        "    # Ensure reproducible splits\n",
        "    torch.manual_seed(config[\"split_seed\"])\n",
        "    np.random.seed(config[\"split_seed\"])\n",
        "\n",
        "    total_size = len(full_dataset)\n",
        "    train_size = int(config[\"train_split\"] * total_size)\n",
        "    val_size = int(config[\"val_split\"] * total_size)\n",
        "    test_size = int(config[\"test_split\"] * total_size)\n",
        "\n",
        "    # Adjust sizes to ensure they sum to total_size (handle rounding issues)\n",
        "    actual_total = train_size + val_size + test_size\n",
        "    if actual_total != total_size:\n",
        "        # Add/subtract the difference to test_size\n",
        "        test_size += (total_size - actual_total)\n",
        "\n",
        "    print(f\"Dataset splits - Train: {train_size}, Val: {val_size}, Test: {test_size}, Total: {total_size}\")\n",
        "\n",
        "    # Create reproducible generator for splitting\n",
        "    generator = torch.Generator().manual_seed(config[\"split_seed\"])\n",
        "    train_subset, val_subset, test_subset = random_split(\n",
        "        full_dataset, [train_size, val_size, test_size], generator=generator\n",
        "    )\n",
        "\n",
        "    # Set worker init function for reproducible DataLoader behavior\n",
        "    def worker_init_fn(worker_id):\n",
        "        np.random.seed(config[\"split_seed\"] + worker_id)\n",
        "\n",
        "    # Create data loaders\n",
        "    train_loader = DataLoader(\n",
        "        train_subset,\n",
        "        batch_size=config[\"batch_size\"],\n",
        "        shuffle=True,\n",
        "        num_workers=2,\n",
        "        pin_memory=True,\n",
        "        worker_init_fn=worker_init_fn,\n",
        "        generator=torch.Generator().manual_seed(config[\"split_seed\"])\n",
        "    )\n",
        "\n",
        "    val_loader = DataLoader(\n",
        "        val_subset,\n",
        "        batch_size=config[\"batch_size\"],\n",
        "        shuffle=False,\n",
        "        num_workers=2,\n",
        "        pin_memory=True,\n",
        "        worker_init_fn=worker_init_fn\n",
        "    )\n",
        "\n",
        "    test_loader = DataLoader(\n",
        "        test_subset,\n",
        "        batch_size=config[\"batch_size\"],\n",
        "        shuffle=False,\n",
        "        num_workers=2,\n",
        "        pin_memory=True,\n",
        "        worker_init_fn=worker_init_fn\n",
        "    )\n",
        "\n",
        "    return train_loader, val_loader, test_loader\n",
        "\n",
        "# ==========================================\n",
        "# Model Architecture and Training\n",
        "# ==========================================\n",
        "\n",
        "class NonResidualBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    This block is a standard convolutional block WITHOUT the residual connection.\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels, out_channels, stride=1):\n",
        "        super(NonResidualBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        out = self.relu(out)\n",
        "        return out\n",
        "\n",
        "class ResNetTransformer(nn.Module):\n",
        "    \"\"\"\n",
        "    A hybrid architecture combining a non-residual CNN backbone with a Transformer encoder.\n",
        "    \"\"\"\n",
        "    def __init__(self, block, layers, num_classes, t_config):\n",
        "        super(ResNetTransformer, self).__init__()\n",
        "        self.in_channels = 64\n",
        "        self.embedding_dim = t_config[\"embedding_dim\"]\n",
        "\n",
        "        # 1. CNN Backbone (Feature Extractor)\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "        self.layer1 = self._make_layer(block, 64, layers[0], stride=1)\n",
        "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n",
        "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n",
        "\n",
        "        # 2. Projection heads to create tokens from feature maps\n",
        "        self.projections = nn.ModuleList([\n",
        "            self._create_projection(64, self.embedding_dim),   # From initial maxpool\n",
        "            self._create_projection(64, self.embedding_dim),   # From layer1\n",
        "            self._create_projection(128, self.embedding_dim),  # From layer2\n",
        "            self._create_projection(256, self.embedding_dim),  # From layer3\n",
        "            self._create_projection(512, self.embedding_dim)   # From layer4\n",
        "        ])\n",
        "\n",
        "        # 3. Transformer Encoder\n",
        "        encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=self.embedding_dim,\n",
        "            nhead=t_config[\"nhead\"],\n",
        "            dim_feedforward=t_config[\"dim_feedforward\"],\n",
        "            dropout=t_config[\"dropout\"],\n",
        "            batch_first=True\n",
        "        )\n",
        "        self.transformer_encoder = nn.TransformerEncoder(\n",
        "            encoder_layer,\n",
        "            num_layers=t_config[\"num_encoder_layers\"]\n",
        "        )\n",
        "\n",
        "        # 4. Final Classifier\n",
        "        self.classifier = nn.Linear(self.embedding_dim, num_classes)\n",
        "\n",
        "    def _create_projection(self, in_features, out_features):\n",
        "        return nn.Sequential(\n",
        "            nn.AdaptiveAvgPool2d((1, 1)),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(in_features, out_features)\n",
        "        )\n",
        "\n",
        "    def _make_layer(self, block, out_channels, num_blocks, stride):\n",
        "        strides = [stride] + [1] * (num_blocks - 1)\n",
        "        layers = []\n",
        "        current_in_channels = self.in_channels\n",
        "        for s in strides:\n",
        "            layers.append(block(current_in_channels, out_channels, s))\n",
        "            current_in_channels = out_channels\n",
        "        self.in_channels = out_channels\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # 1. Pass through CNN backbone and capture features\n",
        "        features = []\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "        features.append(x)\n",
        "\n",
        "        x = self.layer1(x); features.append(x)\n",
        "        x = self.layer2(x); features.append(x)\n",
        "        x = self.layer3(x); features.append(x)\n",
        "        x = self.layer4(x); features.append(x)\n",
        "\n",
        "        # 2. Project features to tokens\n",
        "        tokens = []\n",
        "        for i, feature_map in enumerate(features):\n",
        "            tokens.append(self.projections[i](feature_map))\n",
        "\n",
        "        # 3. Stack tokens and pass through Transformer\n",
        "        token_sequence = torch.stack(tokens, dim=1)\n",
        "        transformer_out = self.transformer_encoder(token_sequence)\n",
        "\n",
        "        # 4. Aggregate and classify\n",
        "        aggregated_vector = transformer_out.mean(dim=1)\n",
        "        logits = self.classifier(aggregated_vector)\n",
        "\n",
        "        return logits\n",
        "\n",
        "def ResNetTransformer18(num_classes=200, t_config=TRANSFORMER_CONFIG):\n",
        "    return ResNetTransformer(NonResidualBlock, [2, 2, 2, 2], num_classes, t_config)\n",
        "\n",
        "def evaluate_model(model, data_loader, criterion, device):\n",
        "    \"\"\"Evaluates the model on a given dataset.\"\"\"\n",
        "    model.eval()\n",
        "    total_loss, correct, total = 0.0, 0, 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in data_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            total_loss += loss.item() * images.size(0)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    avg_loss = total_loss / total\n",
        "    accuracy = 100 * correct / total\n",
        "    return avg_loss, accuracy\n",
        "\n",
        "def train_and_validate(model, train_loader, val_loader, config):\n",
        "    \"\"\"Main training loop with callbacks.\"\"\"\n",
        "    device = config[\"device\"]\n",
        "    model.to(device)\n",
        "\n",
        "    # Set seeds for reproducible training\n",
        "    torch.manual_seed(config[\"split_seed\"])\n",
        "    np.random.seed(config[\"split_seed\"])\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(config[\"split_seed\"])\n",
        "        torch.cuda.manual_seed_all(config[\"split_seed\"])\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=config[\"learning_rate\"], weight_decay=config[\"weight_decay\"])\n",
        "\n",
        "    # Initialize callbacks\n",
        "    callbacks = [\n",
        "        ReduceLROnPlateau(\n",
        "            optimizer=optimizer,\n",
        "            monitor='val_accuracy',\n",
        "            factor=0.2,\n",
        "            patience=3,\n",
        "            min_lr=1e-7,\n",
        "            verbose=1\n",
        "        ),\n",
        "        EarlyStopping(\n",
        "            monitor='val_accuracy',\n",
        "            patience=7,\n",
        "            restore_best_weights=True,\n",
        "            verbose=1\n",
        "        ),\n",
        "        ModelCheckpoint(\n",
        "            filepath=config[\"checkpoint_path\"],\n",
        "            save_best_only=True,\n",
        "            monitor='val_accuracy',\n",
        "            verbose=1\n",
        "        )\n",
        "    ]\n",
        "\n",
        "    print(\"\\n--- Starting Training ---\")\n",
        "    early_stop = False\n",
        "\n",
        "    for epoch in range(config[\"num_epochs\"]):\n",
        "        if early_stop:\n",
        "            break\n",
        "\n",
        "        model.train()\n",
        "        start_time = time.time()\n",
        "        running_loss, train_correct, train_total = 0.0, 0, 0\n",
        "\n",
        "        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{config['num_epochs']} [Train]\", leave=False)\n",
        "        for images, labels in progress_bar:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item() * images.size(0)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            train_total += labels.size(0)\n",
        "            train_correct += (predicted == labels).sum().item()\n",
        "            progress_bar.set_postfix(loss=loss.item())\n",
        "\n",
        "        train_loss = running_loss / train_total\n",
        "        train_acc = 100 * train_correct / train_total\n",
        "        val_loss, val_acc = evaluate_model(model, val_loader, criterion, device)\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{config['num_epochs']} | Time: {time.time() - start_time:.2f}s | \"\n",
        "              f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}% | \"\n",
        "              f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%\")\n",
        "\n",
        "        # Prepare logs for callbacks\n",
        "        logs = {\n",
        "            'train_loss': train_loss,\n",
        "            'train_accuracy': train_acc,\n",
        "            'val_loss': val_loss,\n",
        "            'val_accuracy': val_acc\n",
        "        }\n",
        "\n",
        "        # Execute callbacks\n",
        "        for callback in callbacks:\n",
        "            if isinstance(callback, EarlyStopping):\n",
        "                if callback.on_epoch_end(epoch, logs, model):\n",
        "                    early_stop = True\n",
        "                    break\n",
        "            elif isinstance(callback, ModelCheckpoint):\n",
        "                callback.on_epoch_end(epoch, logs, model)\n",
        "            else:\n",
        "                callback.on_epoch_end(epoch, logs)\n",
        "\n",
        "    # Execute callback cleanup\n",
        "    for callback in callbacks:\n",
        "        if isinstance(callback, EarlyStopping):\n",
        "            callback.on_training_end(model)\n",
        "        else:\n",
        "            callback.on_training_end()\n",
        "\n",
        "    print(\"--- Training Finished ---\\n\")\n",
        "    return model\n",
        "\n",
        "# ==========================================\n",
        "# Evaluation\n",
        "# ==========================================\n",
        "\n",
        "def calculate_top_k_accuracy(outputs, labels, k_values=[1, 3, 5]):\n",
        "    \"\"\"Calculate top-k accuracy for given k values.\"\"\"\n",
        "    batch_size = labels.size(0)\n",
        "    _, pred = outputs.topk(max(k_values), 1, True, True)\n",
        "    pred = pred.t()\n",
        "    correct = pred.eq(labels.view(1, -1).expand_as(pred))\n",
        "\n",
        "    top_k_accuracies = {}\n",
        "    for k in k_values:\n",
        "        correct_k = correct[:k].reshape(-1).float().sum(0, keepdim=True)\n",
        "        top_k_accuracies[k] = correct_k.item() / batch_size\n",
        "\n",
        "    return top_k_accuracies\n",
        "\n",
        "def calculate_entropy(probs):\n",
        "    \"\"\"Calculate entropy of probability distributions.\"\"\"\n",
        "    # Add small epsilon to avoid log(0)\n",
        "    epsilon = 1e-8\n",
        "    probs = probs + epsilon\n",
        "    entropy = -torch.sum(probs * torch.log(probs), dim=1)\n",
        "    return entropy\n",
        "\n",
        "def calculate_ece(confidences, accuracies, n_bins=10):\n",
        "    \"\"\"Calculate Expected Calibration Error.\"\"\"\n",
        "    bin_boundaries = torch.linspace(0, 1, n_bins + 1)\n",
        "    bin_lowers = bin_boundaries[:-1]\n",
        "    bin_uppers = bin_boundaries[1:]\n",
        "\n",
        "    ece = 0\n",
        "    for bin_lower, bin_upper in zip(bin_lowers, bin_uppers):\n",
        "        in_bin = (confidences > bin_lower.item()) & (confidences <= bin_upper.item())\n",
        "        prop_in_bin = in_bin.float().mean()\n",
        "\n",
        "        if prop_in_bin.item() > 0:\n",
        "            accuracy_in_bin = accuracies[in_bin].float().mean()\n",
        "            avg_confidence_in_bin = confidences[in_bin].mean()\n",
        "            ece += torch.abs(avg_confidence_in_bin - accuracy_in_bin) * prop_in_bin\n",
        "\n",
        "    return ece.item()\n",
        "\n",
        "def comprehensive_evaluation(model, test_loader, criterion, device):\n",
        "    \"\"\"Comprehensive evaluation with all requested metrics.\"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    all_predictions = []\n",
        "    all_labels = []\n",
        "    all_confidences = []\n",
        "    all_entropies = []\n",
        "    all_top_k_results = {1: [], 3: [], 5: []}\n",
        "    total_loss = 0.0\n",
        "    total_samples = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in tqdm(test_loader, desc=\"Evaluating\"):\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            # Convert outputs to probabilities\n",
        "            probs = torch.softmax(outputs, dim=1)\n",
        "\n",
        "            # Get predictions and confidences\n",
        "            confidences, predictions = torch.max(probs, dim=1)\n",
        "\n",
        "            # Calculate entropy\n",
        "            entropies = calculate_entropy(probs)\n",
        "\n",
        "            # Calculate top-k accuracies\n",
        "            top_k_accs = calculate_top_k_accuracy(outputs, labels, [1, 3, 5])\n",
        "\n",
        "            # Store results\n",
        "            all_predictions.extend(predictions.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "            all_confidences.extend(confidences.cpu().numpy())\n",
        "            all_entropies.extend(entropies.cpu().numpy())\n",
        "\n",
        "            for k in [1, 3, 5]:\n",
        "                all_top_k_results[k].extend([top_k_accs[k]] * labels.size(0))\n",
        "\n",
        "            total_loss += loss.item() * images.size(0)\n",
        "            total_samples += images.size(0)\n",
        "\n",
        "    # Convert to numpy arrays\n",
        "    all_predictions = np.array(all_predictions)\n",
        "    all_labels = np.array(all_labels)\n",
        "    all_confidences = np.array(all_confidences)\n",
        "    all_entropies = np.array(all_entropies)\n",
        "\n",
        "    # Calculate basic metrics\n",
        "    avg_loss = total_loss / total_samples\n",
        "\n",
        "    # Calculate top-k accuracies\n",
        "    top1 = np.mean([pred == label for pred, label in zip(all_predictions, all_labels)])\n",
        "    top3 = np.mean(all_top_k_results[3])\n",
        "    top5 = np.mean(all_top_k_results[5])\n",
        "\n",
        "    # Calculate precision, recall, f1-score\n",
        "    precision_macro, recall_macro, f1_macro, _ = precision_recall_fscore_support(\n",
        "        all_labels, all_predictions, average='macro', zero_division=0\n",
        "    )\n",
        "    precision_weighted, recall_weighted, f1_weighted, _ = precision_recall_fscore_support(\n",
        "        all_labels, all_predictions, average='weighted', zero_division=0\n",
        "    )\n",
        "\n",
        "    # Calculate confidence and calibration metrics\n",
        "    avg_confidence = np.mean(all_confidences)\n",
        "    std_confidence = np.std(all_confidences)\n",
        "    avg_entropy = np.mean(all_entropies)\n",
        "\n",
        "    # Calculate ECE\n",
        "    accuracies = (all_predictions == all_labels).astype(float)\n",
        "    ece = calculate_ece(torch.tensor(all_confidences), torch.tensor(accuracies))\n",
        "\n",
        "    return {\n",
        "        'test_loss': avg_loss,\n",
        "        'top1': top1,\n",
        "        'top3': top3,\n",
        "        'top5': top5,\n",
        "        'precision_macro': precision_macro,\n",
        "        'recall_macro': recall_macro,\n",
        "        'f1_macro': f1_macro,\n",
        "        'precision_weighted': precision_weighted,\n",
        "        'recall_weighted': recall_weighted,\n",
        "        'f1_weighted': f1_weighted,\n",
        "        'avg_confidence': avg_confidence,\n",
        "        'std_confidence': std_confidence,\n",
        "        'avg_entropy': avg_entropy,\n",
        "        'ece': ece\n",
        "    }\n",
        "\n",
        "# ==========================================\n",
        "# Main Execution\n",
        "# ==========================================\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # Set global seeds for full reproducibility\n",
        "    torch.manual_seed(CONFIG[\"split_seed\"])\n",
        "    np.random.seed(CONFIG[\"split_seed\"])\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(CONFIG[\"split_seed\"])\n",
        "        torch.cuda.manual_seed_all(CONFIG[\"split_seed\"])\n",
        "        # Ensure deterministic behavior on CUDA\n",
        "        torch.backends.cudnn.deterministic = True\n",
        "        torch.backends.cudnn.benchmark = False\n",
        "\n",
        "    print(f\"Using device: {CONFIG['device']}\")\n",
        "    print(f\"Reproducibility seed: {CONFIG['split_seed']}\")\n",
        "\n",
        "    # Get data loaders\n",
        "    dataloaders = get_dataloaders(CONFIG)\n",
        "    if dataloaders is None:\n",
        "        print(\"Could not prepare data. Halting execution.\", file=sys.stderr)\n",
        "        sys.exit(1)  # Exit if data preparation failed\n",
        "\n",
        "    train_loader, val_loader, test_loader = dataloaders\n",
        "\n",
        "    # Initialize and train model\n",
        "    model = ResNetTransformer18(num_classes=CONFIG[\"num_classes\"], t_config=TRANSFORMER_CONFIG)\n",
        "    trained_model = train_and_validate(model, train_loader, val_loader, CONFIG)\n",
        "\n",
        "    # Comprehensive evaluation on the test set\n",
        "    print(\"--- Starting Comprehensive Evaluation on Test Set ---\")\n",
        "    results = comprehensive_evaluation(trained_model, test_loader, nn.CrossEntropyLoss(), CONFIG[\"device\"])\n",
        "\n",
        "    # Print all requested metrics\n",
        "    print(\"\\n--- Top-K Accuracy Results ---\")\n",
        "    print(f\"Top-1 Accuracy: {results['top1'] * 100:.2f}%\")\n",
        "    print(f\"Top-3 Accuracy: {results['top3'] * 100:.2f}%\")\n",
        "    print(f\"Top-5 Accuracy: {results['top5'] * 100:.2f}%\")\n",
        "\n",
        "    print(\"\\n--- Additional Performance Metrics ---\")\n",
        "    print(f\"Macro Average Precision: {results['precision_macro']:.4f}\")\n",
        "    print(f\"Macro Average Recall: {results['recall_macro']:.4f}\")\n",
        "    print(f\"Macro Average F1-Score: {results['f1_macro']:.4f}\")\n",
        "    print(f\"Weighted Average Precision: {results['precision_weighted']:.4f}\")\n",
        "    print(f\"Weighted Average Recall: {results['recall_weighted']:.4f}\")\n",
        "    print(f\"Weighted Average F1-Score: {results['f1_weighted']:.4f}\")\n",
        "\n",
        "    print(\"\\n--- Model Confidence & Calibration Metrics ---\")\n",
        "    print(f\"Average Prediction Confidence: {results['avg_confidence']:.4f}\")\n",
        "    print(f\"Confidence Standard Deviation: {results['std_confidence']:.4f}\")\n",
        "    print(f\"Average Prediction Entropy: {results['avg_entropy']:.4f}\")\n",
        "    print(f\"Expected Calibration Error: {results['ece']:.4f}\")\n",
        "\n",
        "    print(f\"\\nFinal Test Loss: {results['test_loss']:.4f}\")\n",
        "    print(\"===============================================\")"
      ],
      "metadata": {
        "id": "psiyb4x-p6xo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b57da5cd-36ba-495b-bd22-10cf04c8c7a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Reproducibility seed: 42\n",
            "Downloading train data from http://ufldl.stanford.edu/housenumbers/train_32x32.mat...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "SVHN train: 100%|██████████| 182M/182M [00:19<00:00, 9.43MiB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train data downloaded successfully.\n",
            "Downloading test data from http://ufldl.stanford.edu/housenumbers/test_32x32.mat...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "SVHN test: 100%|██████████| 64.3M/64.3M [00:15<00:00, 4.19MiB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test data downloaded successfully.\n",
            "Downloading extra data from http://ufldl.stanford.edu/housenumbers/extra_32x32.mat...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "SVHN extra: 100%|██████████| 1.33G/1.33G [02:36<00:00, 8.51MiB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "extra data downloaded successfully.\n",
            "Loaded 73257 samples\n",
            "Data shape: (73257, 32, 32, 3)\n",
            "Labels range: 0 to 9\n",
            "Label distribution: [ 4948 13861 10585  8497  7458  6882  5727  5595  5045  4659]\n",
            "Loaded 26032 samples\n",
            "Data shape: (26032, 32, 32, 3)\n",
            "Labels range: 0 to 9\n",
            "Label distribution: [1744 5099 4149 2882 2523 2384 1977 2019 1660 1595]\n",
            "Loaded 531131 samples\n",
            "Data shape: (531131, 32, 32, 3)\n",
            "Labels range: 0 to 9\n",
            "Label distribution: [45550 90560 74740 60765 50633 53490 41582 43997 35358 34456]\n",
            "Total merged dataset size: 630420\n",
            "  - Train: 73257 samples\n",
            "  - Test: 26032 samples\n",
            "  - Extra: 531131 samples\n",
            "Dataset splits - Train: 441294, Val: 94563, Test: 94563, Total: 630420\n",
            "\n",
            "--- Starting Training ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100 | Time: 196.51s | Train Loss: 0.2948, Train Acc: 90.86% | Val Loss: 0.1490, Val Acc: 95.61%\n",
            "\n",
            "Model saved to ./best_svhn_model.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/100 | Time: 192.62s | Train Loss: 0.1263, Train Acc: 96.41% | Val Loss: 0.1133, Val Acc: 96.81%\n",
            "\n",
            "Model saved to ./best_svhn_model.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/100 | Time: 192.67s | Train Loss: 0.1107, Train Acc: 96.91% | Val Loss: 0.1027, Val Acc: 97.12%\n",
            "\n",
            "Model saved to ./best_svhn_model.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4/100 | Time: 195.67s | Train Loss: 0.1017, Train Acc: 97.16% | Val Loss: 0.0971, Val Acc: 97.31%\n",
            "\n",
            "Model saved to ./best_svhn_model.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5/100 | Time: 196.51s | Train Loss: 0.0959, Train Acc: 97.35% | Val Loss: 0.0978, Val Acc: 97.29%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6/100 | Time: 203.97s | Train Loss: 0.0921, Train Acc: 97.45% | Val Loss: 0.0957, Val Acc: 97.34%\n",
            "\n",
            "Model saved to ./best_svhn_model.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7/100 | Time: 196.94s | Train Loss: 0.0884, Train Acc: 97.58% | Val Loss: 0.0945, Val Acc: 97.36%\n",
            "\n",
            "Model saved to ./best_svhn_model.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8/100 | Time: 195.49s | Train Loss: 0.0859, Train Acc: 97.64% | Val Loss: 0.1027, Val Acc: 97.24%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9/100 | Time: 198.16s | Train Loss: 0.0835, Train Acc: 97.70% | Val Loss: 0.0994, Val Acc: 97.28%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10/100 | Time: 196.44s | Train Loss: 0.0820, Train Acc: 97.75% | Val Loss: 0.0964, Val Acc: 97.35%\n",
            "\n",
            "Reducing learning rate from 1.00e-03 to 2.00e-04\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 11/100 | Time: 194.67s | Train Loss: 0.0539, Train Acc: 98.62% | Val Loss: 0.0716, Val Acc: 98.13%\n",
            "\n",
            "Model saved to ./best_svhn_model.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 12/100 | Time: 194.53s | Train Loss: 0.0457, Train Acc: 98.86% | Val Loss: 0.0720, Val Acc: 98.15%\n",
            "\n",
            "Model saved to ./best_svhn_model.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 13/100 | Time: 195.54s | Train Loss: 0.0410, Train Acc: 99.01% | Val Loss: 0.0750, Val Acc: 98.13%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 14/100 | Time: 193.14s | Train Loss: 0.0377, Train Acc: 99.10% | Val Loss: 0.0777, Val Acc: 98.09%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 15/100 | Time: 193.97s | Train Loss: 0.0350, Train Acc: 99.19% | Val Loss: 0.0775, Val Acc: 98.05%\n",
            "\n",
            "Reducing learning rate from 2.00e-04 to 4.00e-05\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 16/100 | Time: 193.64s | Train Loss: 0.0254, Train Acc: 99.49% | Val Loss: 0.0807, Val Acc: 98.16%\n",
            "\n",
            "Model saved to ./best_svhn_model.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 17/100 | Time: 193.02s | Train Loss: 0.0224, Train Acc: 99.57% | Val Loss: 0.0841, Val Acc: 98.14%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 18/100 | Time: 192.58s | Train Loss: 0.0205, Train Acc: 99.62% | Val Loss: 0.0870, Val Acc: 98.13%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 19/100 | Time: 192.03s | Train Loss: 0.0189, Train Acc: 99.66% | Val Loss: 0.0896, Val Acc: 98.10%\n",
            "\n",
            "Reducing learning rate from 4.00e-05 to 8.00e-06\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 20/100 | Time: 191.99s | Train Loss: 0.0165, Train Acc: 99.73% | Val Loss: 0.0912, Val Acc: 98.09%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 21/100 | Time: 192.70s | Train Loss: 0.0160, Train Acc: 99.75% | Val Loss: 0.0917, Val Acc: 98.10%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 22/100 | Time: 193.25s | Train Loss: 0.0155, Train Acc: 99.76% | Val Loss: 0.0935, Val Acc: 98.07%\n",
            "\n",
            "Reducing learning rate from 8.00e-06 to 1.60e-06\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 23/100 | Time: 197.30s | Train Loss: 0.0149, Train Acc: 99.77% | Val Loss: 0.0933, Val Acc: 98.08%\n",
            "\n",
            "Early stopping at epoch 23\n",
            "Restored model weights from the end of the best epoch: 16\n",
            "--- Training Finished ---\n",
            "\n",
            "--- Starting Comprehensive Evaluation on Test Set ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating: 100%|██████████| 739/739 [00:25<00:00, 29.23it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Top-K Accuracy Results ---\n",
            "Top-1 Accuracy: 98.12%\n",
            "Top-3 Accuracy: 99.47%\n",
            "Top-5 Accuracy: 99.73%\n",
            "\n",
            "--- Additional Performance Metrics ---\n",
            "Macro Average Precision: 0.9807\n",
            "Macro Average Recall: 0.9804\n",
            "Macro Average F1-Score: 0.9806\n",
            "Weighted Average Precision: 0.9812\n",
            "Weighted Average Recall: 0.9812\n",
            "Weighted Average F1-Score: 0.9812\n",
            "\n",
            "--- Model Confidence & Calibration Metrics ---\n",
            "Average Prediction Confidence: 0.9935\n",
            "Confidence Standard Deviation: 0.0439\n",
            "Average Prediction Entropy: 0.0231\n",
            "Expected Calibration Error: 0.0123\n",
            "\n",
            "Final Test Loss: 0.0909\n",
            "===============================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hybrid Transformer on ResNet-18 (Learnable PE)"
      ],
      "metadata": {
        "id": "bfxny2ijqBUO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "import torchvision.transforms as transforms\n",
        "import scipy.io\n",
        "from PIL import Image\n",
        "import os\n",
        "import requests\n",
        "import zipfile\n",
        "import shutil\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "import sys\n",
        "import numpy as np\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# --- Configuration Parameters ---\n",
        "CONFIG = {\n",
        "    \"data_path\": \"./svhn-data/\",\n",
        "    \"batch_size\": 128,\n",
        "    \"num_epochs\": 100,\n",
        "    \"learning_rate\": 0.001,\n",
        "    \"weight_decay\": 1e-4,\n",
        "    \"split_seed\": 42,\n",
        "    \"train_split\": 0.70,\n",
        "    \"val_split\": 0.15,\n",
        "    \"test_split\": 0.15,\n",
        "    \"num_classes\": 10,    # SVHN has 10 classes (digits 0-9)\n",
        "    \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
        "    \"checkpoint_path\": \"./best_svhn_model.pth\",\n",
        "}\n",
        "\n",
        "# --- Transformer Configuration ---\n",
        "TRANSFORMER_CONFIG = {\n",
        "    \"embedding_dim\": 256,       # Dimension of the tokens fed to the transformer\n",
        "    \"nhead\": 8,                 # Number of attention heads\n",
        "    \"num_encoder_layers\": 3,    # Number of transformer encoder layers\n",
        "    \"dim_feedforward\": 512,     # Hidden dimension in the feed-forward network\n",
        "    \"dropout\": 0.1,\n",
        "}\n",
        "\n",
        "# ==========================================\n",
        "# Callback Classes\n",
        "# ==========================================\n",
        "\n",
        "class Callback:\n",
        "    \"\"\"Base callback class\"\"\"\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        pass\n",
        "\n",
        "    def on_training_end(self):\n",
        "        pass\n",
        "\n",
        "class ReduceLROnPlateau(Callback):\n",
        "    \"\"\"Reduce learning rate when a metric has stopped improving\"\"\"\n",
        "    def __init__(self, optimizer, monitor='val_accuracy', factor=0.2, patience=3, min_lr=1e-7, verbose=1):\n",
        "        self.optimizer = optimizer\n",
        "        self.monitor = monitor\n",
        "        self.factor = factor\n",
        "        self.patience = patience\n",
        "        self.min_lr = min_lr\n",
        "        self.verbose = verbose\n",
        "        self.wait = 0\n",
        "        self.best = None\n",
        "        self.mode = 'min' if 'loss' in monitor else 'max'\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        if logs is None:\n",
        "            return\n",
        "\n",
        "        current = logs.get(self.monitor)\n",
        "        if current is None:\n",
        "            return\n",
        "\n",
        "        if self.best is None:\n",
        "            self.best = current\n",
        "            return\n",
        "\n",
        "        if self.mode == 'min':\n",
        "            improved = current < self.best\n",
        "        else:\n",
        "            improved = current > self.best\n",
        "\n",
        "        if improved:\n",
        "            self.best = current\n",
        "            self.wait = 0\n",
        "        else:\n",
        "            self.wait += 1\n",
        "            if self.wait >= self.patience:\n",
        "                old_lr = self.optimizer.param_groups[0]['lr']\n",
        "                new_lr = max(old_lr * self.factor, self.min_lr)\n",
        "                if new_lr != old_lr:\n",
        "                    self.optimizer.param_groups[0]['lr'] = new_lr\n",
        "                    if self.verbose:\n",
        "                        print(f\"\\nReducing learning rate from {old_lr:.2e} to {new_lr:.2e}\")\n",
        "                    self.wait = 0\n",
        "\n",
        "class EarlyStopping(Callback):\n",
        "    \"\"\"Stop training when a monitored metric has stopped improving\"\"\"\n",
        "    def __init__(self, monitor='val_accuracy', patience=7, restore_best_weights=True, verbose=1):\n",
        "        self.monitor = monitor\n",
        "        self.patience = patience\n",
        "        self.restore_best_weights = restore_best_weights\n",
        "        self.verbose = verbose\n",
        "        self.wait = 0\n",
        "        self.best = None\n",
        "        self.best_weights = None\n",
        "        self.mode = 'min' if 'loss' in monitor else 'max'\n",
        "        self.stopped_epoch = 0\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None, model=None):\n",
        "        if logs is None:\n",
        "            return False\n",
        "\n",
        "        current = logs.get(self.monitor)\n",
        "        if current is None:\n",
        "            return False\n",
        "\n",
        "        if self.best is None:\n",
        "            self.best = current\n",
        "            if model is not None and self.restore_best_weights:\n",
        "                self.best_weights = model.state_dict().copy()\n",
        "            return False\n",
        "\n",
        "        if self.mode == 'min':\n",
        "            improved = current < self.best\n",
        "        else:\n",
        "            improved = current > self.best\n",
        "\n",
        "        if improved:\n",
        "            self.best = current\n",
        "            self.wait = 0\n",
        "            if model is not None and self.restore_best_weights:\n",
        "                self.best_weights = model.state_dict().copy()\n",
        "        else:\n",
        "            self.wait += 1\n",
        "            if self.wait >= self.patience:\n",
        "                self.stopped_epoch = epoch\n",
        "                if self.verbose:\n",
        "                    print(f\"\\nEarly stopping at epoch {epoch + 1}\")\n",
        "                return True\n",
        "        return False\n",
        "\n",
        "    def on_training_end(self, model=None):\n",
        "        if self.stopped_epoch > 0 and self.verbose:\n",
        "            print(f\"Restored model weights from the end of the best epoch: {self.stopped_epoch + 1 - self.patience}\")\n",
        "        if model is not None and self.restore_best_weights and self.best_weights is not None:\n",
        "            model.load_state_dict(self.best_weights)\n",
        "\n",
        "class ModelCheckpoint(Callback):\n",
        "    \"\"\"Save the model after every epoch\"\"\"\n",
        "    def __init__(self, filepath, save_best_only=True, monitor='val_accuracy', verbose=1):\n",
        "        self.filepath = filepath\n",
        "        self.save_best_only = save_best_only\n",
        "        self.monitor = monitor\n",
        "        self.verbose = verbose\n",
        "        self.best = None\n",
        "        self.mode = 'min' if 'loss' in monitor else 'max'\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None, model=None):\n",
        "        if logs is None or model is None:\n",
        "            return\n",
        "\n",
        "        current = logs.get(self.monitor)\n",
        "        if current is None:\n",
        "            return\n",
        "\n",
        "        if not self.save_best_only:\n",
        "            filepath = self.filepath.replace('.pth', f'_epoch_{epoch + 1}.pth')\n",
        "            torch.save(model.state_dict(), filepath)\n",
        "            if self.verbose:\n",
        "                print(f\"\\nModel saved to {filepath}\")\n",
        "            return\n",
        "\n",
        "        if self.best is None:\n",
        "            self.best = current\n",
        "            torch.save(model.state_dict(), self.filepath)\n",
        "            if self.verbose:\n",
        "                print(f\"\\nModel saved to {self.filepath}\")\n",
        "            return\n",
        "\n",
        "        if self.mode == 'min':\n",
        "            improved = current < self.best\n",
        "        else:\n",
        "            improved = current > self.best\n",
        "\n",
        "        if improved:\n",
        "            self.best = current\n",
        "            torch.save(model.state_dict(), self.filepath)\n",
        "            if self.verbose:\n",
        "                print(f\"\\nModel saved to {self.filepath}\")\n",
        "\n",
        "# ==========================================\n",
        "# Data Preprocessing\n",
        "# ==========================================\n",
        "\n",
        "def download_svhn_data(data_path):\n",
        "    \"\"\"Downloads SVHN dataset files.\"\"\"\n",
        "    os.makedirs(data_path, exist_ok=True)\n",
        "\n",
        "    urls = {\n",
        "        'train': 'http://ufldl.stanford.edu/housenumbers/train_32x32.mat',\n",
        "        'test': 'http://ufldl.stanford.edu/housenumbers/test_32x32.mat',\n",
        "        'extra': 'http://ufldl.stanford.edu/housenumbers/extra_32x32.mat'\n",
        "    }\n",
        "\n",
        "    downloaded_files = {}\n",
        "\n",
        "    for split, url in urls.items():\n",
        "        file_path = os.path.join(data_path, f'{split}_32x32.mat')\n",
        "\n",
        "        if os.path.exists(file_path):\n",
        "            print(f\"{split} data already exists.\")\n",
        "            downloaded_files[split] = file_path\n",
        "            continue\n",
        "\n",
        "        print(f\"Downloading {split} data from {url}...\")\n",
        "        try:\n",
        "            with requests.get(url, stream=True, timeout=60) as r:\n",
        "                r.raise_for_status()\n",
        "                total_size = int(r.headers.get('content-length', 0))\n",
        "                with open(file_path, 'wb') as f, tqdm(\n",
        "                    total=total_size, unit='iB', unit_scale=True, desc=f\"SVHN {split}\"\n",
        "                ) as progress_bar:\n",
        "                    for chunk in r.iter_content(chunk_size=8192):\n",
        "                        f.write(chunk)\n",
        "                        progress_bar.update(len(chunk))\n",
        "            downloaded_files[split] = file_path\n",
        "            print(f\"{split} data downloaded successfully.\")\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            print(f\"Error downloading {split} data: {e}\", file=sys.stderr)\n",
        "            return None\n",
        "\n",
        "    return downloaded_files\n",
        "\n",
        "class SVHNDataset(Dataset):\n",
        "    \"\"\"Custom SVHN Dataset class.\"\"\"\n",
        "    def __init__(self, mat_file_path, transform=None):\n",
        "        self.transform = transform\n",
        "\n",
        "        # Load .mat file\n",
        "        mat_data = scipy.io.loadmat(mat_file_path)\n",
        "\n",
        "        # Extract data and labels\n",
        "        # SVHN format: X is (32, 32, 3, N), y is (N, 1)\n",
        "        self.data = mat_data['X'].transpose((3, 0, 1, 2))  # Convert to (N, 32, 32, 3)\n",
        "        self.labels = mat_data['y'].flatten()\n",
        "\n",
        "        # Convert labels: SVHN uses 1-10, we need 0-9\n",
        "        # Label 10 represents digit '0', labels 1-9 represent digits '1'-'9'\n",
        "        self.labels = self.labels % 10  # Convert 10 -> 0, keep 1-9 as is\n",
        "\n",
        "        # Ensure labels are in correct range\n",
        "        assert self.labels.min() >= 0 and self.labels.max() <= 9, f\"Labels should be 0-9, got {self.labels.min()}-{self.labels.max()}\"\n",
        "\n",
        "        print(f\"Loaded {len(self.data)} samples\")\n",
        "        print(f\"Data shape: {self.data.shape}\")\n",
        "        print(f\"Labels range: {self.labels.min()} to {self.labels.max()}\")\n",
        "        print(f\"Label distribution: {np.bincount(self.labels)}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Get image and label\n",
        "        image = self.data[idx]\n",
        "        label = int(self.labels[idx])  # Ensure label is int\n",
        "\n",
        "        # Convert numpy array to PIL Image\n",
        "        image = Image.fromarray(image.astype('uint8'))\n",
        "\n",
        "        # Apply transforms\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, label\n",
        "\n",
        "def get_dataloaders(config):\n",
        "    \"\"\"Downloads, prepares, and splits the SVHN data, returning DataLoaders.\"\"\"\n",
        "\n",
        "    # Download SVHN data\n",
        "    downloaded_files = download_svhn_data(config[\"data_path\"])\n",
        "    if downloaded_files is None:\n",
        "        return None\n",
        "\n",
        "    # Define transforms\n",
        "    transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.4377, 0.4438, 0.4728], std=[0.1980, 0.2010, 0.1970]),  # SVHN stats\n",
        "    ])\n",
        "\n",
        "    # Load all datasets\n",
        "    train_dataset = SVHNDataset(downloaded_files['train'], transform=transform)\n",
        "    test_dataset = SVHNDataset(downloaded_files['test'], transform=transform)\n",
        "    extra_dataset = SVHNDataset(downloaded_files['extra'], transform=transform)\n",
        "\n",
        "    # Merge all datasets together\n",
        "    full_dataset = torch.utils.data.ConcatDataset([train_dataset, test_dataset, extra_dataset])\n",
        "    print(f\"Total merged dataset size: {len(full_dataset)}\")\n",
        "    print(f\"  - Train: {len(train_dataset)} samples\")\n",
        "    print(f\"  - Test: {len(test_dataset)} samples\")\n",
        "    print(f\"  - Extra: {len(extra_dataset)} samples\")\n",
        "\n",
        "    # Ensure reproducible splits\n",
        "    torch.manual_seed(config[\"split_seed\"])\n",
        "    np.random.seed(config[\"split_seed\"])\n",
        "\n",
        "    total_size = len(full_dataset)\n",
        "    train_size = int(config[\"train_split\"] * total_size)\n",
        "    val_size = int(config[\"val_split\"] * total_size)\n",
        "    test_size = int(config[\"test_split\"] * total_size)\n",
        "\n",
        "    # Adjust sizes to ensure they sum to total_size (handle rounding issues)\n",
        "    actual_total = train_size + val_size + test_size\n",
        "    if actual_total != total_size:\n",
        "        # Add/subtract the difference to test_size\n",
        "        test_size += (total_size - actual_total)\n",
        "\n",
        "    print(f\"Dataset splits - Train: {train_size}, Val: {val_size}, Test: {test_size}, Total: {total_size}\")\n",
        "\n",
        "    # Create reproducible generator for splitting\n",
        "    generator = torch.Generator().manual_seed(config[\"split_seed\"])\n",
        "    train_subset, val_subset, test_subset = random_split(\n",
        "        full_dataset, [train_size, val_size, test_size], generator=generator\n",
        "    )\n",
        "\n",
        "    # Set worker init function for reproducible DataLoader behavior\n",
        "    def worker_init_fn(worker_id):\n",
        "        np.random.seed(config[\"split_seed\"] + worker_id)\n",
        "\n",
        "    # Create data loaders\n",
        "    train_loader = DataLoader(\n",
        "        train_subset,\n",
        "        batch_size=config[\"batch_size\"],\n",
        "        shuffle=True,\n",
        "        num_workers=2,\n",
        "        pin_memory=True,\n",
        "        worker_init_fn=worker_init_fn,\n",
        "        generator=torch.Generator().manual_seed(config[\"split_seed\"])\n",
        "    )\n",
        "\n",
        "    val_loader = DataLoader(\n",
        "        val_subset,\n",
        "        batch_size=config[\"batch_size\"],\n",
        "        shuffle=False,\n",
        "        num_workers=2,\n",
        "        pin_memory=True,\n",
        "        worker_init_fn=worker_init_fn\n",
        "    )\n",
        "\n",
        "    test_loader = DataLoader(\n",
        "        test_subset,\n",
        "        batch_size=config[\"batch_size\"],\n",
        "        shuffle=False,\n",
        "        num_workers=2,\n",
        "        pin_memory=True,\n",
        "        worker_init_fn=worker_init_fn\n",
        "    )\n",
        "\n",
        "    return train_loader, val_loader, test_loader\n",
        "\n",
        "# ==========================================\n",
        "# Model Architecture and Training\n",
        "# ==========================================\n",
        "\n",
        "class NonResidualBlock(nn.Module):\n",
        "    \"\"\"A standard convolutional block WITHOUT the residual connection.\"\"\"\n",
        "    def __init__(self, in_channels, out_channels, stride=1):\n",
        "        super(NonResidualBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        out = self.relu(out)\n",
        "        return out\n",
        "\n",
        "class ResNetTransformer(nn.Module):\n",
        "    \"\"\"\n",
        "    A hybrid architecture combining a non-residual CNN backbone with a Transformer encoder,\n",
        "    including positional embeddings.\n",
        "    \"\"\"\n",
        "    def __init__(self, block, layers, num_classes, t_config):\n",
        "        super(ResNetTransformer, self).__init__()\n",
        "        self.in_channels = 64\n",
        "        self.embedding_dim = t_config[\"embedding_dim\"]\n",
        "        self.num_tokens = len(layers) + 1  # 4 layers + 1 initial capture\n",
        "\n",
        "        # 1. CNN Backbone (Feature Extractor)\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "        self.layer1 = self._make_layer(block, 64, layers[0], stride=1)\n",
        "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n",
        "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n",
        "\n",
        "        # 2. Projection heads to create tokens from feature maps\n",
        "        self.projections = nn.ModuleList([\n",
        "            self._create_projection(64, self.embedding_dim),\n",
        "            self._create_projection(64, self.embedding_dim),\n",
        "            self._create_projection(128, self.embedding_dim),\n",
        "            self._create_projection(256, self.embedding_dim),\n",
        "            self._create_projection(512, self.embedding_dim)\n",
        "        ])\n",
        "\n",
        "        # 3. Learnable Positional Embedding\n",
        "        self.positional_embedding = nn.Parameter(torch.randn(1, self.num_tokens, self.embedding_dim))\n",
        "\n",
        "        # 4. Transformer Encoder\n",
        "        encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=self.embedding_dim,\n",
        "            nhead=t_config[\"nhead\"],\n",
        "            dim_feedforward=t_config[\"dim_feedforward\"],\n",
        "            dropout=t_config[\"dropout\"],\n",
        "            batch_first=True\n",
        "        )\n",
        "        self.transformer_encoder = nn.TransformerEncoder(\n",
        "            encoder_layer,\n",
        "            num_layers=t_config[\"num_encoder_layers\"]\n",
        "        )\n",
        "\n",
        "        # 5. Final Classifier\n",
        "        self.classifier = nn.Linear(self.embedding_dim, num_classes)\n",
        "\n",
        "    def _create_projection(self, in_features, out_features):\n",
        "        return nn.Sequential(\n",
        "            nn.AdaptiveAvgPool2d((1, 1)),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(in_features, out_features)\n",
        "        )\n",
        "\n",
        "    def _make_layer(self, block, out_channels, num_blocks, stride):\n",
        "        strides = [stride] + [1] * (num_blocks - 1)\n",
        "        layers = []\n",
        "        current_in_channels = self.in_channels\n",
        "        for s in strides:\n",
        "            layers.append(block(current_in_channels, out_channels, s))\n",
        "            current_in_channels = out_channels\n",
        "        self.in_channels = out_channels\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # 1. Pass through CNN backbone and capture features\n",
        "        features = []\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "        features.append(x)  # Capture 1: After initial maxpool\n",
        "\n",
        "        x = self.layer1(x); features.append(x)  # Capture 2: After layer1\n",
        "        x = self.layer2(x); features.append(x)  # Capture 3: After layer2\n",
        "        x = self.layer3(x); features.append(x)  # Capture 4: After layer3\n",
        "        x = self.layer4(x); features.append(x)  # Capture 5: After layer4\n",
        "\n",
        "        # 2. Project features to tokens\n",
        "        tokens = [self.projections[i](feature_map) for i, feature_map in enumerate(features)]\n",
        "\n",
        "        # 3. Stack tokens into a sequence\n",
        "        token_sequence = torch.stack(tokens, dim=1)\n",
        "\n",
        "        # 4. Add positional embedding\n",
        "        token_sequence += self.positional_embedding\n",
        "\n",
        "        # 5. Pass through Transformer\n",
        "        transformer_out = self.transformer_encoder(token_sequence)\n",
        "\n",
        "        # 6. Aggregate and classify\n",
        "        aggregated_vector = transformer_out.mean(dim=1)\n",
        "        logits = self.classifier(aggregated_vector)\n",
        "\n",
        "        return logits\n",
        "\n",
        "def evaluate_model(model, data_loader, criterion, device):\n",
        "    \"\"\"Evaluates the model on a given dataset.\"\"\"\n",
        "    model.eval()\n",
        "    total_loss, correct, total = 0.0, 0, 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in data_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            total_loss += loss.item() * images.size(0)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    avg_loss = total_loss / total\n",
        "    accuracy = 100 * correct / total\n",
        "    return avg_loss, accuracy\n",
        "\n",
        "def train_and_validate(model, train_loader, val_loader, config):\n",
        "    \"\"\"Main training loop with callbacks.\"\"\"\n",
        "    device = config[\"device\"]\n",
        "    model.to(device)\n",
        "\n",
        "    # Set seeds for reproducible training\n",
        "    torch.manual_seed(config[\"split_seed\"])\n",
        "    np.random.seed(config[\"split_seed\"])\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(config[\"split_seed\"])\n",
        "        torch.cuda.manual_seed_all(config[\"split_seed\"])\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=config[\"learning_rate\"], weight_decay=config[\"weight_decay\"])\n",
        "\n",
        "    # Initialize callbacks\n",
        "    callbacks = [\n",
        "        ReduceLROnPlateau(\n",
        "            optimizer=optimizer,\n",
        "            monitor='val_accuracy',\n",
        "            factor=0.2,\n",
        "            patience=3,\n",
        "            min_lr=1e-7,\n",
        "            verbose=1\n",
        "        ),\n",
        "        EarlyStopping(\n",
        "            monitor='val_accuracy',\n",
        "            patience=7,\n",
        "            restore_best_weights=True,\n",
        "            verbose=1\n",
        "        ),\n",
        "        ModelCheckpoint(\n",
        "            filepath=config[\"checkpoint_path\"],\n",
        "            save_best_only=True,\n",
        "            monitor='val_accuracy',\n",
        "            verbose=1\n",
        "        )\n",
        "    ]\n",
        "\n",
        "    print(\"\\n--- Starting Training ---\")\n",
        "    early_stop = False\n",
        "\n",
        "    for epoch in range(config[\"num_epochs\"]):\n",
        "        if early_stop:\n",
        "            break\n",
        "\n",
        "        model.train()\n",
        "        start_time = time.time()\n",
        "        running_loss, train_correct, train_total = 0.0, 0, 0\n",
        "\n",
        "        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{config['num_epochs']} [Train]\", leave=False)\n",
        "        for images, labels in progress_bar:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item() * images.size(0)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            train_total += labels.size(0)\n",
        "            train_correct += (predicted == labels).sum().item()\n",
        "            progress_bar.set_postfix(loss=loss.item())\n",
        "\n",
        "        train_loss = running_loss / train_total\n",
        "        train_acc = 100 * train_correct / train_total\n",
        "        val_loss, val_acc = evaluate_model(model, val_loader, criterion, device)\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{config['num_epochs']} | Time: {time.time() - start_time:.2f}s | \"\n",
        "              f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}% | \"\n",
        "              f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%\")\n",
        "\n",
        "        # Prepare logs for callbacks\n",
        "        logs = {\n",
        "            'train_loss': train_loss,\n",
        "            'train_accuracy': train_acc,\n",
        "            'val_loss': val_loss,\n",
        "            'val_accuracy': val_acc\n",
        "        }\n",
        "\n",
        "        # Execute callbacks\n",
        "        for callback in callbacks:\n",
        "            if isinstance(callback, EarlyStopping):\n",
        "                if callback.on_epoch_end(epoch, logs, model):\n",
        "                    early_stop = True\n",
        "                    break\n",
        "            elif isinstance(callback, ModelCheckpoint):\n",
        "                callback.on_epoch_end(epoch, logs, model)\n",
        "            else:\n",
        "                callback.on_epoch_end(epoch, logs)\n",
        "\n",
        "    # Execute callback cleanup\n",
        "    for callback in callbacks:\n",
        "        if isinstance(callback, EarlyStopping):\n",
        "            callback.on_training_end(model)\n",
        "        else:\n",
        "            callback.on_training_end()\n",
        "\n",
        "    print(\"--- Training Finished ---\\n\")\n",
        "    return model\n",
        "\n",
        "# ==========================================\n",
        "# Evaluation\n",
        "# ==========================================\n",
        "\n",
        "def calculate_top_k_accuracy(outputs, labels, k_values=[1, 3, 5]):\n",
        "    \"\"\"Calculate top-k accuracy for given k values.\"\"\"\n",
        "    batch_size = labels.size(0)\n",
        "    _, pred = outputs.topk(max(k_values), 1, True, True)\n",
        "    pred = pred.t()\n",
        "    correct = pred.eq(labels.view(1, -1).expand_as(pred))\n",
        "\n",
        "    top_k_accuracies = {}\n",
        "    for k in k_values:\n",
        "        correct_k = correct[:k].reshape(-1).float().sum(0, keepdim=True)\n",
        "        top_k_accuracies[k] = correct_k.item() / batch_size\n",
        "\n",
        "    return top_k_accuracies\n",
        "\n",
        "def calculate_entropy(probs):\n",
        "    \"\"\"Calculate entropy of probability distributions.\"\"\"\n",
        "    # Add small epsilon to avoid log(0)\n",
        "    epsilon = 1e-8\n",
        "    probs = probs + epsilon\n",
        "    entropy = -torch.sum(probs * torch.log(probs), dim=1)\n",
        "    return entropy\n",
        "\n",
        "def calculate_ece(confidences, accuracies, n_bins=10):\n",
        "    \"\"\"Calculate Expected Calibration Error.\"\"\"\n",
        "    bin_boundaries = torch.linspace(0, 1, n_bins + 1)\n",
        "    bin_lowers = bin_boundaries[:-1]\n",
        "    bin_uppers = bin_boundaries[1:]\n",
        "\n",
        "    ece = 0\n",
        "    for bin_lower, bin_upper in zip(bin_lowers, bin_uppers):\n",
        "        in_bin = (confidences > bin_lower.item()) & (confidences <= bin_upper.item())\n",
        "        prop_in_bin = in_bin.float().mean()\n",
        "\n",
        "        if prop_in_bin.item() > 0:\n",
        "            accuracy_in_bin = accuracies[in_bin].float().mean()\n",
        "            avg_confidence_in_bin = confidences[in_bin].mean()\n",
        "            ece += torch.abs(avg_confidence_in_bin - accuracy_in_bin) * prop_in_bin\n",
        "\n",
        "    return ece.item()\n",
        "\n",
        "def comprehensive_evaluation(model, test_loader, criterion, device):\n",
        "    \"\"\"Comprehensive evaluation with all requested metrics.\"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    all_predictions = []\n",
        "    all_labels = []\n",
        "    all_confidences = []\n",
        "    all_entropies = []\n",
        "    all_top_k_results = {1: [], 3: [], 5: []}\n",
        "    total_loss = 0.0\n",
        "    total_samples = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in tqdm(test_loader, desc=\"Evaluating\"):\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            # Convert outputs to probabilities\n",
        "            probs = torch.softmax(outputs, dim=1)\n",
        "\n",
        "            # Get predictions and confidences\n",
        "            confidences, predictions = torch.max(probs, dim=1)\n",
        "\n",
        "            # Calculate entropy\n",
        "            entropies = calculate_entropy(probs)\n",
        "\n",
        "            # Calculate top-k accuracies\n",
        "            top_k_accs = calculate_top_k_accuracy(outputs, labels, [1, 3, 5])\n",
        "\n",
        "            # Store results\n",
        "            all_predictions.extend(predictions.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "            all_confidences.extend(confidences.cpu().numpy())\n",
        "            all_entropies.extend(entropies.cpu().numpy())\n",
        "\n",
        "            for k in [1, 3, 5]:\n",
        "                all_top_k_results[k].extend([top_k_accs[k]] * labels.size(0))\n",
        "\n",
        "            total_loss += loss.item() * images.size(0)\n",
        "            total_samples += images.size(0)\n",
        "\n",
        "    # Convert to numpy arrays\n",
        "    all_predictions = np.array(all_predictions)\n",
        "    all_labels = np.array(all_labels)\n",
        "    all_confidences = np.array(all_confidences)\n",
        "    all_entropies = np.array(all_entropies)\n",
        "\n",
        "    # Calculate basic metrics\n",
        "    avg_loss = total_loss / total_samples\n",
        "\n",
        "    # Calculate top-k accuracies\n",
        "    top1 = np.mean([pred == label for pred, label in zip(all_predictions, all_labels)])\n",
        "    top3 = np.mean(all_top_k_results[3])\n",
        "    top5 = np.mean(all_top_k_results[5])\n",
        "\n",
        "    # Calculate precision, recall, f1-score\n",
        "    precision_macro, recall_macro, f1_macro, _ = precision_recall_fscore_support(\n",
        "        all_labels, all_predictions, average='macro', zero_division=0\n",
        "    )\n",
        "    precision_weighted, recall_weighted, f1_weighted, _ = precision_recall_fscore_support(\n",
        "        all_labels, all_predictions, average='weighted', zero_division=0\n",
        "    )\n",
        "\n",
        "    # Calculate confidence and calibration metrics\n",
        "    avg_confidence = np.mean(all_confidences)\n",
        "    std_confidence = np.std(all_confidences)\n",
        "    avg_entropy = np.mean(all_entropies)\n",
        "\n",
        "    # Calculate ECE\n",
        "    accuracies = (all_predictions == all_labels).astype(float)\n",
        "    ece = calculate_ece(torch.tensor(all_confidences), torch.tensor(accuracies))\n",
        "\n",
        "    return {\n",
        "        'test_loss': avg_loss,\n",
        "        'top1': top1,\n",
        "        'top3': top3,\n",
        "        'top5': top5,\n",
        "        'precision_macro': precision_macro,\n",
        "        'recall_macro': recall_macro,\n",
        "        'f1_macro': f1_macro,\n",
        "        'precision_weighted': precision_weighted,\n",
        "        'recall_weighted': recall_weighted,\n",
        "        'f1_weighted': f1_weighted,\n",
        "        'avg_confidence': avg_confidence,\n",
        "        'std_confidence': std_confidence,\n",
        "        'avg_entropy': avg_entropy,\n",
        "        'ece': ece\n",
        "    }\n",
        "\n",
        "# ==========================================\n",
        "# Main Execution\n",
        "# ==========================================\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # Set global seeds for full reproducibility\n",
        "    torch.manual_seed(CONFIG[\"split_seed\"])\n",
        "    np.random.seed(CONFIG[\"split_seed\"])\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(CONFIG[\"split_seed\"])\n",
        "        torch.cuda.manual_seed_all(CONFIG[\"split_seed\"])\n",
        "        # Ensure deterministic behavior on CUDA\n",
        "        torch.backends.cudnn.deterministic = True\n",
        "        torch.backends.cudnn.benchmark = False\n",
        "\n",
        "    print(f\"Using device: {CONFIG['device']}\")\n",
        "    print(f\"Reproducibility seed: {CONFIG['split_seed']}\")\n",
        "\n",
        "    # Get data loaders\n",
        "    dataloaders = get_dataloaders(CONFIG)\n",
        "    if dataloaders is None:\n",
        "        print(\"Could not prepare data. Halting execution.\", file=sys.stderr)\n",
        "        sys.exit(1)  # Exit if data preparation failed\n",
        "\n",
        "    train_loader, val_loader, test_loader = dataloaders\n",
        "\n",
        "    # Initialize and train model with ResNetTransformer\n",
        "    model = ResNetTransformer(\n",
        "        block=NonResidualBlock,\n",
        "        layers=[2, 2, 2, 2],\n",
        "        num_classes=CONFIG[\"num_classes\"],\n",
        "        t_config=TRANSFORMER_CONFIG\n",
        "    )\n",
        "\n",
        "    print(f\"Model has {sum(p.numel() for p in model.parameters())} parameters\")\n",
        "    print(f\"Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad)}\")\n",
        "\n",
        "    trained_model = train_and_validate(model, train_loader, val_loader, CONFIG)\n",
        "\n",
        "    # Comprehensive evaluation on the test set\n",
        "    print(\"--- Starting Comprehensive Evaluation on Test Set ---\")\n",
        "    results = comprehensive_evaluation(trained_model, test_loader, nn.CrossEntropyLoss(), CONFIG[\"device\"])\n",
        "\n",
        "    # Print all requested metrics\n",
        "    print(\"\\n--- Top-K Accuracy Results ---\")\n",
        "    print(f\"Top-1 Accuracy: {results['top1'] * 100:.2f}%\")\n",
        "    print(f\"Top-3 Accuracy: {results['top3'] * 100:.2f}%\")\n",
        "    print(f\"Top-5 Accuracy: {results['top5'] * 100:.2f}%\")\n",
        "\n",
        "    print(\"\\n--- Additional Performance Metrics ---\")\n",
        "    print(f\"Macro Average Precision: {results['precision_macro']:.4f}\")\n",
        "    print(f\"Macro Average Recall: {results['recall_macro']:.4f}\")\n",
        "    print(f\"Macro Average F1-Score: {results['f1_macro']:.4f}\")\n",
        "    print(f\"Weighted Average Precision: {results['precision_weighted']:.4f}\")\n",
        "    print(f\"Weighted Average Recall: {results['recall_weighted']:.4f}\")\n",
        "    print(f\"Weighted Average F1-Score: {results['f1_weighted']:.4f}\")\n",
        "\n",
        "    print(\"\\n--- Model Confidence & Calibration Metrics ---\")\n",
        "    print(f\"Average Prediction Confidence: {results['avg_confidence']:.4f}\")\n",
        "    print(f\"Confidence Standard Deviation: {results['std_confidence']:.4f}\")\n",
        "    print(f\"Average Prediction Entropy: {results['avg_entropy']:.4f}\")\n",
        "    print(f\"Expected Calibration Error: {results['ece']:.4f}\")\n",
        "\n",
        "    print(f\"\\nFinal Test Loss: {results['test_loss']:.4f}\")\n",
        "    print(\"===============================================\")"
      ],
      "metadata": {
        "id": "g_nIVFM3qDwE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d87c541-7eb5-4470-def3-219ea36d18ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Reproducibility seed: 42\n",
            "Downloading train data from http://ufldl.stanford.edu/housenumbers/train_32x32.mat...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "SVHN train: 100%|██████████| 182M/182M [00:17<00:00, 10.6MiB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train data downloaded successfully.\n",
            "Downloading test data from http://ufldl.stanford.edu/housenumbers/test_32x32.mat...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "SVHN test: 100%|██████████| 64.3M/64.3M [00:10<00:00, 6.10MiB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test data downloaded successfully.\n",
            "Downloading extra data from http://ufldl.stanford.edu/housenumbers/extra_32x32.mat...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "SVHN extra: 100%|██████████| 1.33G/1.33G [01:34<00:00, 14.1MiB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "extra data downloaded successfully.\n",
            "Loaded 73257 samples\n",
            "Data shape: (73257, 32, 32, 3)\n",
            "Labels range: 0 to 9\n",
            "Label distribution: [ 4948 13861 10585  8497  7458  6882  5727  5595  5045  4659]\n",
            "Loaded 26032 samples\n",
            "Data shape: (26032, 32, 32, 3)\n",
            "Labels range: 0 to 9\n",
            "Label distribution: [1744 5099 4149 2882 2523 2384 1977 2019 1660 1595]\n",
            "Loaded 531131 samples\n",
            "Data shape: (531131, 32, 32, 3)\n",
            "Labels range: 0 to 9\n",
            "Label distribution: [45550 90560 74740 60765 50633 53490 41582 43997 35358 34456]\n",
            "Total merged dataset size: 630420\n",
            "  - Train: 73257 samples\n",
            "  - Test: 26032 samples\n",
            "  - Extra: 531131 samples\n",
            "Dataset splits - Train: 441294, Val: 94563, Test: 94563, Total: 630420\n",
            "Model has 12851274 parameters\n",
            "Trainable parameters: 12851274\n",
            "\n",
            "--- Starting Training ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100 | Time: 217.24s | Train Loss: 0.3636, Train Acc: 88.62% | Val Loss: 0.1529, Val Acc: 95.63%\n",
            "\n",
            "Model saved to ./best_svhn_model.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/100 | Time: 213.82s | Train Loss: 0.1406, Train Acc: 96.07% | Val Loss: 0.1345, Val Acc: 96.32%\n",
            "\n",
            "Model saved to ./best_svhn_model.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/100 | Time: 212.67s | Train Loss: 0.1191, Train Acc: 96.71% | Val Loss: 0.1144, Val Acc: 96.86%\n",
            "\n",
            "Model saved to ./best_svhn_model.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4/100 | Time: 213.49s | Train Loss: 0.1068, Train Acc: 97.04% | Val Loss: 0.1048, Val Acc: 97.12%\n",
            "\n",
            "Model saved to ./best_svhn_model.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5/100 | Time: 214.89s | Train Loss: 0.1009, Train Acc: 97.24% | Val Loss: 0.0986, Val Acc: 97.32%\n",
            "\n",
            "Model saved to ./best_svhn_model.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6/100 | Time: 211.77s | Train Loss: 0.0960, Train Acc: 97.36% | Val Loss: 0.1044, Val Acc: 97.12%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7/100 | Time: 214.88s | Train Loss: 0.0918, Train Acc: 97.49% | Val Loss: 0.0970, Val Acc: 97.31%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8/100 | Time: 212.53s | Train Loss: 0.0893, Train Acc: 97.60% | Val Loss: 0.1021, Val Acc: 97.29%\n",
            "\n",
            "Reducing learning rate from 1.00e-03 to 2.00e-04\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9/100 | Time: 214.09s | Train Loss: 0.0578, Train Acc: 98.51% | Val Loss: 0.0767, Val Acc: 97.97%\n",
            "\n",
            "Model saved to ./best_svhn_model.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10/100 | Time: 212.50s | Train Loss: 0.0495, Train Acc: 98.75% | Val Loss: 0.0759, Val Acc: 98.07%\n",
            "\n",
            "Model saved to ./best_svhn_model.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 11/100 | Time: 212.68s | Train Loss: 0.0442, Train Acc: 98.91% | Val Loss: 0.0767, Val Acc: 98.07%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 12/100 | Time: 214.46s | Train Loss: 0.0405, Train Acc: 99.01% | Val Loss: 0.0832, Val Acc: 97.98%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 13/100 | Time: 216.62s | Train Loss: 0.0374, Train Acc: 99.11% | Val Loss: 0.0826, Val Acc: 98.04%\n",
            "\n",
            "Reducing learning rate from 2.00e-04 to 4.00e-05\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 14/100 | Time: 220.70s | Train Loss: 0.0266, Train Acc: 99.44% | Val Loss: 0.0842, Val Acc: 98.15%\n",
            "\n",
            "Model saved to ./best_svhn_model.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 15/100 | Time: 214.11s | Train Loss: 0.0231, Train Acc: 99.54% | Val Loss: 0.0874, Val Acc: 98.13%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 16/100 | Time: 211.14s | Train Loss: 0.0207, Train Acc: 99.61% | Val Loss: 0.0936, Val Acc: 98.06%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 17/100 | Time: 212.60s | Train Loss: 0.0187, Train Acc: 99.66% | Val Loss: 0.0975, Val Acc: 98.05%\n",
            "\n",
            "Reducing learning rate from 4.00e-05 to 8.00e-06\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 18/100 | Time: 213.18s | Train Loss: 0.0162, Train Acc: 99.73% | Val Loss: 0.0995, Val Acc: 98.04%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 19/100 | Time: 215.49s | Train Loss: 0.0154, Train Acc: 99.75% | Val Loss: 0.1012, Val Acc: 98.02%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 20/100 | Time: 215.97s | Train Loss: 0.0149, Train Acc: 99.76% | Val Loss: 0.1035, Val Acc: 98.04%\n",
            "\n",
            "Reducing learning rate from 8.00e-06 to 1.60e-06\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 21/100 | Time: 215.02s | Train Loss: 0.0143, Train Acc: 99.78% | Val Loss: 0.1035, Val Acc: 98.04%\n",
            "\n",
            "Early stopping at epoch 21\n",
            "Restored model weights from the end of the best epoch: 14\n",
            "--- Training Finished ---\n",
            "\n",
            "--- Starting Comprehensive Evaluation on Test Set ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating: 100%|██████████| 739/739 [00:28<00:00, 26.05it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Top-K Accuracy Results ---\n",
            "Top-1 Accuracy: 98.09%\n",
            "Top-3 Accuracy: 99.42%\n",
            "Top-5 Accuracy: 99.71%\n",
            "\n",
            "--- Additional Performance Metrics ---\n",
            "Macro Average Precision: 0.9802\n",
            "Macro Average Recall: 0.9798\n",
            "Macro Average F1-Score: 0.9800\n",
            "Weighted Average Precision: 0.9809\n",
            "Weighted Average Recall: 0.9809\n",
            "Weighted Average F1-Score: 0.9809\n",
            "\n",
            "--- Model Confidence & Calibration Metrics ---\n",
            "Average Prediction Confidence: 0.9940\n",
            "Confidence Standard Deviation: 0.0424\n",
            "Average Prediction Entropy: 0.0200\n",
            "Expected Calibration Error: 0.0131\n",
            "\n",
            "Final Test Loss: 0.1009\n",
            "===============================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hybrid Transformer on ResNet-18 (RoPE)"
      ],
      "metadata": {
        "id": "E-o5l0sbqELR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "import torchvision.transforms as transforms\n",
        "import scipy.io\n",
        "from PIL import Image\n",
        "import os\n",
        "import requests\n",
        "import zipfile\n",
        "import shutil\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "import sys\n",
        "import math\n",
        "import numpy as np\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "\n",
        "# --- Configuration Parameters ---\n",
        "CONFIG = {\n",
        "    \"data_path\": \"./svhn-data/\",\n",
        "    \"batch_size\": 128,\n",
        "    \"num_epochs\": 100,\n",
        "    \"learning_rate\": 0.001,\n",
        "    \"weight_decay\": 1e-4,\n",
        "    \"split_seed\": 42,\n",
        "    \"train_split\": 0.70,\n",
        "    \"val_split\": 0.15,\n",
        "    \"test_split\": 0.15,\n",
        "    \"num_classes\": 10,    # SVHN has 10 classes (digits 0-9)\n",
        "    \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
        "    \"checkpoint_path\": \"./best_svhn_model.pth\",\n",
        "}\n",
        "\n",
        "# --- Transformer Configuration ---\n",
        "TRANSFORMER_CONFIG = {\n",
        "    \"embedding_dim\": 256,\n",
        "    \"nhead\": 8,\n",
        "    \"num_encoder_layers\": 3,\n",
        "    \"dim_feedforward\": 512,\n",
        "    \"dropout\": 0.1,\n",
        "}\n",
        "\n",
        "# ==========================================\n",
        "# Callback Classes\n",
        "# ==========================================\n",
        "\n",
        "class Callback:\n",
        "    \"\"\"Base callback class\"\"\"\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        pass\n",
        "\n",
        "    def on_training_end(self):\n",
        "        pass\n",
        "\n",
        "class ReduceLROnPlateau(Callback):\n",
        "    \"\"\"Reduce learning rate when a metric has stopped improving\"\"\"\n",
        "    def __init__(self, optimizer, monitor='val_accuracy', factor=0.2, patience=3, min_lr=1e-7, verbose=1):\n",
        "        self.optimizer = optimizer\n",
        "        self.monitor = monitor\n",
        "        self.factor = factor\n",
        "        self.patience = patience\n",
        "        self.min_lr = min_lr\n",
        "        self.verbose = verbose\n",
        "        self.wait = 0\n",
        "        self.best = None\n",
        "        self.mode = 'min' if 'loss' in monitor else 'max'\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        if logs is None:\n",
        "            return\n",
        "\n",
        "        current = logs.get(self.monitor)\n",
        "        if current is None:\n",
        "            return\n",
        "\n",
        "        if self.best is None:\n",
        "            self.best = current\n",
        "            return\n",
        "\n",
        "        if self.mode == 'min':\n",
        "            improved = current < self.best\n",
        "        else:\n",
        "            improved = current > self.best\n",
        "\n",
        "        if improved:\n",
        "            self.best = current\n",
        "            self.wait = 0\n",
        "        else:\n",
        "            self.wait += 1\n",
        "            if self.wait >= self.patience:\n",
        "                old_lr = self.optimizer.param_groups[0]['lr']\n",
        "                new_lr = max(old_lr * self.factor, self.min_lr)\n",
        "                if new_lr != old_lr:\n",
        "                    self.optimizer.param_groups[0]['lr'] = new_lr\n",
        "                    if self.verbose:\n",
        "                        print(f\"\\nReducing learning rate from {old_lr:.2e} to {new_lr:.2e}\")\n",
        "                    self.wait = 0\n",
        "\n",
        "class EarlyStopping(Callback):\n",
        "    \"\"\"Stop training when a monitored metric has stopped improving\"\"\"\n",
        "    def __init__(self, monitor='val_accuracy', patience=7, restore_best_weights=True, verbose=1):\n",
        "        self.monitor = monitor\n",
        "        self.patience = patience\n",
        "        self.restore_best_weights = restore_best_weights\n",
        "        self.verbose = verbose\n",
        "        self.wait = 0\n",
        "        self.best = None\n",
        "        self.best_weights = None\n",
        "        self.mode = 'min' if 'loss' in monitor else 'max'\n",
        "        self.stopped_epoch = 0\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None, model=None):\n",
        "        if logs is None:\n",
        "            return False\n",
        "\n",
        "        current = logs.get(self.monitor)\n",
        "        if current is None:\n",
        "            return False\n",
        "\n",
        "        if self.best is None:\n",
        "            self.best = current\n",
        "            if model is not None and self.restore_best_weights:\n",
        "                self.best_weights = model.state_dict().copy()\n",
        "            return False\n",
        "\n",
        "        if self.mode == 'min':\n",
        "            improved = current < self.best\n",
        "        else:\n",
        "            improved = current > self.best\n",
        "\n",
        "        if improved:\n",
        "            self.best = current\n",
        "            self.wait = 0\n",
        "            if model is not None and self.restore_best_weights:\n",
        "                self.best_weights = model.state_dict().copy()\n",
        "        else:\n",
        "            self.wait += 1\n",
        "            if self.wait >= self.patience:\n",
        "                self.stopped_epoch = epoch\n",
        "                if self.verbose:\n",
        "                    print(f\"\\nEarly stopping at epoch {epoch + 1}\")\n",
        "                return True\n",
        "        return False\n",
        "\n",
        "    def on_training_end(self, model=None):\n",
        "        if self.stopped_epoch > 0 and self.verbose:\n",
        "            print(f\"Restored model weights from the end of the best epoch: {self.stopped_epoch + 1 - self.patience}\")\n",
        "        if model is not None and self.restore_best_weights and self.best_weights is not None:\n",
        "            model.load_state_dict(self.best_weights)\n",
        "\n",
        "class ModelCheckpoint(Callback):\n",
        "    \"\"\"Save the model after every epoch\"\"\"\n",
        "    def __init__(self, filepath, save_best_only=True, monitor='val_accuracy', verbose=1):\n",
        "        self.filepath = filepath\n",
        "        self.save_best_only = save_best_only\n",
        "        self.monitor = monitor\n",
        "        self.verbose = verbose\n",
        "        self.best = None\n",
        "        self.mode = 'min' if 'loss' in monitor else 'max'\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None, model=None):\n",
        "        if logs is None or model is None:\n",
        "            return\n",
        "\n",
        "        current = logs.get(self.monitor)\n",
        "        if current is None:\n",
        "            return\n",
        "\n",
        "        if not self.save_best_only:\n",
        "            filepath = self.filepath.replace('.pth', f'_epoch_{epoch + 1}.pth')\n",
        "            torch.save(model.state_dict(), filepath)\n",
        "            if self.verbose:\n",
        "                print(f\"\\nModel saved to {filepath}\")\n",
        "            return\n",
        "\n",
        "        if self.best is None:\n",
        "            self.best = current\n",
        "            torch.save(model.state_dict(), self.filepath)\n",
        "            if self.verbose:\n",
        "                print(f\"\\nModel saved to {self.filepath}\")\n",
        "            return\n",
        "\n",
        "        if self.mode == 'min':\n",
        "            improved = current < self.best\n",
        "        else:\n",
        "            improved = current > self.best\n",
        "\n",
        "        if improved:\n",
        "            self.best = current\n",
        "            torch.save(model.state_dict(), self.filepath)\n",
        "            if self.verbose:\n",
        "                print(f\"\\nModel saved to {self.filepath}\")\n",
        "\n",
        "# ==========================================\n",
        "# Data Preprocessing\n",
        "# ==========================================\n",
        "\n",
        "def download_svhn_data(data_path):\n",
        "    \"\"\"Downloads SVHN dataset files.\"\"\"\n",
        "    os.makedirs(data_path, exist_ok=True)\n",
        "\n",
        "    urls = {\n",
        "        'train': 'http://ufldl.stanford.edu/housenumbers/train_32x32.mat',\n",
        "        'test': 'http://ufldl.stanford.edu/housenumbers/test_32x32.mat',\n",
        "        'extra': 'http://ufldl.stanford.edu/housenumbers/extra_32x32.mat'\n",
        "    }\n",
        "\n",
        "    downloaded_files = {}\n",
        "\n",
        "    for split, url in urls.items():\n",
        "        file_path = os.path.join(data_path, f'{split}_32x32.mat')\n",
        "\n",
        "        if os.path.exists(file_path):\n",
        "            print(f\"{split} data already exists.\")\n",
        "            downloaded_files[split] = file_path\n",
        "            continue\n",
        "\n",
        "        print(f\"Downloading {split} data from {url}...\")\n",
        "        try:\n",
        "            with requests.get(url, stream=True, timeout=60) as r:\n",
        "                r.raise_for_status()\n",
        "                total_size = int(r.headers.get('content-length', 0))\n",
        "                with open(file_path, 'wb') as f, tqdm(\n",
        "                    total=total_size, unit='iB', unit_scale=True, desc=f\"SVHN {split}\"\n",
        "                ) as progress_bar:\n",
        "                    for chunk in r.iter_content(chunk_size=8192):\n",
        "                        f.write(chunk)\n",
        "                        progress_bar.update(len(chunk))\n",
        "            downloaded_files[split] = file_path\n",
        "            print(f\"{split} data downloaded successfully.\")\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            print(f\"Error downloading {split} data: {e}\", file=sys.stderr)\n",
        "            return None\n",
        "\n",
        "    return downloaded_files\n",
        "\n",
        "class SVHNDataset(Dataset):\n",
        "    \"\"\"Custom SVHN Dataset class.\"\"\"\n",
        "    def __init__(self, mat_file_path, transform=None):\n",
        "        self.transform = transform\n",
        "\n",
        "        # Load .mat file\n",
        "        mat_data = scipy.io.loadmat(mat_file_path)\n",
        "\n",
        "        # Extract data and labels\n",
        "        # SVHN format: X is (32, 32, 3, N), y is (N, 1)\n",
        "        self.data = mat_data['X'].transpose((3, 0, 1, 2))  # Convert to (N, 32, 32, 3)\n",
        "        self.labels = mat_data['y'].flatten()\n",
        "\n",
        "        # Convert labels: SVHN uses 1-10, we need 0-9\n",
        "        # Label 10 represents digit '0', labels 1-9 represent digits '1'-'9'\n",
        "        self.labels = self.labels % 10  # Convert 10 -> 0, keep 1-9 as is\n",
        "\n",
        "        # Ensure labels are in correct range\n",
        "        assert self.labels.min() >= 0 and self.labels.max() <= 9, f\"Labels should be 0-9, got {self.labels.min()}-{self.labels.max()}\"\n",
        "\n",
        "        print(f\"Loaded {len(self.data)} samples\")\n",
        "        print(f\"Data shape: {self.data.shape}\")\n",
        "        print(f\"Labels range: {self.labels.min()} to {self.labels.max()}\")\n",
        "        print(f\"Label distribution: {np.bincount(self.labels)}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Get image and label\n",
        "        image = self.data[idx]\n",
        "        label = int(self.labels[idx])  # Ensure label is int\n",
        "\n",
        "        # Convert numpy array to PIL Image\n",
        "        image = Image.fromarray(image.astype('uint8'))\n",
        "\n",
        "        # Apply transforms\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, label\n",
        "\n",
        "def get_dataloaders(config):\n",
        "    \"\"\"Downloads, prepares, and splits the SVHN data, returning DataLoaders.\"\"\"\n",
        "\n",
        "    # Download SVHN data\n",
        "    downloaded_files = download_svhn_data(config[\"data_path\"])\n",
        "    if downloaded_files is None:\n",
        "        return None\n",
        "\n",
        "    # Define transforms\n",
        "    transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.4377, 0.4438, 0.4728], std=[0.1980, 0.2010, 0.1970]),  # SVHN stats\n",
        "    ])\n",
        "\n",
        "    # Load all datasets\n",
        "    train_dataset = SVHNDataset(downloaded_files['train'], transform=transform)\n",
        "    test_dataset = SVHNDataset(downloaded_files['test'], transform=transform)\n",
        "    extra_dataset = SVHNDataset(downloaded_files['extra'], transform=transform)\n",
        "\n",
        "    # Merge all datasets together\n",
        "    full_dataset = torch.utils.data.ConcatDataset([train_dataset, test_dataset, extra_dataset])\n",
        "    print(f\"Total merged dataset size: {len(full_dataset)}\")\n",
        "    print(f\"  - Train: {len(train_dataset)} samples\")\n",
        "    print(f\"  - Test: {len(test_dataset)} samples\")\n",
        "    print(f\"  - Extra: {len(extra_dataset)} samples\")\n",
        "\n",
        "    # Ensure reproducible splits\n",
        "    torch.manual_seed(config[\"split_seed\"])\n",
        "    np.random.seed(config[\"split_seed\"])\n",
        "\n",
        "    total_size = len(full_dataset)\n",
        "    train_size = int(config[\"train_split\"] * total_size)\n",
        "    val_size = int(config[\"val_split\"] * total_size)\n",
        "    test_size = int(config[\"test_split\"] * total_size)\n",
        "\n",
        "    # Adjust sizes to ensure they sum to total_size (handle rounding issues)\n",
        "    actual_total = train_size + val_size + test_size\n",
        "    if actual_total != total_size:\n",
        "        # Add/subtract the difference to test_size\n",
        "        test_size += (total_size - actual_total)\n",
        "\n",
        "    print(f\"Dataset splits - Train: {train_size}, Val: {val_size}, Test: {test_size}, Total: {total_size}\")\n",
        "\n",
        "    # Create reproducible generator for splitting\n",
        "    generator = torch.Generator().manual_seed(config[\"split_seed\"])\n",
        "    train_subset, val_subset, test_subset = random_split(\n",
        "        full_dataset, [train_size, val_size, test_size], generator=generator\n",
        "    )\n",
        "\n",
        "    # Set worker init function for reproducible DataLoader behavior\n",
        "    def worker_init_fn(worker_id):\n",
        "        np.random.seed(config[\"split_seed\"] + worker_id)\n",
        "\n",
        "    # Create data loaders\n",
        "    train_loader = DataLoader(\n",
        "        train_subset,\n",
        "        batch_size=config[\"batch_size\"],\n",
        "        shuffle=True,\n",
        "        num_workers=2,\n",
        "        pin_memory=True,\n",
        "        worker_init_fn=worker_init_fn,\n",
        "        generator=torch.Generator().manual_seed(config[\"split_seed\"])\n",
        "    )\n",
        "\n",
        "    val_loader = DataLoader(\n",
        "        val_subset,\n",
        "        batch_size=config[\"batch_size\"],\n",
        "        shuffle=False,\n",
        "        num_workers=2,\n",
        "        pin_memory=True,\n",
        "        worker_init_fn=worker_init_fn\n",
        "    )\n",
        "\n",
        "    test_loader = DataLoader(\n",
        "        test_subset,\n",
        "        batch_size=config[\"batch_size\"],\n",
        "        shuffle=False,\n",
        "        num_workers=2,\n",
        "        pin_memory=True,\n",
        "        worker_init_fn=worker_init_fn\n",
        "    )\n",
        "\n",
        "    return train_loader, val_loader, test_loader\n",
        "\n",
        "# ==========================================\n",
        "# Model Architecture and Training\n",
        "# ==========================================\n",
        "\n",
        "class RotaryEmbedding(nn.Module):\n",
        "    \"\"\"\n",
        "    The Rotary Positional Embedding (RoPE) module.\n",
        "    This implementation is based on the paper \"RoFormer: Enhanced Transformer with Rotary Position Embedding\".\n",
        "    \"\"\"\n",
        "    def __init__(self, dim, base=10000):\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "        self.base = base\n",
        "        # Create inverse frequencies and register as a buffer\n",
        "        inv_freq = 1.0 / (self.base ** (torch.arange(0, dim, 2).float() / dim))\n",
        "        self.register_buffer(\"inv_freq\", inv_freq)\n",
        "        self.seq_len_cached = None\n",
        "        self.cos_cached = None\n",
        "        self.sin_cached = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        seq_len = x.shape[1]\n",
        "        # Check if we need to recompute the cache\n",
        "        if seq_len != self.seq_len_cached:\n",
        "            self.seq_len_cached = seq_len\n",
        "            t = torch.arange(seq_len, device=x.device).type_as(self.inv_freq)\n",
        "            freqs = torch.einsum(\"i,j->ij\", t, self.inv_freq)\n",
        "            emb = torch.cat((freqs, freqs), dim=-1).to(x.device)\n",
        "            self.cos_cached = emb.cos()[:, None, :]\n",
        "            self.sin_cached = emb.sin()[:, None, :]\n",
        "\n",
        "        # Apply the rotation\n",
        "        return self.cos_cached, self.sin_cached\n",
        "\n",
        "def rotate_half(x):\n",
        "    x1, x2 = x[..., : x.shape[-1] // 2], x[..., x.shape[-1] // 2 :]\n",
        "    return torch.cat((-x2, x1), dim=x1.ndim - 1)\n",
        "\n",
        "def apply_rotary_pos_emb(q, k, cos, sin):\n",
        "    return (q * cos) + (rotate_half(q) * sin), (k * cos) + (rotate_half(k) * sin)\n",
        "\n",
        "class MultiHeadAttentionWithRoPE(nn.Module):\n",
        "    \"\"\"Custom Multi-Head Attention with RoPE support\"\"\"\n",
        "    def __init__(self, d_model, nhead, dropout=0.1, batch_first=True):\n",
        "        super().__init__()\n",
        "        assert d_model % nhead == 0\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.nhead = nhead\n",
        "        self.d_k = d_model // nhead\n",
        "        self.batch_first = batch_first  # Add this attribute\n",
        "\n",
        "        self.w_q = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.w_k = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.w_v = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.w_o = nn.Linear(d_model, d_model)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.rotary_emb = RotaryEmbedding(self.d_k)\n",
        "\n",
        "    def forward(self, query, key, value, key_padding_mask=None, attn_mask=None, need_weights=False, is_causal=False):\n",
        "        batch_size, seq_len, _ = query.size()\n",
        "\n",
        "        # Linear transformations\n",
        "        Q = self.w_q(query)  # [batch_size, seq_len, d_model]\n",
        "        K = self.w_k(key)    # [batch_size, seq_len, d_model]\n",
        "        V = self.w_v(value)  # [batch_size, seq_len, d_model]\n",
        "\n",
        "        # Reshape for multi-head attention\n",
        "        Q = Q.view(batch_size, seq_len, self.nhead, self.d_k)  # [batch, seq, heads, d_k]\n",
        "        K = K.view(batch_size, seq_len, self.nhead, self.d_k)\n",
        "        V = V.view(batch_size, seq_len, self.nhead, self.d_k)\n",
        "\n",
        "        # Apply RoPE to Q and K\n",
        "        cos, sin = self.rotary_emb(query)\n",
        "        Q, K = apply_rotary_pos_emb(Q, K, cos, sin)\n",
        "\n",
        "        # Transpose for attention computation: [batch, heads, seq, d_k]\n",
        "        Q = Q.transpose(1, 2)\n",
        "        K = K.transpose(1, 2)\n",
        "        V = V.transpose(1, 2)\n",
        "\n",
        "        # Scaled dot-product attention\n",
        "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
        "\n",
        "        # Apply masks if provided\n",
        "        if attn_mask is not None:\n",
        "            scores = scores.masked_fill(attn_mask == 0, -1e9)\n",
        "\n",
        "        if key_padding_mask is not None:\n",
        "            # key_padding_mask: [batch_size, seq_len], True for padding positions\n",
        "            key_padding_mask = key_padding_mask.unsqueeze(1).unsqueeze(2)  # [batch, 1, 1, seq]\n",
        "            scores = scores.masked_fill(key_padding_mask, -1e9)\n",
        "\n",
        "        attention_weights = torch.softmax(scores, dim=-1)\n",
        "        attention_weights = self.dropout(attention_weights)\n",
        "\n",
        "        # Apply attention to values\n",
        "        attention_output = torch.matmul(attention_weights, V)  # [batch, heads, seq, d_k]\n",
        "\n",
        "        # Concatenate heads\n",
        "        attention_output = attention_output.transpose(1, 2).contiguous().view(\n",
        "            batch_size, seq_len, self.d_model\n",
        "        )\n",
        "\n",
        "        # Final linear transformation\n",
        "        output = self.w_o(attention_output)\n",
        "\n",
        "        if need_weights:\n",
        "            return output, attention_weights.mean(dim=1)  # Average over heads for compatibility\n",
        "        return output\n",
        "\n",
        "class TransformerEncoderLayerWithRoPE(nn.Module):\n",
        "    \"\"\"\n",
        "    A custom Transformer Encoder Layer that incorporates RoPE.\n",
        "    \"\"\"\n",
        "    def __init__(self, d_model, nhead, dim_feedforward, dropout, batch_first=True):\n",
        "        super().__init__()\n",
        "        self.self_attn = MultiHeadAttentionWithRoPE(d_model, nhead, dropout, batch_first)\n",
        "        self.linear1 = nn.Linear(d_model, dim_feedforward)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.linear2 = nn.Linear(dim_feedforward, d_model)\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "        self.activation = nn.ReLU(inplace=True)\n",
        "        self.batch_first = batch_first  # Add this attribute\n",
        "\n",
        "    def forward(self, src, src_mask=None, src_key_padding_mask=None, is_causal=False):\n",
        "        # Self-attention with RoPE\n",
        "        src2 = self.self_attn(src, src, src, key_padding_mask=src_key_padding_mask, attn_mask=src_mask)\n",
        "        src = src + self.dropout1(src2)\n",
        "        src = self.norm1(src)\n",
        "\n",
        "        # Feed Forward\n",
        "        src2 = self.linear2(self.dropout(self.activation(self.linear1(src))))\n",
        "        src = src + self.dropout2(src2)\n",
        "        src = self.norm2(src)\n",
        "        return src\n",
        "\n",
        "class NonResidualBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, stride=1):\n",
        "        super(NonResidualBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        out = self.relu(out)\n",
        "        return out\n",
        "\n",
        "class ResNetTransformer(nn.Module):\n",
        "    def __init__(self, block, layers, num_classes, t_config):\n",
        "        super(ResNetTransformer, self).__init__()\n",
        "        self.in_channels = 64\n",
        "        self.embedding_dim = t_config[\"embedding_dim\"]\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "        self.layer1 = self._make_layer(block, 64, layers[0], stride=1)\n",
        "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n",
        "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n",
        "\n",
        "        self.projections = nn.ModuleList([\n",
        "            self._create_projection(64, self.embedding_dim),\n",
        "            self._create_projection(64, self.embedding_dim),\n",
        "            self._create_projection(128, self.embedding_dim),\n",
        "            self._create_projection(256, self.embedding_dim),\n",
        "            self._create_projection(512, self.embedding_dim)\n",
        "        ])\n",
        "\n",
        "        # Create transformer encoder layers manually\n",
        "        self.transformer_layers = nn.ModuleList([\n",
        "            TransformerEncoderLayerWithRoPE(\n",
        "                d_model=self.embedding_dim,\n",
        "                nhead=t_config[\"nhead\"],\n",
        "                dim_feedforward=t_config[\"dim_feedforward\"],\n",
        "                dropout=t_config[\"dropout\"],\n",
        "                batch_first=True\n",
        "            ) for _ in range(t_config[\"num_encoder_layers\"])\n",
        "        ])\n",
        "\n",
        "        self.classifier = nn.Linear(self.embedding_dim, num_classes)\n",
        "\n",
        "    def _create_projection(self, in_features, out_features):\n",
        "        return nn.Sequential(\n",
        "            nn.AdaptiveAvgPool2d((1, 1)),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(in_features, out_features)\n",
        "        )\n",
        "\n",
        "    def _make_layer(self, block, out_channels, num_blocks, stride):\n",
        "        strides = [stride] + [1] * (num_blocks - 1)\n",
        "        layers = []\n",
        "        current_in_channels = self.in_channels\n",
        "        for s in strides:\n",
        "            layers.append(block(current_in_channels, out_channels, s))\n",
        "            current_in_channels = out_channels\n",
        "        self.in_channels = out_channels\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        features = []\n",
        "        x = self.maxpool(self.relu(self.bn1(self.conv1(x))))\n",
        "        features.append(x)\n",
        "        x = self.layer1(x)\n",
        "        features.append(x)\n",
        "        x = self.layer2(x)\n",
        "        features.append(x)\n",
        "        x = self.layer3(x)\n",
        "        features.append(x)\n",
        "        x = self.layer4(x)\n",
        "        features.append(x)\n",
        "\n",
        "        tokens = [self.projections[i](feat) for i, feat in enumerate(features)]\n",
        "        token_sequence = torch.stack(tokens, dim=1)\n",
        "\n",
        "        # Apply transformer layers manually\n",
        "        for layer in self.transformer_layers:\n",
        "            token_sequence = layer(token_sequence)\n",
        "\n",
        "        aggregated_vector = token_sequence.mean(dim=1)\n",
        "        logits = self.classifier(aggregated_vector)\n",
        "        return logits\n",
        "\n",
        "def evaluate_model(model, data_loader, criterion, device):\n",
        "    \"\"\"Evaluates the model on a given dataset.\"\"\"\n",
        "    model.eval()\n",
        "    total_loss, correct, total = 0.0, 0, 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in data_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            total_loss += loss.item() * images.size(0)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    avg_loss = total_loss / total\n",
        "    accuracy = 100 * correct / total\n",
        "    return avg_loss, accuracy\n",
        "\n",
        "def train_and_validate(model, train_loader, val_loader, config):\n",
        "    \"\"\"Main training loop with callbacks.\"\"\"\n",
        "    device = config[\"device\"]\n",
        "    model.to(device)\n",
        "\n",
        "    # Set seeds for reproducible training\n",
        "    torch.manual_seed(config[\"split_seed\"])\n",
        "    np.random.seed(config[\"split_seed\"])\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(config[\"split_seed\"])\n",
        "        torch.cuda.manual_seed_all(config[\"split_seed\"])\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=config[\"learning_rate\"], weight_decay=config[\"weight_decay\"])\n",
        "\n",
        "    # Initialize callbacks\n",
        "    callbacks = [\n",
        "        ReduceLROnPlateau(\n",
        "            optimizer=optimizer,\n",
        "            monitor='val_accuracy',\n",
        "            factor=0.2,\n",
        "            patience=3,\n",
        "            min_lr=1e-7,\n",
        "            verbose=1\n",
        "        ),\n",
        "        EarlyStopping(\n",
        "            monitor='val_accuracy',\n",
        "            patience=7,\n",
        "            restore_best_weights=True,\n",
        "            verbose=1\n",
        "        ),\n",
        "        ModelCheckpoint(\n",
        "            filepath=config[\"checkpoint_path\"],\n",
        "            save_best_only=True,\n",
        "            monitor='val_accuracy',\n",
        "            verbose=1\n",
        "        )\n",
        "    ]\n",
        "\n",
        "    print(\"\\n--- Starting Training ---\")\n",
        "    early_stop = False\n",
        "\n",
        "    for epoch in range(config[\"num_epochs\"]):\n",
        "        if early_stop:\n",
        "            break\n",
        "\n",
        "        model.train()\n",
        "        start_time = time.time()\n",
        "        running_loss, train_correct, train_total = 0.0, 0, 0\n",
        "\n",
        "        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{config['num_epochs']} [Train]\", leave=False)\n",
        "        for images, labels in progress_bar:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item() * images.size(0)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            train_total += labels.size(0)\n",
        "            train_correct += (predicted == labels).sum().item()\n",
        "            progress_bar.set_postfix(loss=loss.item())\n",
        "\n",
        "        train_loss = running_loss / train_total\n",
        "        train_acc = 100 * train_correct / train_total\n",
        "        val_loss, val_acc = evaluate_model(model, val_loader, criterion, device)\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{config['num_epochs']} | Time: {time.time() - start_time:.2f}s | \"\n",
        "              f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}% | \"\n",
        "              f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%\")\n",
        "\n",
        "        # Prepare logs for callbacks\n",
        "        logs = {\n",
        "            'train_loss': train_loss,\n",
        "            'train_accuracy': train_acc,\n",
        "            'val_loss': val_loss,\n",
        "            'val_accuracy': val_acc\n",
        "        }\n",
        "\n",
        "        # Execute callbacks\n",
        "        for callback in callbacks:\n",
        "            if isinstance(callback, EarlyStopping):\n",
        "                if callback.on_epoch_end(epoch, logs, model):\n",
        "                    early_stop = True\n",
        "                    break\n",
        "            elif isinstance(callback, ModelCheckpoint):\n",
        "                callback.on_epoch_end(epoch, logs, model)\n",
        "            else:\n",
        "                callback.on_epoch_end(epoch, logs)\n",
        "\n",
        "    # Execute callback cleanup\n",
        "    for callback in callbacks:\n",
        "        if isinstance(callback, EarlyStopping):\n",
        "            callback.on_training_end(model)\n",
        "        else:\n",
        "            callback.on_training_end()\n",
        "\n",
        "    print(\"--- Training Finished ---\\n\")\n",
        "    return model\n",
        "\n",
        "# ==========================================\n",
        "# Evaluation\n",
        "# ==========================================\n",
        "\n",
        "def calculate_top_k_accuracy(outputs, labels, k_values=[1, 3, 5]):\n",
        "    \"\"\"Calculate top-k accuracy for given k values.\"\"\"\n",
        "    batch_size = labels.size(0)\n",
        "    _, pred = outputs.topk(max(k_values), 1, True, True)\n",
        "    pred = pred.t()\n",
        "    correct = pred.eq(labels.view(1, -1).expand_as(pred))\n",
        "\n",
        "    top_k_accuracies = {}\n",
        "    for k in k_values:\n",
        "        correct_k = correct[:k].reshape(-1).float().sum(0, keepdim=True)\n",
        "        top_k_accuracies[k] = correct_k.item() / batch_size\n",
        "\n",
        "    return top_k_accuracies\n",
        "\n",
        "def calculate_entropy(probs):\n",
        "    \"\"\"Calculate entropy of probability distributions.\"\"\"\n",
        "    # Add small epsilon to avoid log(0)\n",
        "    epsilon = 1e-8\n",
        "    probs = probs + epsilon\n",
        "    entropy = -torch.sum(probs * torch.log(probs), dim=1)\n",
        "    return entropy\n",
        "\n",
        "def calculate_ece(confidences, accuracies, n_bins=10):\n",
        "    \"\"\"Calculate Expected Calibration Error.\"\"\"\n",
        "    bin_boundaries = torch.linspace(0, 1, n_bins + 1)\n",
        "    bin_lowers = bin_boundaries[:-1]\n",
        "    bin_uppers = bin_boundaries[1:]\n",
        "\n",
        "    ece = 0\n",
        "    for bin_lower, bin_upper in zip(bin_lowers, bin_uppers):\n",
        "        in_bin = (confidences > bin_lower.item()) & (confidences <= bin_upper.item())\n",
        "        prop_in_bin = in_bin.float().mean()\n",
        "\n",
        "        if prop_in_bin.item() > 0:\n",
        "            accuracy_in_bin = accuracies[in_bin].float().mean()\n",
        "            avg_confidence_in_bin = confidences[in_bin].mean()\n",
        "            ece += torch.abs(avg_confidence_in_bin - accuracy_in_bin) * prop_in_bin\n",
        "\n",
        "    return ece.item()\n",
        "\n",
        "def comprehensive_evaluation(model, test_loader, criterion, device):\n",
        "    \"\"\"Comprehensive evaluation with all requested metrics.\"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    all_predictions = []\n",
        "    all_labels = []\n",
        "    all_confidences = []\n",
        "    all_entropies = []\n",
        "    all_top_k_results = {1: [], 3: [], 5: []}\n",
        "    total_loss = 0.0\n",
        "    total_samples = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in tqdm(test_loader, desc=\"Evaluating\"):\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            # Convert outputs to probabilities\n",
        "            probs = torch.softmax(outputs, dim=1)\n",
        "\n",
        "            # Get predictions and confidences\n",
        "            confidences, predictions = torch.max(probs, dim=1)\n",
        "\n",
        "            # Calculate entropy\n",
        "            entropies = calculate_entropy(probs)\n",
        "\n",
        "            # Calculate top-k accuracies\n",
        "            top_k_accs = calculate_top_k_accuracy(outputs, labels, [1, 3, 5])\n",
        "\n",
        "            # Store results\n",
        "            all_predictions.extend(predictions.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "            all_confidences.extend(confidences.cpu().numpy())\n",
        "            all_entropies.extend(entropies.cpu().numpy())\n",
        "\n",
        "            for k in [1, 3, 5]:\n",
        "                all_top_k_results[k].extend([top_k_accs[k]] * labels.size(0))\n",
        "\n",
        "            total_loss += loss.item() * images.size(0)\n",
        "            total_samples += images.size(0)\n",
        "\n",
        "    # Convert to numpy arrays\n",
        "    all_predictions = np.array(all_predictions)\n",
        "    all_labels = np.array(all_labels)\n",
        "    all_confidences = np.array(all_confidences)\n",
        "    all_entropies = np.array(all_entropies)\n",
        "\n",
        "    # Calculate basic metrics\n",
        "    avg_loss = total_loss / total_samples\n",
        "\n",
        "    # Calculate top-k accuracies\n",
        "    top1 = np.mean([pred == label for pred, label in zip(all_predictions, all_labels)])\n",
        "    top3 = np.mean(all_top_k_results[3])\n",
        "    top5 = np.mean(all_top_k_results[5])\n",
        "\n",
        "    # Calculate precision, recall, f1-score\n",
        "    precision_macro, recall_macro, f1_macro, _ = precision_recall_fscore_support(\n",
        "        all_labels, all_predictions, average='macro', zero_division=0\n",
        "    )\n",
        "    precision_weighted, recall_weighted, f1_weighted, _ = precision_recall_fscore_support(\n",
        "        all_labels, all_predictions, average='weighted', zero_division=0\n",
        "    )\n",
        "\n",
        "    # Calculate confidence and calibration metrics\n",
        "    avg_confidence = np.mean(all_confidences)\n",
        "    std_confidence = np.std(all_confidences)\n",
        "    avg_entropy = np.mean(all_entropies)\n",
        "\n",
        "    # Calculate ECE\n",
        "    accuracies = (all_predictions == all_labels).astype(float)\n",
        "    ece = calculate_ece(torch.tensor(all_confidences), torch.tensor(accuracies))\n",
        "\n",
        "    return {\n",
        "        'test_loss': avg_loss,\n",
        "        'top1': top1,\n",
        "        'top3': top3,\n",
        "        'top5': top5,\n",
        "        'precision_macro': precision_macro,\n",
        "        'recall_macro': recall_macro,\n",
        "        'f1_macro': f1_macro,\n",
        "        'precision_weighted': precision_weighted,\n",
        "        'recall_weighted': recall_weighted,\n",
        "        'f1_weighted': f1_weighted,\n",
        "        'avg_confidence': avg_confidence,\n",
        "        'std_confidence': std_confidence,\n",
        "        'avg_entropy': avg_entropy,\n",
        "        'ece': ece\n",
        "    }\n",
        "\n",
        "# ==========================================\n",
        "# Main Execution\n",
        "# ==========================================\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # Set global seeds for full reproducibility\n",
        "    torch.manual_seed(CONFIG[\"split_seed\"])\n",
        "    np.random.seed(CONFIG[\"split_seed\"])\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(CONFIG[\"split_seed\"])\n",
        "        torch.cuda.manual_seed_all(CONFIG[\"split_seed\"])\n",
        "        # Ensure deterministic behavior on CUDA\n",
        "        torch.backends.cudnn.deterministic = True\n",
        "        torch.backends.cudnn.benchmark = False\n",
        "\n",
        "    print(f\"Using device: {CONFIG['device']}\")\n",
        "    print(f\"Reproducibility seed: {CONFIG['split_seed']}\")\n",
        "\n",
        "    # Get data loaders\n",
        "    dataloaders = get_dataloaders(CONFIG)\n",
        "    if dataloaders is None:\n",
        "        print(\"Could not prepare data. Halting execution.\", file=sys.stderr)\n",
        "        sys.exit(1)  # Exit if data preparation failed\n",
        "\n",
        "    train_loader, val_loader, test_loader = dataloaders\n",
        "\n",
        "    # Initialize and train model\n",
        "    model = ResNetTransformer(\n",
        "        block=NonResidualBlock,\n",
        "        layers=[2, 2, 2, 2],\n",
        "        num_classes=CONFIG[\"num_classes\"],\n",
        "        t_config=TRANSFORMER_CONFIG\n",
        "    )\n",
        "    trained_model = train_and_validate(model, train_loader, val_loader, CONFIG)\n",
        "\n",
        "    # Comprehensive evaluation on the test set\n",
        "    print(\"--- Starting Comprehensive Evaluation on Test Set ---\")\n",
        "    results = comprehensive_evaluation(trained_model, test_loader, nn.CrossEntropyLoss(), CONFIG[\"device\"])\n",
        "\n",
        "    # Print all requested metrics\n",
        "    print(\"\\n--- Top-K Accuracy Results ---\")\n",
        "    print(f\"Top-1 Accuracy: {results['top1'] * 100:.2f}%\")\n",
        "    print(f\"Top-3 Accuracy: {results['top3'] * 100:.2f}%\")\n",
        "    print(f\"Top-5 Accuracy: {results['top5'] * 100:.2f}%\")\n",
        "\n",
        "    print(\"\\n--- Additional Performance Metrics ---\")\n",
        "    print(f\"Macro Average Precision: {results['precision_macro']:.4f}\")\n",
        "    print(f\"Macro Average Recall: {results['recall_macro']:.4f}\")\n",
        "    print(f\"Macro Average F1-Score: {results['f1_macro']:.4f}\")\n",
        "    print(f\"Weighted Average Precision: {results['precision_weighted']:.4f}\")\n",
        "    print(f\"Weighted Average Recall: {results['recall_weighted']:.4f}\")\n",
        "    print(f\"Weighted Average F1-Score: {results['f1_weighted']:.4f}\")\n",
        "\n",
        "    print(\"\\n--- Model Confidence & Calibration Metrics ---\")\n",
        "    print(f\"Average Prediction Confidence: {results['avg_confidence']:.4f}\")\n",
        "    print(f\"Confidence Standard Deviation: {results['std_confidence']:.4f}\")\n",
        "    print(f\"Average Prediction Entropy: {results['avg_entropy']:.4f}\")\n",
        "    print(f\"Expected Calibration Error: {results['ece']:.4f}\")\n",
        "\n",
        "    print(f\"\\nFinal Test Loss: {results['test_loss']:.4f}\")\n",
        "    print(\"===============================================\")"
      ],
      "metadata": {
        "id": "zK829D0NqE-X",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "236163cf-adbd-4786-c33d-a46e25282bff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Reproducibility seed: 42\n",
            "Downloading train data from http://ufldl.stanford.edu/housenumbers/train_32x32.mat...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "SVHN train: 100%|██████████| 182M/182M [00:11<00:00, 15.9MiB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train data downloaded successfully.\n",
            "Downloading test data from http://ufldl.stanford.edu/housenumbers/test_32x32.mat...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "SVHN test: 100%|██████████| 64.3M/64.3M [00:01<00:00, 49.2MiB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test data downloaded successfully.\n",
            "Downloading extra data from http://ufldl.stanford.edu/housenumbers/extra_32x32.mat...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "SVHN extra: 100%|██████████| 1.33G/1.33G [00:16<00:00, 79.1MiB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "extra data downloaded successfully.\n",
            "Loaded 73257 samples\n",
            "Data shape: (73257, 32, 32, 3)\n",
            "Labels range: 0 to 9\n",
            "Label distribution: [ 4948 13861 10585  8497  7458  6882  5727  5595  5045  4659]\n",
            "Loaded 26032 samples\n",
            "Data shape: (26032, 32, 32, 3)\n",
            "Labels range: 0 to 9\n",
            "Label distribution: [1744 5099 4149 2882 2523 2384 1977 2019 1660 1595]\n",
            "Loaded 531131 samples\n",
            "Data shape: (531131, 32, 32, 3)\n",
            "Labels range: 0 to 9\n",
            "Label distribution: [45550 90560 74740 60765 50633 53490 41582 43997 35358 34456]\n",
            "Total merged dataset size: 630420\n",
            "  - Train: 73257 samples\n",
            "  - Test: 26032 samples\n",
            "  - Extra: 531131 samples\n",
            "Dataset splits - Train: 441294, Val: 94563, Test: 94563, Total: 630420\n",
            "\n",
            "--- Starting Training ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100 | Time: 215.26s | Train Loss: 0.2811, Train Acc: 91.36% | Val Loss: 0.1585, Val Acc: 95.35%\n",
            "\n",
            "Model saved to ./best_svhn_model.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/100 | Time: 213.13s | Train Loss: 0.1310, Train Acc: 96.29% | Val Loss: 0.1246, Val Acc: 96.61%\n",
            "\n",
            "Model saved to ./best_svhn_model.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/100 | Time: 210.78s | Train Loss: 0.1134, Train Acc: 96.85% | Val Loss: 0.1194, Val Acc: 96.66%\n",
            "\n",
            "Model saved to ./best_svhn_model.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4/100 | Time: 210.62s | Train Loss: 0.1027, Train Acc: 97.15% | Val Loss: 0.1107, Val Acc: 96.93%\n",
            "\n",
            "Model saved to ./best_svhn_model.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5/100 | Time: 209.02s | Train Loss: 0.0972, Train Acc: 97.32% | Val Loss: 0.0992, Val Acc: 97.30%\n",
            "\n",
            "Model saved to ./best_svhn_model.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6/100 | Time: 211.47s | Train Loss: 0.0926, Train Acc: 97.44% | Val Loss: 0.0999, Val Acc: 97.24%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7/100 | Time: 207.33s | Train Loss: 0.0891, Train Acc: 97.53% | Val Loss: 0.0976, Val Acc: 97.34%\n",
            "\n",
            "Model saved to ./best_svhn_model.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8/100 | Time: 207.41s | Train Loss: 0.0859, Train Acc: 97.65% | Val Loss: 0.1032, Val Acc: 97.18%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9/100 | Time: 212.56s | Train Loss: 0.0841, Train Acc: 97.69% | Val Loss: 0.1101, Val Acc: 97.01%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10/100 | Time: 209.91s | Train Loss: 0.0822, Train Acc: 97.76% | Val Loss: 0.0951, Val Acc: 97.40%\n",
            "\n",
            "Model saved to ./best_svhn_model.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 11/100 | Time: 211.67s | Train Loss: 0.0807, Train Acc: 97.79% | Val Loss: 0.0943, Val Acc: 97.47%\n",
            "\n",
            "Model saved to ./best_svhn_model.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 12/100 | Time: 209.72s | Train Loss: 0.0792, Train Acc: 97.84% | Val Loss: 0.0951, Val Acc: 97.45%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 13/100 | Time: 213.32s | Train Loss: 0.0789, Train Acc: 97.85% | Val Loss: 0.0866, Val Acc: 97.67%\n",
            "\n",
            "Model saved to ./best_svhn_model.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 14/100 | Time: 209.88s | Train Loss: 0.0780, Train Acc: 97.89% | Val Loss: 0.0959, Val Acc: 97.37%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 15/100 | Time: 210.11s | Train Loss: 0.0775, Train Acc: 97.89% | Val Loss: 0.0905, Val Acc: 97.59%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 16/100 | Time: 210.58s | Train Loss: 0.0759, Train Acc: 97.93% | Val Loss: 0.0893, Val Acc: 97.59%\n",
            "\n",
            "Reducing learning rate from 1.00e-03 to 2.00e-04\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 17/100 | Time: 210.56s | Train Loss: 0.0492, Train Acc: 98.76% | Val Loss: 0.0733, Val Acc: 98.12%\n",
            "\n",
            "Model saved to ./best_svhn_model.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 18/100 | Time: 208.37s | Train Loss: 0.0415, Train Acc: 98.99% | Val Loss: 0.0752, Val Acc: 98.09%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 19/100 | Time: 212.36s | Train Loss: 0.0371, Train Acc: 99.12% | Val Loss: 0.0770, Val Acc: 98.08%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 20/100 | Time: 208.53s | Train Loss: 0.0337, Train Acc: 99.20% | Val Loss: 0.0816, Val Acc: 98.06%\n",
            "\n",
            "Reducing learning rate from 2.00e-04 to 4.00e-05\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 21/100 | Time: 210.08s | Train Loss: 0.0253, Train Acc: 99.48% | Val Loss: 0.0823, Val Acc: 98.14%\n",
            "\n",
            "Model saved to ./best_svhn_model.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 22/100 | Time: 214.65s | Train Loss: 0.0226, Train Acc: 99.57% | Val Loss: 0.0850, Val Acc: 98.11%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 23/100 | Time: 209.72s | Train Loss: 0.0208, Train Acc: 99.62% | Val Loss: 0.0894, Val Acc: 98.10%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 24/100 | Time: 214.17s | Train Loss: 0.0195, Train Acc: 99.65% | Val Loss: 0.0920, Val Acc: 98.06%\n",
            "\n",
            "Reducing learning rate from 4.00e-05 to 8.00e-06\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 25/100 | Time: 211.06s | Train Loss: 0.0175, Train Acc: 99.71% | Val Loss: 0.0935, Val Acc: 98.06%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 26/100 | Time: 210.16s | Train Loss: 0.0170, Train Acc: 99.73% | Val Loss: 0.0947, Val Acc: 98.04%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 27/100 | Time: 217.31s | Train Loss: 0.0166, Train Acc: 99.73% | Val Loss: 0.0954, Val Acc: 98.06%\n",
            "\n",
            "Reducing learning rate from 8.00e-06 to 1.60e-06\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 28/100 | Time: 216.14s | Train Loss: 0.0161, Train Acc: 99.74% | Val Loss: 0.0965, Val Acc: 98.05%\n",
            "\n",
            "Early stopping at epoch 28\n",
            "Restored model weights from the end of the best epoch: 21\n",
            "--- Training Finished ---\n",
            "\n",
            "--- Starting Comprehensive Evaluation on Test Set ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating: 100%|██████████| 739/739 [00:27<00:00, 27.33it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Top-K Accuracy Results ---\n",
            "Top-1 Accuracy: 98.15%\n",
            "Top-3 Accuracy: 99.46%\n",
            "Top-5 Accuracy: 99.72%\n",
            "\n",
            "--- Additional Performance Metrics ---\n",
            "Macro Average Precision: 0.9808\n",
            "Macro Average Recall: 0.9806\n",
            "Macro Average F1-Score: 0.9807\n",
            "Weighted Average Precision: 0.9815\n",
            "Weighted Average Recall: 0.9815\n",
            "Weighted Average F1-Score: 0.9815\n",
            "\n",
            "--- Model Confidence & Calibration Metrics ---\n",
            "Average Prediction Confidence: 0.9934\n",
            "Confidence Standard Deviation: 0.0457\n",
            "Average Prediction Entropy: 0.0219\n",
            "Expected Calibration Error: 0.0119\n",
            "\n",
            "Final Test Loss: 0.0934\n",
            "===============================================\n"
          ]
        }
      ]
    }
  ]
}