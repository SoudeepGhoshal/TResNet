{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SoudeepGhoshal/TResNet/blob/main/TResNet_MedMNIST.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MedMNIST (TissueMNIST)"
      ],
      "metadata": {
        "id": "tQ5IyOHZOwUq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o5HMUaZBN2c4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab73e289-f1af-4870-8c47-a99d2ecadf9d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting medmnist\n",
            "  Downloading medmnist-3.0.2-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from medmnist) (2.0.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from medmnist) (2.2.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from medmnist) (1.6.1)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.11/dist-packages (from medmnist) (0.25.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from medmnist) (4.67.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from medmnist) (11.3.0)\n",
            "Collecting fire (from medmnist)\n",
            "  Downloading fire-0.7.0.tar.gz (87 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.2/87.2 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from medmnist) (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (from medmnist) (0.21.0+cu124)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.11/dist-packages (from fire->medmnist) (3.1.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->medmnist) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->medmnist) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->medmnist) (2025.2)\n",
            "Requirement already satisfied: scipy>=1.11.4 in /usr/local/lib/python3.11/dist-packages (from scikit-image->medmnist) (1.16.0)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.11/dist-packages (from scikit-image->medmnist) (3.5)\n",
            "Requirement already satisfied: imageio!=2.35.0,>=2.33 in /usr/local/lib/python3.11/dist-packages (from scikit-image->medmnist) (2.37.0)\n",
            "Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.11/dist-packages (from scikit-image->medmnist) (2025.6.11)\n",
            "Requirement already satisfied: packaging>=21 in /usr/local/lib/python3.11/dist-packages (from scikit-image->medmnist) (25.0)\n",
            "Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.11/dist-packages (from scikit-image->medmnist) (0.4)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->medmnist) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->medmnist) (3.6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->medmnist) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch->medmnist) (4.14.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->medmnist) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->medmnist) (2025.3.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch->medmnist)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch->medmnist)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch->medmnist)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch->medmnist)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch->medmnist)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch->medmnist)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch->medmnist)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch->medmnist)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch->medmnist)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->medmnist) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->medmnist) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->medmnist) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch->medmnist)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch->medmnist) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->medmnist) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->medmnist) (1.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->medmnist) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->medmnist) (3.0.2)\n",
            "Downloading medmnist-3.0.2-py3-none-any.whl (25 kB)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m60.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m29.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m39.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m102.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: fire\n",
            "  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fire: filename=fire-0.7.0-py3-none-any.whl size=114249 sha256=4d31acda21c3ec6161fdc894496825669f2acde0c4c2e44509039d9e98642613\n",
            "  Stored in directory: /root/.cache/pip/wheels/46/54/24/1624fd5b8674eb1188623f7e8e17cdf7c0f6c24b609dfb8a89\n",
            "Successfully built fire\n",
            "Installing collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, fire, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, medmnist\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed fire-0.7.0 medmnist-3.0.2 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"
          ]
        }
      ],
      "source": [
        "!pip install medmnist"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ResNet-18"
      ],
      "metadata": {
        "id": "neuxzlykN_fM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.datasets import ImageFolder\n",
        "import os\n",
        "import requests\n",
        "import zipfile\n",
        "import shutil\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "import sys\n",
        "import numpy as np\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "import medmnist\n",
        "from medmnist import INFO\n",
        "\n",
        "# --- Configuration Parameters ---\n",
        "CONFIG = {\n",
        "    \"batch_size\": 128,\n",
        "    \"num_epochs\": 100,\n",
        "    \"learning_rate\": 0.001,\n",
        "    \"weight_decay\": 1e-4,\n",
        "    \"split_seed\": 42, # Seed for creating reproducible dataset splits\n",
        "    \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
        "    \"checkpoint_path\": \"./best_model.pth\",  # Added checkpoint path\n",
        "}\n",
        "\n",
        "# ==========================================\n",
        "# Callback Classes\n",
        "# ==========================================\n",
        "\n",
        "class Callback:\n",
        "    \"\"\"Base callback class\"\"\"\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        pass\n",
        "\n",
        "    def on_training_end(self):\n",
        "        pass\n",
        "\n",
        "class ReduceLROnPlateau(Callback):\n",
        "    \"\"\"Reduce learning rate when a metric has stopped improving\"\"\"\n",
        "    def __init__(self, optimizer, monitor='val_accuracy', factor=0.2, patience=3, min_lr=1e-7, verbose=1):\n",
        "        self.optimizer = optimizer\n",
        "        self.monitor = monitor\n",
        "        self.factor = factor\n",
        "        self.patience = patience\n",
        "        self.min_lr = min_lr\n",
        "        self.verbose = verbose\n",
        "        self.wait = 0\n",
        "        self.best = None\n",
        "        self.mode = 'min' if 'loss' in monitor else 'max'\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        if logs is None:\n",
        "            return\n",
        "\n",
        "        current = logs.get(self.monitor)\n",
        "        if current is None:\n",
        "            return\n",
        "\n",
        "        if self.best is None:\n",
        "            self.best = current\n",
        "            return\n",
        "\n",
        "        if self.mode == 'min':\n",
        "            improved = current < self.best\n",
        "        else:\n",
        "            improved = current > self.best\n",
        "\n",
        "        if improved:\n",
        "            self.best = current\n",
        "            self.wait = 0\n",
        "        else:\n",
        "            self.wait += 1\n",
        "            if self.wait >= self.patience:\n",
        "                old_lr = self.optimizer.param_groups[0]['lr']\n",
        "                new_lr = max(old_lr * self.factor, self.min_lr)\n",
        "                if new_lr != old_lr:\n",
        "                    self.optimizer.param_groups[0]['lr'] = new_lr\n",
        "                    if self.verbose:\n",
        "                        print(f\"\\nReducing learning rate from {old_lr:.2e} to {new_lr:.2e}\")\n",
        "                    self.wait = 0\n",
        "\n",
        "class EarlyStopping(Callback):\n",
        "    \"\"\"Stop training when a monitored metric has stopped improving\"\"\"\n",
        "    def __init__(self, monitor='val_accuracy', patience=7, restore_best_weights=True, verbose=1):\n",
        "        self.monitor = monitor\n",
        "        self.patience = patience\n",
        "        self.restore_best_weights = restore_best_weights\n",
        "        self.verbose = verbose\n",
        "        self.wait = 0\n",
        "        self.best = None\n",
        "        self.best_weights = None\n",
        "        self.mode = 'min' if 'loss' in monitor else 'max'\n",
        "        self.stopped_epoch = 0\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None, model=None):\n",
        "        if logs is None:\n",
        "            return False\n",
        "\n",
        "        current = logs.get(self.monitor)\n",
        "        if current is None:\n",
        "            return False\n",
        "\n",
        "        if self.best is None:\n",
        "            self.best = current\n",
        "            if model is not None and self.restore_best_weights:\n",
        "                self.best_weights = model.state_dict().copy()\n",
        "            return False\n",
        "\n",
        "        if self.mode == 'min':\n",
        "            improved = current < self.best\n",
        "        else:\n",
        "            improved = current > self.best\n",
        "\n",
        "        if improved:\n",
        "            self.best = current\n",
        "            self.wait = 0\n",
        "            if model is not None and self.restore_best_weights:\n",
        "                self.best_weights = model.state_dict().copy()\n",
        "        else:\n",
        "            self.wait += 1\n",
        "            if self.wait >= self.patience:\n",
        "                self.stopped_epoch = epoch\n",
        "                if self.verbose:\n",
        "                    print(f\"\\nEarly stopping at epoch {epoch + 1}\")\n",
        "                return True\n",
        "        return False\n",
        "\n",
        "    def on_training_end(self, model=None):\n",
        "        if self.stopped_epoch > 0 and self.verbose:\n",
        "            print(f\"Restored model weights from the end of the best epoch: {self.stopped_epoch + 1 - self.patience}\")\n",
        "        if model is not None and self.restore_best_weights and self.best_weights is not None:\n",
        "            model.load_state_dict(self.best_weights)\n",
        "\n",
        "class ModelCheckpoint(Callback):\n",
        "    \"\"\"Save the model after every epoch\"\"\"\n",
        "    def __init__(self, filepath, save_best_only=True, monitor='val_accuracy', verbose=1):\n",
        "        self.filepath = filepath\n",
        "        self.save_best_only = save_best_only\n",
        "        self.monitor = monitor\n",
        "        self.verbose = verbose\n",
        "        self.best = None\n",
        "        self.mode = 'min' if 'loss' in monitor else 'max'\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None, model=None):\n",
        "        if logs is None or model is None:\n",
        "            return\n",
        "\n",
        "        current = logs.get(self.monitor)\n",
        "        if current is None:\n",
        "            return\n",
        "\n",
        "        if not self.save_best_only:\n",
        "            filepath = self.filepath.replace('.pth', f'_epoch_{epoch + 1}.pth')\n",
        "            torch.save(model.state_dict(), filepath)\n",
        "            if self.verbose:\n",
        "                print(f\"\\nModel saved to {filepath}\")\n",
        "            return\n",
        "\n",
        "        if self.best is None:\n",
        "            self.best = current\n",
        "            torch.save(model.state_dict(), self.filepath)\n",
        "            if self.verbose:\n",
        "                print(f\"\\nModel saved to {self.filepath}\")\n",
        "            return\n",
        "\n",
        "        if self.mode == 'min':\n",
        "            improved = current < self.best\n",
        "        else:\n",
        "            improved = current > self.best\n",
        "\n",
        "        if improved:\n",
        "            self.best = current\n",
        "            torch.save(model.state_dict(), self.filepath)\n",
        "            if self.verbose:\n",
        "                print(f\"\\nModel saved to {self.filepath}\")\n",
        "\n",
        "# ==========================================\n",
        "# Data Preprocessing\n",
        "# ==========================================\n",
        "\n",
        "def get_dataloaders(config):\n",
        "    \"\"\"Downloads, prepares MedMNIST data with augmentation, returning DataLoaders.\"\"\"\n",
        "\n",
        "    # MedMNIST dataset configuration\n",
        "    dataset_name = \"tissuemnist\"  # Options: pathmnist, chestmnist, dermamnist, octmnist, pneumoniamnist, retinamnist, breastmnist, bloodmnist, tissuemnist, organamnist, organcmnist, organsmnist\n",
        "\n",
        "    # Get dataset info\n",
        "    info = INFO[dataset_name]\n",
        "    task = info['task']\n",
        "    n_channels = info['n_channels']\n",
        "    n_classes = len(info['label'])\n",
        "\n",
        "    print(f\"Dataset: {dataset_name}\")\n",
        "    print(f\"Task: {task}\")\n",
        "    print(f\"Number of channels: {n_channels}\")\n",
        "    print(f\"Number of classes: {n_classes}\")\n",
        "\n",
        "    # Update config with dataset info\n",
        "    config[\"num_classes\"] = n_classes\n",
        "\n",
        "    # Get the dataset class\n",
        "    DataClass = getattr(medmnist, info['python_class'])\n",
        "\n",
        "    # Define transforms\n",
        "    # Training transforms (with augmentation)\n",
        "    train_transform = transforms.Compose([\n",
        "        transforms.Resize((64, 64)),  # Resize to match original Tiny ImageNet size\n",
        "        transforms.RandomRotation(degrees=10),\n",
        "        transforms.RandomHorizontalFlip(p=0.5),\n",
        "        transforms.ColorJitter(brightness=0.1, contrast=0.1),\n",
        "        transforms.ToTensor(),\n",
        "        # Convert grayscale to RGB if needed\n",
        "        transforms.Lambda(lambda x: x.repeat(3, 1, 1) if x.size(0) == 1 else x),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "    # Validation/Test transforms (no augmentation)\n",
        "    val_test_transform = transforms.Compose([\n",
        "        transforms.Resize((64, 64)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Lambda(lambda x: x.repeat(3, 1, 1) if x.size(0) == 1 else x),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "    print(f\"Loading {dataset_name} dataset...\")\n",
        "\n",
        "    # Load datasets - MedMNIST provides predefined train/val/test splits\n",
        "    train_dataset = DataClass(\n",
        "        split='train',\n",
        "        transform=train_transform,\n",
        "        download=True,  # Will download if not present\n",
        "        as_rgb=False    # We handle RGB conversion in transforms\n",
        "    )\n",
        "\n",
        "    val_dataset = DataClass(\n",
        "        split='val',\n",
        "        transform=val_test_transform,\n",
        "        download=True,\n",
        "        as_rgb=False\n",
        "    )\n",
        "\n",
        "    test_dataset = DataClass(\n",
        "        split='test',\n",
        "        transform=val_test_transform,\n",
        "        download=True,\n",
        "        as_rgb=False\n",
        "    )\n",
        "\n",
        "    print(f\"Dataset splits - Train: {len(train_dataset)}, Val: {len(val_dataset)}, Test: {len(test_dataset)}\")\n",
        "\n",
        "    # Handle different task types\n",
        "    if task == 'multi-label, binary-class':\n",
        "        def collate_fn(batch):\n",
        "            images = torch.stack([item[0] for item in batch])\n",
        "            # Fix: Convert to numpy array first, then to tensor\n",
        "            labels_np = np.array([item[1] for item in batch])\n",
        "            labels = torch.tensor(labels_np, dtype=torch.float32).squeeze()\n",
        "            return images, labels\n",
        "    else:\n",
        "        def collate_fn(batch):\n",
        "            images = torch.stack([item[0] for item in batch])\n",
        "            # Fix: Convert to numpy array first, then to tensor\n",
        "            labels_np = np.array([item[1] for item in batch])\n",
        "            labels = torch.tensor(labels_np, dtype=torch.long).squeeze()\n",
        "            return images, labels\n",
        "\n",
        "    # Set worker init function for reproducible DataLoader behavior\n",
        "    def worker_init_fn(worker_id):\n",
        "        np.random.seed(config[\"split_seed\"] + worker_id)\n",
        "\n",
        "    # Create DataLoaders\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=config[\"batch_size\"],\n",
        "        shuffle=True,\n",
        "        num_workers=2,\n",
        "        pin_memory=True,\n",
        "        worker_init_fn=worker_init_fn,\n",
        "        collate_fn=collate_fn,\n",
        "        generator=torch.Generator().manual_seed(config[\"split_seed\"])\n",
        "    )\n",
        "\n",
        "    val_loader = DataLoader(\n",
        "        val_dataset,\n",
        "        batch_size=config[\"batch_size\"],\n",
        "        shuffle=False,\n",
        "        num_workers=2,\n",
        "        pin_memory=True,\n",
        "        worker_init_fn=worker_init_fn,\n",
        "        collate_fn=collate_fn\n",
        "    )\n",
        "\n",
        "    test_loader = DataLoader(\n",
        "        test_dataset,\n",
        "        batch_size=config[\"batch_size\"],\n",
        "        shuffle=False,\n",
        "        num_workers=2,\n",
        "        pin_memory=True,\n",
        "        worker_init_fn=worker_init_fn,\n",
        "        collate_fn=collate_fn\n",
        "    )\n",
        "\n",
        "    return train_loader, val_loader, test_loader\n",
        "\n",
        "# ==========================================\n",
        "# Model Architecture and Training\n",
        "# ==========================================\n",
        "\n",
        "class ResidualBlock(nn.Module):\n",
        "    \"\"\"A residual block, the fundamental building block of ResNet.\"\"\"\n",
        "    def __init__(self, in_channels, out_channels, stride=1):\n",
        "        super(ResidualBlock, self).__init__()\n",
        "        # Main path\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "\n",
        "        # Shortcut path (for matching dimensions)\n",
        "        self.downsample = nn.Sequential()\n",
        "        if stride != 1 or in_channels != out_channels:\n",
        "            self.downsample = nn.Sequential(\n",
        "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(out_channels)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = self.downsample(x)\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        out += identity\n",
        "        out = self.relu(out)\n",
        "        return out\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "    \"\"\"A modular ResNet implementation.\"\"\"\n",
        "    def __init__(self, block, layers, num_classes=200):\n",
        "        super(ResNet, self).__init__()\n",
        "        self.in_channels = 64\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "        self.layer1 = self._make_layer(block, 64, layers[0], stride=1)\n",
        "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n",
        "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Linear(512, num_classes)\n",
        "\n",
        "    def _make_layer(self, block, out_channels, num_blocks, stride):\n",
        "        strides = [stride] + [1] * (num_blocks - 1)\n",
        "        layers = []\n",
        "        for s in strides:\n",
        "            layers.append(block(self.in_channels, out_channels, s))\n",
        "            self.in_channels = out_channels\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "        x = self.avgpool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "def ResNet18(num_classes=200):\n",
        "    return ResNet(ResidualBlock, [2, 2, 2, 2], num_classes)\n",
        "\n",
        "def evaluate_model(model, data_loader, criterion, device):\n",
        "    \"\"\"Evaluates the model on a given dataset.\"\"\"\n",
        "    model.eval()\n",
        "    total_loss, correct, total = 0.0, 0, 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in data_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            total_loss += loss.item() * images.size(0)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    avg_loss = total_loss / total\n",
        "    accuracy = 100 * correct / total\n",
        "    return avg_loss, accuracy\n",
        "\n",
        "def train_and_validate(model, train_loader, val_loader, config):\n",
        "    \"\"\"Main training loop with callbacks.\"\"\"\n",
        "    device = config[\"device\"]\n",
        "    model.to(device)\n",
        "\n",
        "    # Set seeds for reproducible training\n",
        "    torch.manual_seed(config[\"split_seed\"])\n",
        "    np.random.seed(config[\"split_seed\"])\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(config[\"split_seed\"])\n",
        "        torch.cuda.manual_seed_all(config[\"split_seed\"])\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=config[\"learning_rate\"], weight_decay=config[\"weight_decay\"])\n",
        "\n",
        "    # Initialize callbacks\n",
        "    callbacks = [\n",
        "        ReduceLROnPlateau(\n",
        "            optimizer=optimizer,\n",
        "            monitor='val_accuracy',\n",
        "            factor=0.2,\n",
        "            patience=3,\n",
        "            min_lr=1e-7,\n",
        "            verbose=1\n",
        "        ),\n",
        "        EarlyStopping(\n",
        "            monitor='val_accuracy',\n",
        "            patience=7,\n",
        "            restore_best_weights=True,\n",
        "            verbose=1\n",
        "        ),\n",
        "        ModelCheckpoint(\n",
        "            filepath=config[\"checkpoint_path\"],\n",
        "            save_best_only=True,\n",
        "            monitor='val_accuracy',\n",
        "            verbose=1\n",
        "        )\n",
        "    ]\n",
        "\n",
        "    print(\"\\n--- Starting Training ---\")\n",
        "    early_stop = False\n",
        "\n",
        "    for epoch in range(config[\"num_epochs\"]):\n",
        "        if early_stop:\n",
        "            break\n",
        "\n",
        "        model.train()\n",
        "        start_time = time.time()\n",
        "        running_loss, train_correct, train_total = 0.0, 0, 0\n",
        "\n",
        "        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{config['num_epochs']} [Train]\", leave=False)\n",
        "        for images, labels in progress_bar:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item() * images.size(0)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            train_total += labels.size(0)\n",
        "            train_correct += (predicted == labels).sum().item()\n",
        "            progress_bar.set_postfix(loss=loss.item())\n",
        "\n",
        "        train_loss = running_loss / train_total\n",
        "        train_acc = 100 * train_correct / train_total\n",
        "        val_loss, val_acc = evaluate_model(model, val_loader, criterion, device)\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{config['num_epochs']} | Time: {time.time() - start_time:.2f}s | \"\n",
        "              f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}% | \"\n",
        "              f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%\")\n",
        "\n",
        "        # Prepare logs for callbacks\n",
        "        logs = {\n",
        "            'train_loss': train_loss,\n",
        "            'train_accuracy': train_acc,\n",
        "            'val_loss': val_loss,\n",
        "            'val_accuracy': val_acc\n",
        "        }\n",
        "\n",
        "        # Execute callbacks\n",
        "        for callback in callbacks:\n",
        "            if isinstance(callback, EarlyStopping):\n",
        "                if callback.on_epoch_end(epoch, logs, model):\n",
        "                    early_stop = True\n",
        "                    break\n",
        "            elif isinstance(callback, ModelCheckpoint):\n",
        "                callback.on_epoch_end(epoch, logs, model)\n",
        "            else:\n",
        "                callback.on_epoch_end(epoch, logs)\n",
        "\n",
        "    # Execute callback cleanup\n",
        "    for callback in callbacks:\n",
        "        if isinstance(callback, EarlyStopping):\n",
        "            callback.on_training_end(model)\n",
        "        else:\n",
        "            callback.on_training_end()\n",
        "\n",
        "    print(\"--- Training Finished ---\\n\")\n",
        "    return model\n",
        "\n",
        "# ==========================================\n",
        "# Evaluation\n",
        "# ==========================================\n",
        "\n",
        "def calculate_top_k_accuracy(outputs, labels, k_values=[1, 3, 5]):\n",
        "    \"\"\"Calculate top-k accuracy for given k values.\"\"\"\n",
        "    batch_size = labels.size(0)\n",
        "    _, pred = outputs.topk(max(k_values), 1, True, True)\n",
        "    pred = pred.t()\n",
        "    correct = pred.eq(labels.view(1, -1).expand_as(pred))\n",
        "\n",
        "    top_k_accuracies = {}\n",
        "    for k in k_values:\n",
        "        correct_k = correct[:k].reshape(-1).float().sum(0, keepdim=True)\n",
        "        top_k_accuracies[k] = correct_k.item() / batch_size\n",
        "\n",
        "    return top_k_accuracies\n",
        "\n",
        "def calculate_entropy(probs):\n",
        "    \"\"\"Calculate entropy of probability distributions.\"\"\"\n",
        "    # Add small epsilon to avoid log(0)\n",
        "    epsilon = 1e-8\n",
        "    probs = probs + epsilon\n",
        "    entropy = -torch.sum(probs * torch.log(probs), dim=1)\n",
        "    return entropy\n",
        "\n",
        "def calculate_ece(confidences, accuracies, n_bins=10):\n",
        "    \"\"\"Calculate Expected Calibration Error.\"\"\"\n",
        "    bin_boundaries = torch.linspace(0, 1, n_bins + 1)\n",
        "    bin_lowers = bin_boundaries[:-1]\n",
        "    bin_uppers = bin_boundaries[1:]\n",
        "\n",
        "    ece = 0\n",
        "    for bin_lower, bin_upper in zip(bin_lowers, bin_uppers):\n",
        "        in_bin = (confidences > bin_lower.item()) & (confidences <= bin_upper.item())\n",
        "        prop_in_bin = in_bin.float().mean()\n",
        "\n",
        "        if prop_in_bin.item() > 0:\n",
        "            accuracy_in_bin = accuracies[in_bin].float().mean()\n",
        "            avg_confidence_in_bin = confidences[in_bin].mean()\n",
        "            ece += torch.abs(avg_confidence_in_bin - accuracy_in_bin) * prop_in_bin\n",
        "\n",
        "    return ece.item()\n",
        "\n",
        "def comprehensive_evaluation(model, test_loader, criterion, device):\n",
        "    \"\"\"Comprehensive evaluation with all requested metrics.\"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    all_predictions = []\n",
        "    all_labels = []\n",
        "    all_confidences = []\n",
        "    all_entropies = []\n",
        "    all_top_k_results = {1: [], 3: [], 5: []}\n",
        "    total_loss = 0.0\n",
        "    total_samples = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in tqdm(test_loader, desc=\"Evaluating\"):\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            # Convert outputs to probabilities\n",
        "            probs = torch.softmax(outputs, dim=1)\n",
        "\n",
        "            # Get predictions and confidences\n",
        "            confidences, predictions = torch.max(probs, dim=1)\n",
        "\n",
        "            # Calculate entropy\n",
        "            entropies = calculate_entropy(probs)\n",
        "\n",
        "            # Calculate top-k accuracies\n",
        "            top_k_accs = calculate_top_k_accuracy(outputs, labels, [1, 3, 5])\n",
        "\n",
        "            # Store results\n",
        "            all_predictions.extend(predictions.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "            all_confidences.extend(confidences.cpu().numpy())\n",
        "            all_entropies.extend(entropies.cpu().numpy())\n",
        "\n",
        "            for k in [1, 3, 5]:\n",
        "                all_top_k_results[k].extend([top_k_accs[k]] * labels.size(0))\n",
        "\n",
        "            total_loss += loss.item() * images.size(0)\n",
        "            total_samples += images.size(0)\n",
        "\n",
        "    # Convert to numpy arrays\n",
        "    all_predictions = np.array(all_predictions)\n",
        "    all_labels = np.array(all_labels)\n",
        "    all_confidences = np.array(all_confidences)\n",
        "    all_entropies = np.array(all_entropies)\n",
        "\n",
        "    # Calculate basic metrics\n",
        "    avg_loss = total_loss / total_samples\n",
        "\n",
        "    # Calculate top-k accuracies\n",
        "    top1 = np.mean([pred == label for pred, label in zip(all_predictions, all_labels)])\n",
        "    top3 = np.mean(all_top_k_results[3])\n",
        "    top5 = np.mean(all_top_k_results[5])\n",
        "\n",
        "    # Calculate precision, recall, f1-score\n",
        "    precision_macro, recall_macro, f1_macro, _ = precision_recall_fscore_support(\n",
        "        all_labels, all_predictions, average='macro', zero_division=0\n",
        "    )\n",
        "    precision_weighted, recall_weighted, f1_weighted, _ = precision_recall_fscore_support(\n",
        "        all_labels, all_predictions, average='weighted', zero_division=0\n",
        "    )\n",
        "\n",
        "    # Calculate confidence and calibration metrics\n",
        "    avg_confidence = np.mean(all_confidences)\n",
        "    std_confidence = np.std(all_confidences)\n",
        "    avg_entropy = np.mean(all_entropies)\n",
        "\n",
        "    # Calculate ECE\n",
        "    accuracies = (all_predictions == all_labels).astype(float)\n",
        "    ece = calculate_ece(torch.tensor(all_confidences), torch.tensor(accuracies))\n",
        "\n",
        "    return {\n",
        "        'test_loss': avg_loss,\n",
        "        'top1': top1,\n",
        "        'top3': top3,\n",
        "        'top5': top5,\n",
        "        'precision_macro': precision_macro,\n",
        "        'recall_macro': recall_macro,\n",
        "        'f1_macro': f1_macro,\n",
        "        'precision_weighted': precision_weighted,\n",
        "        'recall_weighted': recall_weighted,\n",
        "        'f1_weighted': f1_weighted,\n",
        "        'avg_confidence': avg_confidence,\n",
        "        'std_confidence': std_confidence,\n",
        "        'avg_entropy': avg_entropy,\n",
        "        'ece': ece\n",
        "    }\n",
        "\n",
        "# ==========================================\n",
        "# Main Execution\n",
        "# ==========================================\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # Set global seeds for full reproducibility\n",
        "    torch.manual_seed(CONFIG[\"split_seed\"])\n",
        "    np.random.seed(CONFIG[\"split_seed\"])\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(CONFIG[\"split_seed\"])\n",
        "        torch.cuda.manual_seed_all(CONFIG[\"split_seed\"])\n",
        "        # Ensure deterministic behavior on CUDA\n",
        "        torch.backends.cudnn.deterministic = True\n",
        "        torch.backends.cudnn.benchmark = False\n",
        "\n",
        "    print(f\"Using device: {CONFIG['device']}\")\n",
        "    print(f\"Reproducibility seed: {CONFIG['split_seed']}\")\n",
        "\n",
        "    # Get data loaders\n",
        "    dataloaders = get_dataloaders(CONFIG)\n",
        "    if dataloaders is None:\n",
        "        print(\"Could not prepare data. Halting execution.\", file=sys.stderr)\n",
        "        sys.exit(1) # Exit if data preparation failed\n",
        "\n",
        "    train_loader, val_loader, test_loader = dataloaders\n",
        "\n",
        "    # Initialize and train model\n",
        "    model = ResNet18(num_classes=CONFIG[\"num_classes\"])\n",
        "    trained_model = train_and_validate(model, train_loader, val_loader, CONFIG)\n",
        "\n",
        "    # Comprehensive evaluation on the test set\n",
        "    print(\"--- Starting Comprehensive Evaluation on Test Set ---\")\n",
        "    results = comprehensive_evaluation(trained_model, test_loader, nn.CrossEntropyLoss(), CONFIG[\"device\"])\n",
        "\n",
        "    # Print all requested metrics\n",
        "    print(\"\\n--- Top-K Accuracy Results ---\")\n",
        "    print(f\"Top-1 Accuracy: {results['top1'] * 100:.2f}%\")\n",
        "    print(f\"Top-3 Accuracy: {results['top3'] * 100:.2f}%\")\n",
        "    print(f\"Top-5 Accuracy: {results['top5'] * 100:.2f}%\")\n",
        "\n",
        "    print(\"\\n--- Additional Performance Metrics ---\")\n",
        "    print(f\"Macro Average Precision: {results['precision_macro']:.4f}\")\n",
        "    print(f\"Macro Average Recall: {results['recall_macro']:.4f}\")\n",
        "    print(f\"Macro Average F1-Score: {results['f1_macro']:.4f}\")\n",
        "    print(f\"Weighted Average Precision: {results['precision_weighted']:.4f}\")\n",
        "    print(f\"Weighted Average Recall: {results['recall_weighted']:.4f}\")\n",
        "    print(f\"Weighted Average F1-Score: {results['f1_weighted']:.4f}\")\n",
        "\n",
        "    print(\"\\n--- Model Confidence & Calibration Metrics ---\")\n",
        "    print(f\"Average Prediction Confidence: {results['avg_confidence']:.4f}\")\n",
        "    print(f\"Confidence Standard Deviation: {results['std_confidence']:.4f}\")\n",
        "    print(f\"Average Prediction Entropy: {results['avg_entropy']:.4f}\")\n",
        "    print(f\"Expected Calibration Error: {results['ece']:.4f}\")\n",
        "\n",
        "    print(f\"\\nFinal Test Loss: {results['test_loss']:.4f}\")\n",
        "    print(\"===============================================\")"
      ],
      "metadata": {
        "id": "NsP5FJN9OClL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d667e43b-528d-482b-a56f-e4665ab1b582"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Reproducibility seed: 42\n",
            "Dataset: tissuemnist\n",
            "Task: multi-class\n",
            "Number of channels: 1\n",
            "Number of classes: 8\n",
            "Loading tissuemnist dataset...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 125M/125M [01:26<00:00, 1.44MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset splits - Train: 165466, Val: 23640, Test: 47280\n",
            "\n",
            "--- Starting Training ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100 | Time: 145.07s | Train Loss: 1.1916, Train Acc: 55.60% | Val Loss: 1.2253, Val Acc: 55.77%\n",
            "\n",
            "Model saved to ./best_model.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/100 | Time: 144.43s | Train Loss: 1.0581, Train Acc: 60.93% | Val Loss: 1.0597, Val Acc: 60.96%\n",
            "\n",
            "Model saved to ./best_model.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/100 | Time: 142.79s | Train Loss: 1.0069, Train Acc: 62.78% | Val Loss: 1.0654, Val Acc: 60.77%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4/100 | Time: 141.31s | Train Loss: 0.9772, Train Acc: 64.00% | Val Loss: 0.9683, Val Acc: 64.40%\n",
            "\n",
            "Model saved to ./best_model.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5/100 | Time: 146.48s | Train Loss: 0.9553, Train Acc: 64.86% | Val Loss: 0.9897, Val Acc: 63.35%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6/100 | Time: 143.49s | Train Loss: 0.9359, Train Acc: 65.56% | Val Loss: 1.0389, Val Acc: 61.75%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7/100 | Time: 142.67s | Train Loss: 0.9217, Train Acc: 66.10% | Val Loss: 1.0905, Val Acc: 59.41%\n",
            "\n",
            "Reducing learning rate from 1.00e-03 to 2.00e-04\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8/100 | Time: 143.38s | Train Loss: 0.8469, Train Acc: 68.94% | Val Loss: 0.8566, Val Acc: 68.82%\n",
            "\n",
            "Model saved to ./best_model.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9/100 | Time: 142.13s | Train Loss: 0.8236, Train Acc: 69.91% | Val Loss: 0.8681, Val Acc: 68.26%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10/100 | Time: 142.56s | Train Loss: 0.8095, Train Acc: 70.36% | Val Loss: 0.8455, Val Acc: 69.10%\n",
            "\n",
            "Model saved to ./best_model.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 11/100 | Time: 141.31s | Train Loss: 0.7972, Train Acc: 70.91% | Val Loss: 0.8388, Val Acc: 69.42%\n",
            "\n",
            "Model saved to ./best_model.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 12/100 | Time: 142.06s | Train Loss: 0.7880, Train Acc: 71.28% | Val Loss: 0.8687, Val Acc: 68.13%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 13/100 | Time: 140.76s | Train Loss: 0.7767, Train Acc: 71.68% | Val Loss: 0.8371, Val Acc: 69.60%\n",
            "\n",
            "Model saved to ./best_model.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 14/100 | Time: 141.05s | Train Loss: 0.7656, Train Acc: 72.06% | Val Loss: 0.8416, Val Acc: 69.51%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 15/100 | Time: 141.95s | Train Loss: 0.7573, Train Acc: 72.38% | Val Loss: 0.8416, Val Acc: 69.70%\n",
            "\n",
            "Model saved to ./best_model.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 16/100 | Time: 142.20s | Train Loss: 0.7458, Train Acc: 72.75% | Val Loss: 0.8877, Val Acc: 68.40%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 17/100 | Time: 141.41s | Train Loss: 0.7338, Train Acc: 73.17% | Val Loss: 0.8796, Val Acc: 68.79%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 18/100 | Time: 140.17s | Train Loss: 0.7223, Train Acc: 73.58% | Val Loss: 0.8495, Val Acc: 69.56%\n",
            "\n",
            "Reducing learning rate from 2.00e-04 to 4.00e-05\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 19/100 | Time: 138.62s | Train Loss: 0.6771, Train Acc: 75.39% | Val Loss: 0.8440, Val Acc: 70.00%\n",
            "\n",
            "Model saved to ./best_model.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 20/100 | Time: 139.01s | Train Loss: 0.6602, Train Acc: 76.01% | Val Loss: 0.8543, Val Acc: 69.98%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 21/100 | Time: 139.16s | Train Loss: 0.6530, Train Acc: 76.23% | Val Loss: 0.8577, Val Acc: 69.82%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 22/100 | Time: 138.35s | Train Loss: 0.6429, Train Acc: 76.70% | Val Loss: 0.8608, Val Acc: 69.90%\n",
            "\n",
            "Reducing learning rate from 4.00e-05 to 8.00e-06\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 23/100 | Time: 138.82s | Train Loss: 0.6287, Train Acc: 77.25% | Val Loss: 0.8631, Val Acc: 69.86%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 24/100 | Time: 139.83s | Train Loss: 0.6250, Train Acc: 77.32% | Val Loss: 0.8646, Val Acc: 69.89%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 25/100 | Time: 139.03s | Train Loss: 0.6232, Train Acc: 77.42% | Val Loss: 0.8687, Val Acc: 69.75%\n",
            "\n",
            "Reducing learning rate from 8.00e-06 to 1.60e-06\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 26/100 | Time: 138.84s | Train Loss: 0.6191, Train Acc: 77.54% | Val Loss: 0.8675, Val Acc: 69.79%\n",
            "\n",
            "Early stopping at epoch 26\n",
            "Restored model weights from the end of the best epoch: 19\n",
            "--- Training Finished ---\n",
            "\n",
            "--- Starting Comprehensive Evaluation on Test Set ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating: 100%|██████████| 370/370 [00:17<00:00, 21.05it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Top-K Accuracy Results ---\n",
            "Top-1 Accuracy: 69.88%\n",
            "Top-3 Accuracy: 93.83%\n",
            "Top-5 Accuracy: 98.55%\n",
            "\n",
            "--- Additional Performance Metrics ---\n",
            "Macro Average Precision: 0.6306\n",
            "Macro Average Recall: 0.6091\n",
            "Macro Average F1-Score: 0.6167\n",
            "Weighted Average Precision: 0.6947\n",
            "Weighted Average Recall: 0.6988\n",
            "Weighted Average F1-Score: 0.6948\n",
            "\n",
            "--- Model Confidence & Calibration Metrics ---\n",
            "Average Prediction Confidence: 0.7657\n",
            "Confidence Standard Deviation: 0.2044\n",
            "Average Prediction Entropy: 0.6400\n",
            "Expected Calibration Error: 0.0669\n",
            "\n",
            "Final Test Loss: 0.8633\n",
            "===============================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hybrid Transformer on ResNet-18 (No PE)"
      ],
      "metadata": {
        "id": "aduQ-kthOEw1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.datasets import ImageFolder\n",
        "import os\n",
        "import requests\n",
        "import zipfile\n",
        "import shutil\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "import sys\n",
        "import numpy as np\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "import medmnist\n",
        "from medmnist import INFO\n",
        "\n",
        "# --- Configuration Parameters ---\n",
        "CONFIG = {\n",
        "    \"batch_size\": 128,\n",
        "    \"num_epochs\": 100,\n",
        "    \"learning_rate\": 0.001,\n",
        "    \"weight_decay\": 1e-4,\n",
        "    \"split_seed\": 42,\n",
        "    \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
        "    \"checkpoint_path\": \"./best_model.pth\",\n",
        "}\n",
        "\n",
        "# --- Transformer Configuration ---\n",
        "TRANSFORMER_CONFIG = {\n",
        "    \"embedding_dim\": 32,\n",
        "    \"nhead\": 16,\n",
        "    \"num_encoder_layers\": 3,\n",
        "    \"dim_feedforward\": 2,\n",
        "    \"dropout\": 0.1,\n",
        "}\n",
        "\n",
        "# ==========================================\n",
        "# Callback Classes\n",
        "# ==========================================\n",
        "\n",
        "class Callback:\n",
        "    \"\"\"Base callback class\"\"\"\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        pass\n",
        "\n",
        "    def on_training_end(self):\n",
        "        pass\n",
        "\n",
        "class ReduceLROnPlateau(Callback):\n",
        "    \"\"\"Reduce learning rate when a metric has stopped improving\"\"\"\n",
        "    def __init__(self, optimizer, monitor='val_loss', factor=0.2, patience=3, min_lr=1e-7, verbose=1):\n",
        "        self.optimizer = optimizer\n",
        "        self.monitor = monitor\n",
        "        self.factor = factor\n",
        "        self.patience = patience\n",
        "        self.min_lr = min_lr\n",
        "        self.verbose = verbose\n",
        "        self.wait = 0\n",
        "        self.best = None\n",
        "        self.mode = 'min' if 'loss' in monitor else 'max'\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        if logs is None:\n",
        "            return\n",
        "\n",
        "        current = logs.get(self.monitor)\n",
        "        if current is None:\n",
        "            return\n",
        "\n",
        "        if self.best is None:\n",
        "            self.best = current\n",
        "            return\n",
        "\n",
        "        if self.mode == 'min':\n",
        "            improved = current < self.best\n",
        "        else:\n",
        "            improved = current > self.best\n",
        "\n",
        "        if improved:\n",
        "            self.best = current\n",
        "            self.wait = 0\n",
        "        else:\n",
        "            self.wait += 1\n",
        "            if self.wait >= self.patience:\n",
        "                old_lr = self.optimizer.param_groups[0]['lr']\n",
        "                new_lr = max(old_lr * self.factor, self.min_lr)\n",
        "                if new_lr != old_lr:\n",
        "                    self.optimizer.param_groups[0]['lr'] = new_lr\n",
        "                    if self.verbose:\n",
        "                        print(f\"\\nReducing learning rate from {old_lr:.2e} to {new_lr:.2e}\")\n",
        "                    self.wait = 0\n",
        "\n",
        "class EarlyStopping(Callback):\n",
        "    \"\"\"Stop training when a monitored metric has stopped improving\"\"\"\n",
        "    def __init__(self, monitor='val_loss', patience=7, restore_best_weights=True, verbose=1):\n",
        "        self.monitor = monitor\n",
        "        self.patience = patience\n",
        "        self.restore_best_weights = restore_best_weights\n",
        "        self.verbose = verbose\n",
        "        self.wait = 0\n",
        "        self.best = None\n",
        "        self.best_weights = None\n",
        "        self.mode = 'min' if 'loss' in monitor else 'max'\n",
        "        self.stopped_epoch = 0\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None, model=None):\n",
        "        if logs is None:\n",
        "            return False\n",
        "\n",
        "        current = logs.get(self.monitor)\n",
        "        if current is None:\n",
        "            return False\n",
        "\n",
        "        if self.best is None:\n",
        "            self.best = current\n",
        "            if model is not None and self.restore_best_weights:\n",
        "                self.best_weights = model.state_dict().copy()\n",
        "            return False\n",
        "\n",
        "        if self.mode == 'min':\n",
        "            improved = current < self.best\n",
        "        else:\n",
        "            improved = current > self.best\n",
        "\n",
        "        if improved:\n",
        "            self.best = current\n",
        "            self.wait = 0\n",
        "            if model is not None and self.restore_best_weights:\n",
        "                self.best_weights = model.state_dict().copy()\n",
        "        else:\n",
        "            self.wait += 1\n",
        "            if self.wait >= self.patience:\n",
        "                self.stopped_epoch = epoch\n",
        "                if self.verbose:\n",
        "                    print(f\"\\nEarly stopping at epoch {epoch + 1}\")\n",
        "                return True\n",
        "        return False\n",
        "\n",
        "    def on_training_end(self, model=None):\n",
        "        if self.stopped_epoch > 0 and self.verbose:\n",
        "            print(f\"Restored model weights from the end of the best epoch: {self.stopped_epoch + 1 - self.patience}\")\n",
        "        if model is not None and self.restore_best_weights and self.best_weights is not None:\n",
        "            model.load_state_dict(self.best_weights)\n",
        "\n",
        "class ModelCheckpoint(Callback):\n",
        "    \"\"\"Save the model after every epoch\"\"\"\n",
        "    def __init__(self, filepath, save_best_only=True, monitor='val_accuracy', verbose=1):\n",
        "        self.filepath = filepath\n",
        "        self.save_best_only = save_best_only\n",
        "        self.monitor = monitor\n",
        "        self.verbose = verbose\n",
        "        self.best = None\n",
        "        self.mode = 'min' if 'loss' in monitor else 'max'\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None, model=None):\n",
        "        if logs is None or model is None:\n",
        "            return\n",
        "\n",
        "        current = logs.get(self.monitor)\n",
        "        if current is None:\n",
        "            return\n",
        "\n",
        "        if not self.save_best_only:\n",
        "            filepath = self.filepath.replace('.pth', f'_epoch_{epoch + 1}.pth')\n",
        "            torch.save(model.state_dict(), filepath)\n",
        "            if self.verbose:\n",
        "                print(f\"\\nModel saved to {filepath}\")\n",
        "            return\n",
        "\n",
        "        if self.best is None:\n",
        "            self.best = current\n",
        "            torch.save(model.state_dict(), self.filepath)\n",
        "            if self.verbose:\n",
        "                print(f\"\\nModel saved to {self.filepath}\")\n",
        "            return\n",
        "\n",
        "        if self.mode == 'min':\n",
        "            improved = current < self.best\n",
        "        else:\n",
        "            improved = current > self.best\n",
        "\n",
        "        if improved:\n",
        "            self.best = current\n",
        "            torch.save(model.state_dict(), self.filepath)\n",
        "            if self.verbose:\n",
        "                print(f\"\\nModel saved to {self.filepath}\")\n",
        "\n",
        "# ==========================================\n",
        "# Data Preprocessing\n",
        "# ==========================================\n",
        "\n",
        "def get_dataloaders(config):\n",
        "    \"\"\"Downloads, prepares MedMNIST data with augmentation, returning DataLoaders.\"\"\"\n",
        "\n",
        "    # MedMNIST dataset configuration\n",
        "    dataset_name = \"tissuemnist\"  # Options: pathmnist, chestmnist, dermamnist, octmnist, pneumoniamnist, retinamnist, breastmnist, bloodmnist, tissuemnist, organamnist, organcmnist, organsmnist\n",
        "\n",
        "    # Get dataset info\n",
        "    info = INFO[dataset_name]\n",
        "    task = info['task']\n",
        "    n_channels = info['n_channels']\n",
        "    n_classes = len(info['label'])\n",
        "\n",
        "    print(f\"Dataset: {dataset_name}\")\n",
        "    print(f\"Task: {task}\")\n",
        "    print(f\"Number of channels: {n_channels}\")\n",
        "    print(f\"Number of classes: {n_classes}\")\n",
        "\n",
        "    # Update config with dataset info\n",
        "    config[\"num_classes\"] = n_classes\n",
        "\n",
        "    # Get the dataset class\n",
        "    DataClass = getattr(medmnist, info['python_class'])\n",
        "\n",
        "    # Define transforms\n",
        "    # Training transforms (with augmentation)\n",
        "    train_transform = transforms.Compose([\n",
        "        transforms.Resize((64, 64)),  # Resize to match original Tiny ImageNet size\n",
        "        transforms.RandomRotation(degrees=10),\n",
        "        transforms.RandomHorizontalFlip(p=0.5),\n",
        "        transforms.ColorJitter(brightness=0.1, contrast=0.1),\n",
        "        transforms.ToTensor(),\n",
        "        # Convert grayscale to RGB if needed\n",
        "        transforms.Lambda(lambda x: x.repeat(3, 1, 1) if x.size(0) == 1 else x),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "    # Validation/Test transforms (no augmentation)\n",
        "    val_test_transform = transforms.Compose([\n",
        "        transforms.Resize((64, 64)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Lambda(lambda x: x.repeat(3, 1, 1) if x.size(0) == 1 else x),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "    print(f\"Loading {dataset_name} dataset...\")\n",
        "\n",
        "    # Load datasets - MedMNIST provides predefined train/val/test splits\n",
        "    train_dataset = DataClass(\n",
        "        split='train',\n",
        "        transform=train_transform,\n",
        "        download=True,  # Will download if not present\n",
        "        as_rgb=False    # We handle RGB conversion in transforms\n",
        "    )\n",
        "\n",
        "    val_dataset = DataClass(\n",
        "        split='val',\n",
        "        transform=val_test_transform,\n",
        "        download=True,\n",
        "        as_rgb=False\n",
        "    )\n",
        "\n",
        "    test_dataset = DataClass(\n",
        "        split='test',\n",
        "        transform=val_test_transform,\n",
        "        download=True,\n",
        "        as_rgb=False\n",
        "    )\n",
        "\n",
        "    print(f\"Dataset splits - Train: {len(train_dataset)}, Val: {len(val_dataset)}, Test: {len(test_dataset)}\")\n",
        "\n",
        "    # Handle different task types\n",
        "    if task == 'multi-label, binary-class':\n",
        "        def collate_fn(batch):\n",
        "            images = torch.stack([item[0] for item in batch])\n",
        "            # Fix: Convert to numpy array first, then to tensor\n",
        "            labels_np = np.array([item[1] for item in batch])\n",
        "            labels = torch.tensor(labels_np, dtype=torch.float32).squeeze()\n",
        "            return images, labels\n",
        "    else:\n",
        "        def collate_fn(batch):\n",
        "            images = torch.stack([item[0] for item in batch])\n",
        "            # Fix: Convert to numpy array first, then to tensor\n",
        "            labels_np = np.array([item[1] for item in batch])\n",
        "            labels = torch.tensor(labels_np, dtype=torch.long).squeeze()\n",
        "            return images, labels\n",
        "\n",
        "    # Set worker init function for reproducible DataLoader behavior\n",
        "    def worker_init_fn(worker_id):\n",
        "        np.random.seed(config[\"split_seed\"] + worker_id)\n",
        "\n",
        "    # Create DataLoaders\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=config[\"batch_size\"],\n",
        "        shuffle=True,\n",
        "        num_workers=2,\n",
        "        pin_memory=True,\n",
        "        worker_init_fn=worker_init_fn,\n",
        "        collate_fn=collate_fn,\n",
        "        generator=torch.Generator().manual_seed(config[\"split_seed\"])\n",
        "    )\n",
        "\n",
        "    val_loader = DataLoader(\n",
        "        val_dataset,\n",
        "        batch_size=config[\"batch_size\"],\n",
        "        shuffle=False,\n",
        "        num_workers=2,\n",
        "        pin_memory=True,\n",
        "        worker_init_fn=worker_init_fn,\n",
        "        collate_fn=collate_fn\n",
        "    )\n",
        "\n",
        "    test_loader = DataLoader(\n",
        "        test_dataset,\n",
        "        batch_size=config[\"batch_size\"],\n",
        "        shuffle=False,\n",
        "        num_workers=2,\n",
        "        pin_memory=True,\n",
        "        worker_init_fn=worker_init_fn,\n",
        "        collate_fn=collate_fn\n",
        "    )\n",
        "\n",
        "    return train_loader, val_loader, test_loader\n",
        "\n",
        "# ==========================================\n",
        "# Model Architecture and Training\n",
        "# ==========================================\n",
        "\n",
        "class NonResidualBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    This block is a standard convolutional block WITHOUT the residual connection.\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels, out_channels, stride=1):\n",
        "        super(NonResidualBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        out = self.relu(out)\n",
        "        return out\n",
        "\n",
        "class ResNetTransformer(nn.Module):\n",
        "    \"\"\"\n",
        "    A hybrid architecture combining a non-residual CNN backbone with a Transformer encoder.\n",
        "    \"\"\"\n",
        "    def __init__(self, block, layers, num_classes, t_config):\n",
        "        super(ResNetTransformer, self).__init__()\n",
        "        self.in_channels = 64\n",
        "        self.embedding_dim = t_config[\"embedding_dim\"]\n",
        "\n",
        "        # 1. CNN Backbone (Feature Extractor)\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "        self.layer1 = self._make_layer(block, 64, layers[0], stride=1)\n",
        "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n",
        "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n",
        "\n",
        "        # 2. Projection heads to create tokens from feature maps\n",
        "        self.projections = nn.ModuleList([\n",
        "            self._create_projection(64, self.embedding_dim),   # From initial maxpool\n",
        "            self._create_projection(64, self.embedding_dim),   # From layer1\n",
        "            self._create_projection(128, self.embedding_dim),  # From layer2\n",
        "            self._create_projection(256, self.embedding_dim),  # From layer3\n",
        "            self._create_projection(512, self.embedding_dim)   # From layer4\n",
        "        ])\n",
        "\n",
        "        # 3. Transformer Encoder\n",
        "        encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=self.embedding_dim,\n",
        "            nhead=t_config[\"nhead\"],\n",
        "            dim_feedforward=t_config[\"dim_feedforward\"],\n",
        "            dropout=t_config[\"dropout\"],\n",
        "            batch_first=True\n",
        "        )\n",
        "        self.transformer_encoder = nn.TransformerEncoder(\n",
        "            encoder_layer,\n",
        "            num_layers=t_config[\"num_encoder_layers\"]\n",
        "        )\n",
        "\n",
        "        # 4. Final Classifier\n",
        "        self.classifier = nn.Linear(self.embedding_dim, num_classes)\n",
        "\n",
        "    def _create_projection(self, in_features, out_features):\n",
        "        return nn.Sequential(\n",
        "            nn.AdaptiveAvgPool2d((1, 1)),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(in_features, out_features)\n",
        "        )\n",
        "\n",
        "    def _make_layer(self, block, out_channels, num_blocks, stride):\n",
        "        strides = [stride] + [1] * (num_blocks - 1)\n",
        "        layers = []\n",
        "        current_in_channels = self.in_channels\n",
        "        for s in strides:\n",
        "            layers.append(block(current_in_channels, out_channels, s))\n",
        "            current_in_channels = out_channels\n",
        "        self.in_channels = out_channels\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # 1. Pass through CNN backbone and capture features\n",
        "        features = []\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "        features.append(x)\n",
        "\n",
        "        x = self.layer1(x); features.append(x)\n",
        "        x = self.layer2(x); features.append(x)\n",
        "        x = self.layer3(x); features.append(x)\n",
        "        x = self.layer4(x); features.append(x)\n",
        "\n",
        "        # 2. Project features to tokens\n",
        "        tokens = []\n",
        "        for i, feature_map in enumerate(features):\n",
        "            tokens.append(self.projections[i](feature_map))\n",
        "\n",
        "        # 3. Stack tokens and pass through Transformer\n",
        "        token_sequence = torch.stack(tokens, dim=1)\n",
        "        transformer_out = self.transformer_encoder(token_sequence)\n",
        "\n",
        "        # 4. Aggregate and classify\n",
        "        aggregated_vector = transformer_out.mean(dim=1)\n",
        "        logits = self.classifier(aggregated_vector)\n",
        "\n",
        "        return logits\n",
        "\n",
        "def ResNetTransformer18(num_classes=200, t_config=TRANSFORMER_CONFIG):\n",
        "    return ResNetTransformer(NonResidualBlock, [2, 2, 2, 2], num_classes, t_config)\n",
        "\n",
        "def evaluate_model(model, data_loader, criterion, device):\n",
        "    \"\"\"Evaluates the model on a given dataset.\"\"\"\n",
        "    model.eval()\n",
        "    total_loss, correct, total = 0.0, 0, 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in data_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            total_loss += loss.item() * images.size(0)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    avg_loss = total_loss / total\n",
        "    accuracy = 100 * correct / total\n",
        "    return avg_loss, accuracy\n",
        "\n",
        "def train_and_validate(model, train_loader, val_loader, config):\n",
        "    \"\"\"Main training loop with callbacks.\"\"\"\n",
        "    device = config[\"device\"]\n",
        "    model.to(device)\n",
        "\n",
        "    # Set seeds for reproducible training\n",
        "    torch.manual_seed(config[\"split_seed\"])\n",
        "    np.random.seed(config[\"split_seed\"])\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(config[\"split_seed\"])\n",
        "        torch.cuda.manual_seed_all(config[\"split_seed\"])\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=config[\"learning_rate\"], weight_decay=config[\"weight_decay\"])\n",
        "\n",
        "    # Initialize callbacks\n",
        "    callbacks = [\n",
        "        ReduceLROnPlateau(\n",
        "            optimizer=optimizer,\n",
        "            monitor='val_accuracy',\n",
        "            factor=0.2,\n",
        "            patience=3,\n",
        "            min_lr=1e-7,\n",
        "            verbose=1\n",
        "        ),\n",
        "        EarlyStopping(\n",
        "            monitor='val_accuracy',\n",
        "            patience=7,\n",
        "            restore_best_weights=True,\n",
        "            verbose=1\n",
        "        ),\n",
        "        ModelCheckpoint(\n",
        "            filepath=config[\"checkpoint_path\"],\n",
        "            save_best_only=True,\n",
        "            monitor='val_accuracy',\n",
        "            verbose=1\n",
        "        )\n",
        "    ]\n",
        "\n",
        "    print(\"\\n--- Starting Training ---\")\n",
        "    early_stop = False\n",
        "\n",
        "    for epoch in range(config[\"num_epochs\"]):\n",
        "        if early_stop:\n",
        "            break\n",
        "\n",
        "        model.train()\n",
        "        start_time = time.time()\n",
        "        running_loss, train_correct, train_total = 0.0, 0, 0\n",
        "\n",
        "        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{config['num_epochs']} [Train]\", leave=False)\n",
        "        for images, labels in progress_bar:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item() * images.size(0)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            train_total += labels.size(0)\n",
        "            train_correct += (predicted == labels).sum().item()\n",
        "            progress_bar.set_postfix(loss=loss.item())\n",
        "\n",
        "        train_loss = running_loss / train_total\n",
        "        train_acc = 100 * train_correct / train_total\n",
        "        val_loss, val_acc = evaluate_model(model, val_loader, criterion, device)\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{config['num_epochs']} | Time: {time.time() - start_time:.2f}s | \"\n",
        "              f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}% | \"\n",
        "              f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%\")\n",
        "\n",
        "        # Prepare logs for callbacks\n",
        "        logs = {\n",
        "            'train_loss': train_loss,\n",
        "            'train_accuracy': train_acc,\n",
        "            'val_loss': val_loss,\n",
        "            'val_accuracy': val_acc\n",
        "        }\n",
        "\n",
        "        # Execute callbacks\n",
        "        for callback in callbacks:\n",
        "            if isinstance(callback, EarlyStopping):\n",
        "                if callback.on_epoch_end(epoch, logs, model):\n",
        "                    early_stop = True\n",
        "                    break\n",
        "            elif isinstance(callback, ModelCheckpoint):\n",
        "                callback.on_epoch_end(epoch, logs, model)\n",
        "            else:\n",
        "                callback.on_epoch_end(epoch, logs)\n",
        "\n",
        "    # Execute callback cleanup\n",
        "    for callback in callbacks:\n",
        "        if isinstance(callback, EarlyStopping):\n",
        "            callback.on_training_end(model)\n",
        "        else:\n",
        "            callback.on_training_end()\n",
        "\n",
        "    print(\"--- Training Finished ---\\n\")\n",
        "    return model\n",
        "\n",
        "# ==========================================\n",
        "# Evaluation\n",
        "# ==========================================\n",
        "\n",
        "def calculate_top_k_accuracy(outputs, labels, k_values=[1, 3, 5]):\n",
        "    \"\"\"Calculate top-k accuracy for given k values.\"\"\"\n",
        "    batch_size = labels.size(0)\n",
        "    _, pred = outputs.topk(max(k_values), 1, True, True)\n",
        "    pred = pred.t()\n",
        "    correct = pred.eq(labels.view(1, -1).expand_as(pred))\n",
        "\n",
        "    top_k_accuracies = {}\n",
        "    for k in k_values:\n",
        "        correct_k = correct[:k].reshape(-1).float().sum(0, keepdim=True)\n",
        "        top_k_accuracies[k] = correct_k.item() / batch_size\n",
        "\n",
        "    return top_k_accuracies\n",
        "\n",
        "def calculate_entropy(probs):\n",
        "    \"\"\"Calculate entropy of probability distributions.\"\"\"\n",
        "    # Add small epsilon to avoid log(0)\n",
        "    epsilon = 1e-8\n",
        "    probs = probs + epsilon\n",
        "    entropy = -torch.sum(probs * torch.log(probs), dim=1)\n",
        "    return entropy\n",
        "\n",
        "def calculate_ece(confidences, accuracies, n_bins=10):\n",
        "    \"\"\"Calculate Expected Calibration Error.\"\"\"\n",
        "    bin_boundaries = torch.linspace(0, 1, n_bins + 1)\n",
        "    bin_lowers = bin_boundaries[:-1]\n",
        "    bin_uppers = bin_boundaries[1:]\n",
        "\n",
        "    ece = 0\n",
        "    for bin_lower, bin_upper in zip(bin_lowers, bin_uppers):\n",
        "        in_bin = (confidences > bin_lower.item()) & (confidences <= bin_upper.item())\n",
        "        prop_in_bin = in_bin.float().mean()\n",
        "\n",
        "        if prop_in_bin.item() > 0:\n",
        "            accuracy_in_bin = accuracies[in_bin].float().mean()\n",
        "            avg_confidence_in_bin = confidences[in_bin].mean()\n",
        "            ece += torch.abs(avg_confidence_in_bin - accuracy_in_bin) * prop_in_bin\n",
        "\n",
        "    return ece.item()\n",
        "\n",
        "def comprehensive_evaluation(model, test_loader, criterion, device):\n",
        "    \"\"\"Comprehensive evaluation with all requested metrics.\"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    all_predictions = []\n",
        "    all_labels = []\n",
        "    all_confidences = []\n",
        "    all_entropies = []\n",
        "    all_top_k_results = {1: [], 3: [], 5: []}\n",
        "    total_loss = 0.0\n",
        "    total_samples = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in tqdm(test_loader, desc=\"Evaluating\"):\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            # Convert outputs to probabilities\n",
        "            probs = torch.softmax(outputs, dim=1)\n",
        "\n",
        "            # Get predictions and confidences\n",
        "            confidences, predictions = torch.max(probs, dim=1)\n",
        "\n",
        "            # Calculate entropy\n",
        "            entropies = calculate_entropy(probs)\n",
        "\n",
        "            # Calculate top-k accuracies\n",
        "            top_k_accs = calculate_top_k_accuracy(outputs, labels, [1, 3, 5])\n",
        "\n",
        "            # Store results\n",
        "            all_predictions.extend(predictions.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "            all_confidences.extend(confidences.cpu().numpy())\n",
        "            all_entropies.extend(entropies.cpu().numpy())\n",
        "\n",
        "            for k in [1, 3, 5]:\n",
        "                all_top_k_results[k].extend([top_k_accs[k]] * labels.size(0))\n",
        "\n",
        "            total_loss += loss.item() * images.size(0)\n",
        "            total_samples += images.size(0)\n",
        "\n",
        "    # Convert to numpy arrays\n",
        "    all_predictions = np.array(all_predictions)\n",
        "    all_labels = np.array(all_labels)\n",
        "    all_confidences = np.array(all_confidences)\n",
        "    all_entropies = np.array(all_entropies)\n",
        "\n",
        "    # Calculate basic metrics\n",
        "    avg_loss = total_loss / total_samples\n",
        "\n",
        "    # Calculate top-k accuracies\n",
        "    top1 = np.mean([pred == label for pred, label in zip(all_predictions, all_labels)])\n",
        "    top3 = np.mean(all_top_k_results[3])\n",
        "    top5 = np.mean(all_top_k_results[5])\n",
        "\n",
        "    # Calculate precision, recall, f1-score\n",
        "    precision_macro, recall_macro, f1_macro, _ = precision_recall_fscore_support(\n",
        "        all_labels, all_predictions, average='macro', zero_division=0\n",
        "    )\n",
        "    precision_weighted, recall_weighted, f1_weighted, _ = precision_recall_fscore_support(\n",
        "        all_labels, all_predictions, average='weighted', zero_division=0\n",
        "    )\n",
        "\n",
        "    # Calculate confidence and calibration metrics\n",
        "    avg_confidence = np.mean(all_confidences)\n",
        "    std_confidence = np.std(all_confidences)\n",
        "    avg_entropy = np.mean(all_entropies)\n",
        "\n",
        "    # Calculate ECE\n",
        "    accuracies = (all_predictions == all_labels).astype(float)\n",
        "    ece = calculate_ece(torch.tensor(all_confidences), torch.tensor(accuracies))\n",
        "\n",
        "    return {\n",
        "        'test_loss': avg_loss,\n",
        "        'top1': top1,\n",
        "        'top3': top3,\n",
        "        'top5': top5,\n",
        "        'precision_macro': precision_macro,\n",
        "        'recall_macro': recall_macro,\n",
        "        'f1_macro': f1_macro,\n",
        "        'precision_weighted': precision_weighted,\n",
        "        'recall_weighted': recall_weighted,\n",
        "        'f1_weighted': f1_weighted,\n",
        "        'avg_confidence': avg_confidence,\n",
        "        'std_confidence': std_confidence,\n",
        "        'avg_entropy': avg_entropy,\n",
        "        'ece': ece\n",
        "    }\n",
        "\n",
        "# ==========================================\n",
        "# Main Execution\n",
        "# ==========================================\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # Set global seeds for full reproducibility\n",
        "    torch.manual_seed(CONFIG[\"split_seed\"])\n",
        "    np.random.seed(CONFIG[\"split_seed\"])\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(CONFIG[\"split_seed\"])\n",
        "        torch.cuda.manual_seed_all(CONFIG[\"split_seed\"])\n",
        "        # Ensure deterministic behavior on CUDA\n",
        "        torch.backends.cudnn.deterministic = True\n",
        "        torch.backends.cudnn.benchmark = False\n",
        "\n",
        "    print(f\"Using device: {CONFIG['device']}\")\n",
        "    print(f\"Reproducibility seed: {CONFIG['split_seed']}\")\n",
        "\n",
        "    # Get data loaders\n",
        "    dataloaders = get_dataloaders(CONFIG)\n",
        "    if dataloaders is None:\n",
        "        print(\"Could not prepare data. Halting execution.\", file=sys.stderr)\n",
        "        sys.exit(1)  # Exit if data preparation failed\n",
        "\n",
        "    train_loader, val_loader, test_loader = dataloaders\n",
        "\n",
        "    # Initialize and train model\n",
        "    model = ResNetTransformer18(num_classes=CONFIG[\"num_classes\"], t_config=TRANSFORMER_CONFIG)\n",
        "    trained_model = train_and_validate(model, train_loader, val_loader, CONFIG)\n",
        "\n",
        "    # Comprehensive evaluation on the test set\n",
        "    print(\"--- Starting Comprehensive Evaluation on Test Set ---\")\n",
        "    results = comprehensive_evaluation(trained_model, test_loader, nn.CrossEntropyLoss(), CONFIG[\"device\"])\n",
        "\n",
        "    # Print all requested metrics\n",
        "    print(\"\\n--- Top-K Accuracy Results ---\")\n",
        "    print(f\"Top-1 Accuracy: {results['top1'] * 100:.2f}%\")\n",
        "    print(f\"Top-3 Accuracy: {results['top3'] * 100:.2f}%\")\n",
        "    print(f\"Top-5 Accuracy: {results['top5'] * 100:.2f}%\")\n",
        "\n",
        "    print(\"\\n--- Additional Performance Metrics ---\")\n",
        "    print(f\"Macro Average Precision: {results['precision_macro']:.4f}\")\n",
        "    print(f\"Macro Average Recall: {results['recall_macro']:.4f}\")\n",
        "    print(f\"Macro Average F1-Score: {results['f1_macro']:.4f}\")\n",
        "    print(f\"Weighted Average Precision: {results['precision_weighted']:.4f}\")\n",
        "    print(f\"Weighted Average Recall: {results['recall_weighted']:.4f}\")\n",
        "    print(f\"Weighted Average F1-Score: {results['f1_weighted']:.4f}\")\n",
        "\n",
        "    print(\"\\n--- Model Confidence & Calibration Metrics ---\")\n",
        "    print(f\"Average Prediction Confidence: {results['avg_confidence']:.4f}\")\n",
        "    print(f\"Confidence Standard Deviation: {results['std_confidence']:.4f}\")\n",
        "    print(f\"Average Prediction Entropy: {results['avg_entropy']:.4f}\")\n",
        "    print(f\"Expected Calibration Error: {results['ece']:.4f}\")\n",
        "\n",
        "    print(f\"\\nFinal Test Loss: {results['test_loss']:.4f}\")\n",
        "    print(\"===============================================\")"
      ],
      "metadata": {
        "id": "EEQ1dy0zOJN3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "10984864-13fa-4076-a5dc-42b0eae7d297"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Reproducibility seed: 42\n",
            "Dataset: tissuemnist\n",
            "Task: multi-class\n",
            "Number of channels: 1\n",
            "Number of classes: 8\n",
            "Loading tissuemnist dataset...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 125M/125M [00:21<00:00, 5.90MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset splits - Train: 165466, Val: 23640, Test: 47280\n",
            "\n",
            "--- Starting Training ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100 | Time: 148.24s | Train Loss: 1.2420, Train Acc: 53.36% | Val Loss: 1.2369, Val Acc: 51.59%\n",
            "\n",
            "Model saved to ./best_model.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/100 | Time: 148.87s | Train Loss: 1.0898, Train Acc: 59.64% | Val Loss: 1.5447, Val Acc: 50.41%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/100 | Time: 147.62s | Train Loss: 1.0280, Train Acc: 62.05% | Val Loss: 1.0529, Val Acc: 61.32%\n",
            "\n",
            "Model saved to ./best_model.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4/100 | Time: 147.53s | Train Loss: 0.9977, Train Acc: 63.14% | Val Loss: 1.0590, Val Acc: 61.67%\n",
            "\n",
            "Model saved to ./best_model.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5/100 | Time: 145.88s | Train Loss: 0.9764, Train Acc: 64.02% | Val Loss: 1.0937, Val Acc: 58.95%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6/100 | Time: 146.88s | Train Loss: 0.9582, Train Acc: 64.70% | Val Loss: 1.0497, Val Acc: 62.71%\n",
            "\n",
            "Model saved to ./best_model.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7/100 | Time: 146.89s | Train Loss: 0.9471, Train Acc: 65.20% | Val Loss: 0.9833, Val Acc: 64.02%\n",
            "\n",
            "Model saved to ./best_model.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8/100 | Time: 145.03s | Train Loss: 0.9351, Train Acc: 65.60% | Val Loss: 1.0173, Val Acc: 62.65%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9/100 | Time: 146.89s | Train Loss: 0.9274, Train Acc: 65.86% | Val Loss: 0.9414, Val Acc: 65.49%\n",
            "\n",
            "Model saved to ./best_model.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10/100 | Time: 145.03s | Train Loss: 0.9186, Train Acc: 66.17% | Val Loss: 0.9523, Val Acc: 64.97%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 11/100 | Time: 144.24s | Train Loss: 0.9126, Train Acc: 66.40% | Val Loss: 0.9188, Val Acc: 66.29%\n",
            "\n",
            "Model saved to ./best_model.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 12/100 | Time: 145.40s | Train Loss: 0.9063, Train Acc: 66.83% | Val Loss: 0.9584, Val Acc: 64.74%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 13/100 | Time: 145.57s | Train Loss: 0.9022, Train Acc: 66.97% | Val Loss: 0.9214, Val Acc: 66.30%\n",
            "\n",
            "Model saved to ./best_model.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 14/100 | Time: 145.29s | Train Loss: 0.8953, Train Acc: 67.27% | Val Loss: 0.9844, Val Acc: 63.13%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 15/100 | Time: 145.87s | Train Loss: 0.8940, Train Acc: 67.36% | Val Loss: 1.2242, Val Acc: 56.24%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 16/100 | Time: 145.14s | Train Loss: 0.8888, Train Acc: 67.41% | Val Loss: 0.9282, Val Acc: 65.56%\n",
            "\n",
            "Reducing learning rate from 1.00e-03 to 2.00e-04\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 17/100 | Time: 143.71s | Train Loss: 0.8302, Train Acc: 69.68% | Val Loss: 0.8617, Val Acc: 68.57%\n",
            "\n",
            "Model saved to ./best_model.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 18/100 | Time: 143.89s | Train Loss: 0.8152, Train Acc: 70.32% | Val Loss: 0.8346, Val Acc: 69.99%\n",
            "\n",
            "Model saved to ./best_model.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 19/100 | Time: 143.83s | Train Loss: 0.8093, Train Acc: 70.53% | Val Loss: 0.8631, Val Acc: 68.82%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 20/100 | Time: 144.32s | Train Loss: 0.8021, Train Acc: 70.89% | Val Loss: 0.8378, Val Acc: 69.88%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 21/100 | Time: 143.66s | Train Loss: 0.7977, Train Acc: 71.05% | Val Loss: 0.8317, Val Acc: 69.95%\n",
            "\n",
            "Reducing learning rate from 2.00e-04 to 4.00e-05\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 22/100 | Time: 143.26s | Train Loss: 0.7789, Train Acc: 71.69% | Val Loss: 0.8223, Val Acc: 70.30%\n",
            "\n",
            "Model saved to ./best_model.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 23/100 | Time: 146.20s | Train Loss: 0.7750, Train Acc: 71.91% | Val Loss: 0.8266, Val Acc: 70.24%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 24/100 | Time: 146.54s | Train Loss: 0.7710, Train Acc: 72.00% | Val Loss: 0.8281, Val Acc: 70.16%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 25/100 | Time: 144.28s | Train Loss: 0.7696, Train Acc: 72.10% | Val Loss: 0.8241, Val Acc: 70.33%\n",
            "\n",
            "Model saved to ./best_model.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 26/100 | Time: 144.06s | Train Loss: 0.7670, Train Acc: 72.22% | Val Loss: 0.8254, Val Acc: 70.34%\n",
            "\n",
            "Model saved to ./best_model.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 27/100 | Time: 144.03s | Train Loss: 0.7664, Train Acc: 72.24% | Val Loss: 0.8236, Val Acc: 70.47%\n",
            "\n",
            "Model saved to ./best_model.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 28/100 | Time: 145.40s | Train Loss: 0.7629, Train Acc: 72.42% | Val Loss: 0.8247, Val Acc: 70.40%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 29/100 | Time: 145.59s | Train Loss: 0.7641, Train Acc: 72.25% | Val Loss: 0.8230, Val Acc: 70.44%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 30/100 | Time: 145.15s | Train Loss: 0.7615, Train Acc: 72.38% | Val Loss: 0.8304, Val Acc: 69.98%\n",
            "\n",
            "Reducing learning rate from 4.00e-05 to 8.00e-06\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 31/100 | Time: 145.09s | Train Loss: 0.7566, Train Acc: 72.53% | Val Loss: 0.8281, Val Acc: 70.16%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 32/100 | Time: 145.01s | Train Loss: 0.7551, Train Acc: 72.73% | Val Loss: 0.8251, Val Acc: 70.44%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 33/100 | Time: 144.95s | Train Loss: 0.7561, Train Acc: 72.61% | Val Loss: 0.8251, Val Acc: 70.24%\n",
            "\n",
            "Reducing learning rate from 8.00e-06 to 1.60e-06\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 34/100 | Time: 144.37s | Train Loss: 0.7531, Train Acc: 72.72% | Val Loss: 0.8238, Val Acc: 70.45%\n",
            "\n",
            "Early stopping at epoch 34\n",
            "Restored model weights from the end of the best epoch: 27\n",
            "--- Training Finished ---\n",
            "\n",
            "--- Starting Comprehensive Evaluation on Test Set ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating: 100%|██████████| 370/370 [00:17<00:00, 20.83it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Top-K Accuracy Results ---\n",
            "Top-1 Accuracy: 70.69%\n",
            "Top-3 Accuracy: 94.06%\n",
            "Top-5 Accuracy: 98.63%\n",
            "\n",
            "--- Additional Performance Metrics ---\n",
            "Macro Average Precision: 0.6439\n",
            "Macro Average Recall: 0.6216\n",
            "Macro Average F1-Score: 0.6279\n",
            "Weighted Average Precision: 0.7040\n",
            "Weighted Average Recall: 0.7069\n",
            "Weighted Average F1-Score: 0.7026\n",
            "\n",
            "--- Model Confidence & Calibration Metrics ---\n",
            "Average Prediction Confidence: 0.7379\n",
            "Confidence Standard Deviation: 0.2072\n",
            "Average Prediction Entropy: 0.7268\n",
            "Expected Calibration Error: 0.0310\n",
            "\n",
            "Final Test Loss: 0.8136\n",
            "===============================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hybrid Transformer on ResNet-18 (Learnable PE)"
      ],
      "metadata": {
        "id": "50zF1u-wOK6G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.datasets import ImageFolder\n",
        "import os\n",
        "import requests\n",
        "import zipfile\n",
        "import shutil\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "import sys\n",
        "import numpy as np\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "import torch.nn.functional as F\n",
        "import medmnist\n",
        "from medmnist import INFO\n",
        "\n",
        "# --- Configuration Parameters ---\n",
        "CONFIG = {\n",
        "    \"batch_size\": 128,\n",
        "    \"num_epochs\": 100,\n",
        "    \"learning_rate\": 0.001,\n",
        "    \"weight_decay\": 1e-4,\n",
        "    \"split_seed\": 42,\n",
        "    \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
        "    \"checkpoint_path\": \"./best_model.pth\",\n",
        "}\n",
        "\n",
        "# --- Transformer Configuration ---\n",
        "TRANSFORMER_CONFIG = {\n",
        "    \"embedding_dim\": 256,       # Dimension of the tokens fed to the transformer\n",
        "    \"nhead\": 8,                 # Number of attention heads\n",
        "    \"num_encoder_layers\": 3,    # Number of transformer encoder layers\n",
        "    \"dim_feedforward\": 512,     # Hidden dimension in the feed-forward network\n",
        "    \"dropout\": 0.1,\n",
        "}\n",
        "\n",
        "# ==========================================\n",
        "# Callback Classes\n",
        "# ==========================================\n",
        "\n",
        "class Callback:\n",
        "    \"\"\"Base callback class\"\"\"\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        pass\n",
        "\n",
        "    def on_training_end(self):\n",
        "        pass\n",
        "\n",
        "class ReduceLROnPlateau(Callback):\n",
        "    \"\"\"Reduce learning rate when a metric has stopped improving\"\"\"\n",
        "    def __init__(self, optimizer, monitor='val_accuracy', factor=0.2, patience=3, min_lr=1e-7, verbose=1):\n",
        "        self.optimizer = optimizer\n",
        "        self.monitor = monitor\n",
        "        self.factor = factor\n",
        "        self.patience = patience\n",
        "        self.min_lr = min_lr\n",
        "        self.verbose = verbose\n",
        "        self.wait = 0\n",
        "        self.best = None\n",
        "        self.mode = 'min' if 'loss' in monitor else 'max'\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        if logs is None:\n",
        "            return\n",
        "\n",
        "        current = logs.get(self.monitor)\n",
        "        if current is None:\n",
        "            return\n",
        "\n",
        "        if self.best is None:\n",
        "            self.best = current\n",
        "            return\n",
        "\n",
        "        if self.mode == 'min':\n",
        "            improved = current < self.best\n",
        "        else:\n",
        "            improved = current > self.best\n",
        "\n",
        "        if improved:\n",
        "            self.best = current\n",
        "            self.wait = 0\n",
        "        else:\n",
        "            self.wait += 1\n",
        "            if self.wait >= self.patience:\n",
        "                old_lr = self.optimizer.param_groups[0]['lr']\n",
        "                new_lr = max(old_lr * self.factor, self.min_lr)\n",
        "                if new_lr != old_lr:\n",
        "                    self.optimizer.param_groups[0]['lr'] = new_lr\n",
        "                    if self.verbose:\n",
        "                        print(f\"\\nReducing learning rate from {old_lr:.2e} to {new_lr:.2e}\")\n",
        "                    self.wait = 0\n",
        "\n",
        "class EarlyStopping(Callback):\n",
        "    \"\"\"Stop training when a monitored metric has stopped improving\"\"\"\n",
        "    def __init__(self, monitor='val_accuracy', patience=7, restore_best_weights=True, verbose=1):\n",
        "        self.monitor = monitor\n",
        "        self.patience = patience\n",
        "        self.restore_best_weights = restore_best_weights\n",
        "        self.verbose = verbose\n",
        "        self.wait = 0\n",
        "        self.best = None\n",
        "        self.best_weights = None\n",
        "        self.mode = 'min' if 'loss' in monitor else 'max'\n",
        "        self.stopped_epoch = 0\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None, model=None):\n",
        "        if logs is None:\n",
        "            return False\n",
        "\n",
        "        current = logs.get(self.monitor)\n",
        "        if current is None:\n",
        "            return False\n",
        "\n",
        "        if self.best is None:\n",
        "            self.best = current\n",
        "            if model is not None and self.restore_best_weights:\n",
        "                self.best_weights = model.state_dict().copy()\n",
        "            return False\n",
        "\n",
        "        if self.mode == 'min':\n",
        "            improved = current < self.best\n",
        "        else:\n",
        "            improved = current > self.best\n",
        "\n",
        "        if improved:\n",
        "            self.best = current\n",
        "            self.wait = 0\n",
        "            if model is not None and self.restore_best_weights:\n",
        "                self.best_weights = model.state_dict().copy()\n",
        "        else:\n",
        "            self.wait += 1\n",
        "            if self.wait >= self.patience:\n",
        "                self.stopped_epoch = epoch\n",
        "                if self.verbose:\n",
        "                    print(f\"\\nEarly stopping at epoch {epoch + 1}\")\n",
        "                return True\n",
        "        return False\n",
        "\n",
        "    def on_training_end(self, model=None):\n",
        "        if self.stopped_epoch > 0 and self.verbose:\n",
        "            print(f\"Restored model weights from the end of the best epoch: {self.stopped_epoch + 1 - self.patience}\")\n",
        "        if model is not None and self.restore_best_weights and self.best_weights is not None:\n",
        "            model.load_state_dict(self.best_weights)\n",
        "\n",
        "class ModelCheckpoint(Callback):\n",
        "    \"\"\"Save the model after every epoch\"\"\"\n",
        "    def __init__(self, filepath, save_best_only=True, monitor='val_accuracy', verbose=1):\n",
        "        self.filepath = filepath\n",
        "        self.save_best_only = save_best_only\n",
        "        self.monitor = monitor\n",
        "        self.verbose = verbose\n",
        "        self.best = None\n",
        "        self.mode = 'min' if 'loss' in monitor else 'max'\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None, model=None):\n",
        "        if logs is None or model is None:\n",
        "            return\n",
        "\n",
        "        current = logs.get(self.monitor)\n",
        "        if current is None:\n",
        "            return\n",
        "\n",
        "        if not self.save_best_only:\n",
        "            filepath = self.filepath.replace('.pth', f'_epoch_{epoch + 1}.pth')\n",
        "            torch.save(model.state_dict(), filepath)\n",
        "            if self.verbose:\n",
        "                print(f\"\\nModel saved to {filepath}\")\n",
        "            return\n",
        "\n",
        "        if self.best is None:\n",
        "            self.best = current\n",
        "            torch.save(model.state_dict(), self.filepath)\n",
        "            if self.verbose:\n",
        "                print(f\"\\nModel saved to {self.filepath}\")\n",
        "            return\n",
        "\n",
        "        if self.mode == 'min':\n",
        "            improved = current < self.best\n",
        "        else:\n",
        "            improved = current > self.best\n",
        "\n",
        "        if improved:\n",
        "            self.best = current\n",
        "            torch.save(model.state_dict(), self.filepath)\n",
        "            if self.verbose:\n",
        "                print(f\"\\nModel saved to {self.filepath}\")\n",
        "\n",
        "# ==========================================\n",
        "# Data Preprocessing\n",
        "# ==========================================\n",
        "\n",
        "def get_dataloaders(config):\n",
        "    \"\"\"Downloads, prepares MedMNIST data with augmentation, returning DataLoaders.\"\"\"\n",
        "\n",
        "    # MedMNIST dataset configuration\n",
        "    dataset_name = \"tissuemnist\"  # Options: pathmnist, chestmnist, dermamnist, octmnist, pneumoniamnist, retinamnist, breastmnist, bloodmnist, tissuemnist, organamnist, organcmnist, organsmnist\n",
        "\n",
        "    # Get dataset info\n",
        "    info = INFO[dataset_name]\n",
        "    task = info['task']\n",
        "    n_channels = info['n_channels']\n",
        "    n_classes = len(info['label'])\n",
        "\n",
        "    print(f\"Dataset: {dataset_name}\")\n",
        "    print(f\"Task: {task}\")\n",
        "    print(f\"Number of channels: {n_channels}\")\n",
        "    print(f\"Number of classes: {n_classes}\")\n",
        "\n",
        "    # Update config with dataset info\n",
        "    config[\"num_classes\"] = n_classes\n",
        "\n",
        "    # Get the dataset class\n",
        "    DataClass = getattr(medmnist, info['python_class'])\n",
        "\n",
        "    # Define transforms\n",
        "    # Training transforms (with augmentation)\n",
        "    train_transform = transforms.Compose([\n",
        "        transforms.Resize((64, 64)),  # Resize to match original Tiny ImageNet size\n",
        "        transforms.RandomRotation(degrees=10),\n",
        "        transforms.RandomHorizontalFlip(p=0.5),\n",
        "        transforms.ColorJitter(brightness=0.1, contrast=0.1),\n",
        "        transforms.ToTensor(),\n",
        "        # Convert grayscale to RGB if needed\n",
        "        transforms.Lambda(lambda x: x.repeat(3, 1, 1) if x.size(0) == 1 else x),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "    # Validation/Test transforms (no augmentation)\n",
        "    val_test_transform = transforms.Compose([\n",
        "        transforms.Resize((64, 64)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Lambda(lambda x: x.repeat(3, 1, 1) if x.size(0) == 1 else x),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "    print(f\"Loading {dataset_name} dataset...\")\n",
        "\n",
        "    # Load datasets - MedMNIST provides predefined train/val/test splits\n",
        "    train_dataset = DataClass(\n",
        "        split='train',\n",
        "        transform=train_transform,\n",
        "        download=True,  # Will download if not present\n",
        "        as_rgb=False    # We handle RGB conversion in transforms\n",
        "    )\n",
        "\n",
        "    val_dataset = DataClass(\n",
        "        split='val',\n",
        "        transform=val_test_transform,\n",
        "        download=True,\n",
        "        as_rgb=False\n",
        "    )\n",
        "\n",
        "    test_dataset = DataClass(\n",
        "        split='test',\n",
        "        transform=val_test_transform,\n",
        "        download=True,\n",
        "        as_rgb=False\n",
        "    )\n",
        "\n",
        "    print(f\"Dataset splits - Train: {len(train_dataset)}, Val: {len(val_dataset)}, Test: {len(test_dataset)}\")\n",
        "\n",
        "    # Handle different task types\n",
        "    if task == 'multi-label, binary-class':\n",
        "        def collate_fn(batch):\n",
        "            images = torch.stack([item[0] for item in batch])\n",
        "            # Fix: Convert to numpy array first, then to tensor\n",
        "            labels_np = np.array([item[1] for item in batch])\n",
        "            labels = torch.tensor(labels_np, dtype=torch.float32).squeeze()\n",
        "            return images, labels\n",
        "    else:\n",
        "        def collate_fn(batch):\n",
        "            images = torch.stack([item[0] for item in batch])\n",
        "            # Fix: Convert to numpy array first, then to tensor\n",
        "            labels_np = np.array([item[1] for item in batch])\n",
        "            labels = torch.tensor(labels_np, dtype=torch.long).squeeze()\n",
        "            return images, labels\n",
        "\n",
        "    # Set worker init function for reproducible DataLoader behavior\n",
        "    def worker_init_fn(worker_id):\n",
        "        np.random.seed(config[\"split_seed\"] + worker_id)\n",
        "\n",
        "    # Create DataLoaders\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=config[\"batch_size\"],\n",
        "        shuffle=True,\n",
        "        num_workers=2,\n",
        "        pin_memory=True,\n",
        "        worker_init_fn=worker_init_fn,\n",
        "        collate_fn=collate_fn,\n",
        "        generator=torch.Generator().manual_seed(config[\"split_seed\"])\n",
        "    )\n",
        "\n",
        "    val_loader = DataLoader(\n",
        "        val_dataset,\n",
        "        batch_size=config[\"batch_size\"],\n",
        "        shuffle=False,\n",
        "        num_workers=2,\n",
        "        pin_memory=True,\n",
        "        worker_init_fn=worker_init_fn,\n",
        "        collate_fn=collate_fn\n",
        "    )\n",
        "\n",
        "    test_loader = DataLoader(\n",
        "        test_dataset,\n",
        "        batch_size=config[\"batch_size\"],\n",
        "        shuffle=False,\n",
        "        num_workers=2,\n",
        "        pin_memory=True,\n",
        "        worker_init_fn=worker_init_fn,\n",
        "        collate_fn=collate_fn\n",
        "    )\n",
        "\n",
        "    return train_loader, val_loader, test_loader\n",
        "\n",
        "# ==========================================\n",
        "# Model Architecture and Training\n",
        "# ==========================================\n",
        "\n",
        "class NonResidualBlock(nn.Module):\n",
        "    \"\"\"A standard convolutional block WITHOUT the residual connection.\"\"\"\n",
        "    def __init__(self, in_channels, out_channels, stride=1):\n",
        "        super(NonResidualBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        out = self.relu(out)\n",
        "        return out\n",
        "\n",
        "class ResNetTransformer(nn.Module):\n",
        "    \"\"\"\n",
        "    A hybrid architecture combining a non-residual CNN backbone with a Transformer encoder,\n",
        "    including positional embeddings.\n",
        "    \"\"\"\n",
        "    def __init__(self, block, layers, num_classes, t_config):\n",
        "        super(ResNetTransformer, self).__init__()\n",
        "        self.in_channels = 64\n",
        "        self.embedding_dim = t_config[\"embedding_dim\"]\n",
        "        self.num_tokens = len(layers) + 1  # 4 layers + 1 initial capture\n",
        "\n",
        "        # 1. CNN Backbone (Feature Extractor)\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "        self.layer1 = self._make_layer(block, 64, layers[0], stride=1)\n",
        "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n",
        "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n",
        "\n",
        "        # 2. Projection heads to create tokens from feature maps\n",
        "        self.projections = nn.ModuleList([\n",
        "            self._create_projection(64, self.embedding_dim),\n",
        "            self._create_projection(64, self.embedding_dim),\n",
        "            self._create_projection(128, self.embedding_dim),\n",
        "            self._create_projection(256, self.embedding_dim),\n",
        "            self._create_projection(512, self.embedding_dim)\n",
        "        ])\n",
        "\n",
        "        # 3. Learnable Positional Embedding\n",
        "        self.positional_embedding = nn.Parameter(torch.randn(1, self.num_tokens, self.embedding_dim))\n",
        "\n",
        "        # 4. Transformer Encoder\n",
        "        encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=self.embedding_dim,\n",
        "            nhead=t_config[\"nhead\"],\n",
        "            dim_feedforward=t_config[\"dim_feedforward\"],\n",
        "            dropout=t_config[\"dropout\"],\n",
        "            batch_first=True\n",
        "        )\n",
        "        self.transformer_encoder = nn.TransformerEncoder(\n",
        "            encoder_layer,\n",
        "            num_layers=t_config[\"num_encoder_layers\"]\n",
        "        )\n",
        "\n",
        "        # 5. Final Classifier\n",
        "        self.classifier = nn.Linear(self.embedding_dim, num_classes)\n",
        "\n",
        "    def _create_projection(self, in_features, out_features):\n",
        "        return nn.Sequential(\n",
        "            nn.AdaptiveAvgPool2d((1, 1)),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(in_features, out_features)\n",
        "        )\n",
        "\n",
        "    def _make_layer(self, block, out_channels, num_blocks, stride):\n",
        "        strides = [stride] + [1] * (num_blocks - 1)\n",
        "        layers = []\n",
        "        current_in_channels = self.in_channels\n",
        "        for s in strides:\n",
        "            layers.append(block(current_in_channels, out_channels, s))\n",
        "            current_in_channels = out_channels\n",
        "        self.in_channels = out_channels\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # 1. Pass through CNN backbone and capture features\n",
        "        features = []\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "        features.append(x)  # Capture 1: After initial maxpool\n",
        "\n",
        "        x = self.layer1(x); features.append(x)  # Capture 2: After layer1\n",
        "        x = self.layer2(x); features.append(x)  # Capture 3: After layer2\n",
        "        x = self.layer3(x); features.append(x)  # Capture 4: After layer3\n",
        "        x = self.layer4(x); features.append(x)  # Capture 5: After layer4\n",
        "\n",
        "        # 2. Project features to tokens\n",
        "        tokens = [self.projections[i](feature_map) for i, feature_map in enumerate(features)]\n",
        "\n",
        "        # 3. Stack tokens into a sequence\n",
        "        token_sequence = torch.stack(tokens, dim=1)\n",
        "\n",
        "        # 4. Add positional embedding\n",
        "        token_sequence += self.positional_embedding\n",
        "\n",
        "        # 5. Pass through Transformer\n",
        "        transformer_out = self.transformer_encoder(token_sequence)\n",
        "\n",
        "        # 6. Aggregate and classify\n",
        "        aggregated_vector = transformer_out.mean(dim=1)\n",
        "        logits = self.classifier(aggregated_vector)\n",
        "\n",
        "        return logits\n",
        "\n",
        "def evaluate_model(model, data_loader, criterion, device):\n",
        "    \"\"\"Evaluates the model on a given dataset.\"\"\"\n",
        "    model.eval()\n",
        "    total_loss, correct, total = 0.0, 0, 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in data_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            total_loss += loss.item() * images.size(0)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    avg_loss = total_loss / total\n",
        "    accuracy = 100 * correct / total\n",
        "    return avg_loss, accuracy\n",
        "\n",
        "def train_and_validate(model, train_loader, val_loader, config):\n",
        "    \"\"\"Main training loop with callbacks.\"\"\"\n",
        "    device = config[\"device\"]\n",
        "    model.to(device)\n",
        "\n",
        "    # Set seeds for reproducible training\n",
        "    torch.manual_seed(config[\"split_seed\"])\n",
        "    np.random.seed(config[\"split_seed\"])\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(config[\"split_seed\"])\n",
        "        torch.cuda.manual_seed_all(config[\"split_seed\"])\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=config[\"learning_rate\"], weight_decay=config[\"weight_decay\"])\n",
        "\n",
        "    # Initialize callbacks\n",
        "    callbacks = [\n",
        "        ReduceLROnPlateau(\n",
        "            optimizer=optimizer,\n",
        "            monitor='val_accuracy',\n",
        "            factor=0.2,\n",
        "            patience=3,\n",
        "            min_lr=1e-7,\n",
        "            verbose=1\n",
        "        ),\n",
        "        EarlyStopping(\n",
        "            monitor='val_accuracy',\n",
        "            patience=7,\n",
        "            restore_best_weights=True,\n",
        "            verbose=1\n",
        "        ),\n",
        "        ModelCheckpoint(\n",
        "            filepath=config[\"checkpoint_path\"],\n",
        "            save_best_only=True,\n",
        "            monitor='val_accuracy',\n",
        "            verbose=1\n",
        "        )\n",
        "    ]\n",
        "\n",
        "    print(\"\\n--- Starting Training ---\")\n",
        "    early_stop = False\n",
        "\n",
        "    for epoch in range(config[\"num_epochs\"]):\n",
        "        if early_stop:\n",
        "            break\n",
        "\n",
        "        model.train()\n",
        "        start_time = time.time()\n",
        "        running_loss, train_correct, train_total = 0.0, 0, 0\n",
        "\n",
        "        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{config['num_epochs']} [Train]\", leave=False)\n",
        "        for images, labels in progress_bar:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item() * images.size(0)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            train_total += labels.size(0)\n",
        "            train_correct += (predicted == labels).sum().item()\n",
        "            progress_bar.set_postfix(loss=loss.item())\n",
        "\n",
        "        train_loss = running_loss / train_total\n",
        "        train_acc = 100 * train_correct / train_total\n",
        "        val_loss, val_acc = evaluate_model(model, val_loader, criterion, device)\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{config['num_epochs']} | Time: {time.time() - start_time:.2f}s | \"\n",
        "              f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}% | \"\n",
        "              f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%\")\n",
        "\n",
        "        # Prepare logs for callbacks\n",
        "        logs = {\n",
        "            'train_loss': train_loss,\n",
        "            'train_accuracy': train_acc,\n",
        "            'val_loss': val_loss,\n",
        "            'val_accuracy': val_acc\n",
        "        }\n",
        "\n",
        "        # Execute callbacks\n",
        "        for callback in callbacks:\n",
        "            if isinstance(callback, EarlyStopping):\n",
        "                if callback.on_epoch_end(epoch, logs, model):\n",
        "                    early_stop = True\n",
        "                    break\n",
        "            elif isinstance(callback, ModelCheckpoint):\n",
        "                callback.on_epoch_end(epoch, logs, model)\n",
        "            else:\n",
        "                callback.on_epoch_end(epoch, logs)\n",
        "\n",
        "    # Execute callback cleanup\n",
        "    for callback in callbacks:\n",
        "        if isinstance(callback, EarlyStopping):\n",
        "            callback.on_training_end(model)\n",
        "        else:\n",
        "            callback.on_training_end()\n",
        "\n",
        "    print(\"--- Training Finished ---\\n\")\n",
        "    return model\n",
        "\n",
        "# ==========================================\n",
        "# Evaluation\n",
        "# ==========================================\n",
        "\n",
        "def calculate_top_k_accuracy(outputs, labels, k_values=[1, 3, 5]):\n",
        "    \"\"\"Calculate top-k accuracy for given k values.\"\"\"\n",
        "    batch_size = labels.size(0)\n",
        "    _, pred = outputs.topk(max(k_values), 1, True, True)\n",
        "    pred = pred.t()\n",
        "    correct = pred.eq(labels.view(1, -1).expand_as(pred))\n",
        "\n",
        "    top_k_accuracies = {}\n",
        "    for k in k_values:\n",
        "        correct_k = correct[:k].reshape(-1).float().sum(0, keepdim=True)\n",
        "        top_k_accuracies[k] = correct_k.item() / batch_size\n",
        "\n",
        "    return top_k_accuracies\n",
        "\n",
        "def calculate_entropy(probs):\n",
        "    \"\"\"Calculate entropy of probability distributions.\"\"\"\n",
        "    # Add small epsilon to avoid log(0)\n",
        "    epsilon = 1e-8\n",
        "    probs = probs + epsilon\n",
        "    entropy = -torch.sum(probs * torch.log(probs), dim=1)\n",
        "    return entropy\n",
        "\n",
        "def calculate_ece(confidences, accuracies, n_bins=10):\n",
        "    \"\"\"Calculate Expected Calibration Error.\"\"\"\n",
        "    bin_boundaries = torch.linspace(0, 1, n_bins + 1)\n",
        "    bin_lowers = bin_boundaries[:-1]\n",
        "    bin_uppers = bin_boundaries[1:]\n",
        "\n",
        "    ece = 0\n",
        "    for bin_lower, bin_upper in zip(bin_lowers, bin_uppers):\n",
        "        in_bin = (confidences > bin_lower.item()) & (confidences <= bin_upper.item())\n",
        "        prop_in_bin = in_bin.float().mean()\n",
        "\n",
        "        if prop_in_bin.item() > 0:\n",
        "            accuracy_in_bin = accuracies[in_bin].float().mean()\n",
        "            avg_confidence_in_bin = confidences[in_bin].mean()\n",
        "            ece += torch.abs(avg_confidence_in_bin - accuracy_in_bin) * prop_in_bin\n",
        "\n",
        "    return ece.item()\n",
        "\n",
        "def comprehensive_evaluation(model, test_loader, criterion, device):\n",
        "    \"\"\"Comprehensive evaluation with all requested metrics.\"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    all_predictions = []\n",
        "    all_labels = []\n",
        "    all_confidences = []\n",
        "    all_entropies = []\n",
        "    all_top_k_results = {1: [], 3: [], 5: []}\n",
        "    total_loss = 0.0\n",
        "    total_samples = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in tqdm(test_loader, desc=\"Evaluating\"):\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            # Convert outputs to probabilities\n",
        "            probs = torch.softmax(outputs, dim=1)\n",
        "\n",
        "            # Get predictions and confidences\n",
        "            confidences, predictions = torch.max(probs, dim=1)\n",
        "\n",
        "            # Calculate entropy\n",
        "            entropies = calculate_entropy(probs)\n",
        "\n",
        "            # Calculate top-k accuracies\n",
        "            top_k_accs = calculate_top_k_accuracy(outputs, labels, [1, 3, 5])\n",
        "\n",
        "            # Store results\n",
        "            all_predictions.extend(predictions.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "            all_confidences.extend(confidences.cpu().numpy())\n",
        "            all_entropies.extend(entropies.cpu().numpy())\n",
        "\n",
        "            for k in [1, 3, 5]:\n",
        "                all_top_k_results[k].extend([top_k_accs[k]] * labels.size(0))\n",
        "\n",
        "            total_loss += loss.item() * images.size(0)\n",
        "            total_samples += images.size(0)\n",
        "\n",
        "    # Convert to numpy arrays\n",
        "    all_predictions = np.array(all_predictions)\n",
        "    all_labels = np.array(all_labels)\n",
        "    all_confidences = np.array(all_confidences)\n",
        "    all_entropies = np.array(all_entropies)\n",
        "\n",
        "    # Calculate basic metrics\n",
        "    avg_loss = total_loss / total_samples\n",
        "\n",
        "    # Calculate top-k accuracies\n",
        "    top1 = np.mean([pred == label for pred, label in zip(all_predictions, all_labels)])\n",
        "    top3 = np.mean(all_top_k_results[3])\n",
        "    top5 = np.mean(all_top_k_results[5])\n",
        "\n",
        "    # Calculate precision, recall, f1-score\n",
        "    precision_macro, recall_macro, f1_macro, _ = precision_recall_fscore_support(\n",
        "        all_labels, all_predictions, average='macro', zero_division=0\n",
        "    )\n",
        "    precision_weighted, recall_weighted, f1_weighted, _ = precision_recall_fscore_support(\n",
        "        all_labels, all_predictions, average='weighted', zero_division=0\n",
        "    )\n",
        "\n",
        "    # Calculate confidence and calibration metrics\n",
        "    avg_confidence = np.mean(all_confidences)\n",
        "    std_confidence = np.std(all_confidences)\n",
        "    avg_entropy = np.mean(all_entropies)\n",
        "\n",
        "    # Calculate ECE\n",
        "    accuracies = (all_predictions == all_labels).astype(float)\n",
        "    ece = calculate_ece(torch.tensor(all_confidences), torch.tensor(accuracies))\n",
        "\n",
        "    return {\n",
        "        'test_loss': avg_loss,\n",
        "        'top1': top1,\n",
        "        'top3': top3,\n",
        "        'top5': top5,\n",
        "        'precision_macro': precision_macro,\n",
        "        'recall_macro': recall_macro,\n",
        "        'f1_macro': f1_macro,\n",
        "        'precision_weighted': precision_weighted,\n",
        "        'recall_weighted': recall_weighted,\n",
        "        'f1_weighted': f1_weighted,\n",
        "        'avg_confidence': avg_confidence,\n",
        "        'std_confidence': std_confidence,\n",
        "        'avg_entropy': avg_entropy,\n",
        "        'ece': ece\n",
        "    }\n",
        "\n",
        "# ==========================================\n",
        "# Main Execution\n",
        "# ==========================================\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # Set global seeds for full reproducibility\n",
        "    torch.manual_seed(CONFIG[\"split_seed\"])\n",
        "    np.random.seed(CONFIG[\"split_seed\"])\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(CONFIG[\"split_seed\"])\n",
        "        torch.cuda.manual_seed_all(CONFIG[\"split_seed\"])\n",
        "        # Ensure deterministic behavior on CUDA\n",
        "        torch.backends.cudnn.deterministic = True\n",
        "        torch.backends.cudnn.benchmark = False\n",
        "\n",
        "    print(f\"Using device: {CONFIG['device']}\")\n",
        "    print(f\"Reproducibility seed: {CONFIG['split_seed']}\")\n",
        "\n",
        "    # Get data loaders\n",
        "    dataloaders = get_dataloaders(CONFIG)\n",
        "    if dataloaders is None:\n",
        "        print(\"Could not prepare data. Halting execution.\", file=sys.stderr)\n",
        "        sys.exit(1)  # Exit if data preparation failed\n",
        "\n",
        "    train_loader, val_loader, test_loader = dataloaders\n",
        "\n",
        "    # Initialize and train model with ResNetTransformer\n",
        "    model = ResNetTransformer(\n",
        "        block=NonResidualBlock,\n",
        "        layers=[2, 2, 2, 2],\n",
        "        num_classes=CONFIG[\"num_classes\"],\n",
        "        t_config=TRANSFORMER_CONFIG\n",
        "    )\n",
        "\n",
        "    print(f\"Model has {sum(p.numel() for p in model.parameters())} parameters\")\n",
        "    print(f\"Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad)}\")\n",
        "\n",
        "    trained_model = train_and_validate(model, train_loader, val_loader, CONFIG)\n",
        "\n",
        "    # Comprehensive evaluation on the test set\n",
        "    print(\"--- Starting Comprehensive Evaluation on Test Set ---\")\n",
        "    results = comprehensive_evaluation(trained_model, test_loader, nn.CrossEntropyLoss(), CONFIG[\"device\"])\n",
        "\n",
        "    # Print all requested metrics\n",
        "    print(\"\\n--- Top-K Accuracy Results ---\")\n",
        "    print(f\"Top-1 Accuracy: {results['top1'] * 100:.2f}%\")\n",
        "    print(f\"Top-3 Accuracy: {results['top3'] * 100:.2f}%\")\n",
        "    print(f\"Top-5 Accuracy: {results['top5'] * 100:.2f}%\")\n",
        "\n",
        "    print(\"\\n--- Additional Performance Metrics ---\")\n",
        "    print(f\"Macro Average Precision: {results['precision_macro']:.4f}\")\n",
        "    print(f\"Macro Average Recall: {results['recall_macro']:.4f}\")\n",
        "    print(f\"Macro Average F1-Score: {results['f1_macro']:.4f}\")\n",
        "    print(f\"Weighted Average Precision: {results['precision_weighted']:.4f}\")\n",
        "    print(f\"Weighted Average Recall: {results['recall_weighted']:.4f}\")\n",
        "    print(f\"Weighted Average F1-Score: {results['f1_weighted']:.4f}\")\n",
        "\n",
        "    print(\"\\n--- Model Confidence & Calibration Metrics ---\")\n",
        "    print(f\"Average Prediction Confidence: {results['avg_confidence']:.4f}\")\n",
        "    print(f\"Confidence Standard Deviation: {results['std_confidence']:.4f}\")\n",
        "    print(f\"Average Prediction Entropy: {results['avg_entropy']:.4f}\")\n",
        "    print(f\"Expected Calibration Error: {results['ece']:.4f}\")\n",
        "\n",
        "    print(f\"\\nFinal Test Loss: {results['test_loss']:.4f}\")\n",
        "    print(\"===============================================\")"
      ],
      "metadata": {
        "id": "-GxwWEhyOOV8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6eb42559-8488-458f-ea50-b2a472d16614"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Reproducibility seed: 42\n",
            "Dataset: tissuemnist\n",
            "Task: multi-class\n",
            "Number of channels: 1\n",
            "Number of classes: 8\n",
            "Loading tissuemnist dataset...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 125M/125M [00:01<00:00, 85.2MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset splits - Train: 165466, Val: 23640, Test: 47280\n",
            "Model has 12850760 parameters\n",
            "Trainable parameters: 12850760\n",
            "\n",
            "--- Starting Training ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100 | Time: 164.71s | Train Loss: 1.3057, Train Acc: 50.24% | Val Loss: 1.4656, Val Acc: 48.65%\n",
            "\n",
            "Model saved to ./best_model.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/100 | Time: 161.76s | Train Loss: 1.1490, Train Acc: 57.30% | Val Loss: 1.4011, Val Acc: 54.01%\n",
            "\n",
            "Model saved to ./best_model.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/100 | Time: 162.54s | Train Loss: 1.0730, Train Acc: 60.62% | Val Loss: 1.0377, Val Acc: 61.40%\n",
            "\n",
            "Model saved to ./best_model.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4/100 | Time: 158.03s | Train Loss: 1.0320, Train Acc: 62.09% | Val Loss: 1.0803, Val Acc: 60.63%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5/100 | Time: 158.48s | Train Loss: 0.9983, Train Acc: 63.48% | Val Loss: 1.1180, Val Acc: 58.79%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6/100 | Time: 157.74s | Train Loss: 0.9763, Train Acc: 64.13% | Val Loss: 1.0313, Val Acc: 62.45%\n",
            "\n",
            "Model saved to ./best_model.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7/100 | Time: 158.97s | Train Loss: 0.9595, Train Acc: 64.79% | Val Loss: 1.0006, Val Acc: 63.17%\n",
            "\n",
            "Model saved to ./best_model.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8/100 | Time: 161.24s | Train Loss: 0.9476, Train Acc: 65.13% | Val Loss: 0.9543, Val Acc: 64.84%\n",
            "\n",
            "Model saved to ./best_model.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9/100 | Time: 160.46s | Train Loss: 0.9382, Train Acc: 65.62% | Val Loss: 0.9737, Val Acc: 64.26%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10/100 | Time: 158.27s | Train Loss: 0.9277, Train Acc: 66.02% | Val Loss: 0.9349, Val Acc: 65.81%\n",
            "\n",
            "Model saved to ./best_model.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 11/100 | Time: 158.71s | Train Loss: 0.9211, Train Acc: 66.21% | Val Loss: 1.0612, Val Acc: 61.37%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 12/100 | Time: 158.69s | Train Loss: 0.9161, Train Acc: 66.43% | Val Loss: 1.0138, Val Acc: 62.75%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 13/100 | Time: 157.61s | Train Loss: 0.9099, Train Acc: 66.65% | Val Loss: 0.9815, Val Acc: 64.82%\n",
            "\n",
            "Reducing learning rate from 1.00e-03 to 2.00e-04\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 14/100 | Time: 158.21s | Train Loss: 0.8493, Train Acc: 69.02% | Val Loss: 0.8845, Val Acc: 67.90%\n",
            "\n",
            "Model saved to ./best_model.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 15/100 | Time: 158.73s | Train Loss: 0.8355, Train Acc: 69.66% | Val Loss: 0.8560, Val Acc: 68.97%\n",
            "\n",
            "Model saved to ./best_model.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 16/100 | Time: 160.79s | Train Loss: 0.8269, Train Acc: 69.83% | Val Loss: 0.8631, Val Acc: 68.71%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 17/100 | Time: 158.67s | Train Loss: 0.8202, Train Acc: 70.18% | Val Loss: 0.8604, Val Acc: 68.90%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 18/100 | Time: 156.58s | Train Loss: 0.8146, Train Acc: 70.28% | Val Loss: 0.8480, Val Acc: 69.48%\n",
            "\n",
            "Model saved to ./best_model.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 19/100 | Time: 157.30s | Train Loss: 0.8103, Train Acc: 70.59% | Val Loss: 0.8655, Val Acc: 68.69%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 20/100 | Time: 157.29s | Train Loss: 0.8044, Train Acc: 70.82% | Val Loss: 0.8550, Val Acc: 69.30%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 21/100 | Time: 156.97s | Train Loss: 0.8020, Train Acc: 70.93% | Val Loss: 0.8440, Val Acc: 69.36%\n",
            "\n",
            "Reducing learning rate from 2.00e-04 to 4.00e-05\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 22/100 | Time: 157.28s | Train Loss: 0.7787, Train Acc: 71.74% | Val Loss: 0.8349, Val Acc: 69.98%\n",
            "\n",
            "Model saved to ./best_model.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 23/100 | Time: 158.05s | Train Loss: 0.7743, Train Acc: 71.90% | Val Loss: 0.8365, Val Acc: 69.91%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 24/100 | Time: 157.37s | Train Loss: 0.7726, Train Acc: 71.92% | Val Loss: 0.8349, Val Acc: 70.17%\n",
            "\n",
            "Model saved to ./best_model.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 25/100 | Time: 158.63s | Train Loss: 0.7698, Train Acc: 72.14% | Val Loss: 0.8351, Val Acc: 69.95%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 26/100 | Time: 157.71s | Train Loss: 0.7672, Train Acc: 72.26% | Val Loss: 0.8374, Val Acc: 69.89%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 27/100 | Time: 158.33s | Train Loss: 0.7649, Train Acc: 72.33% | Val Loss: 0.8350, Val Acc: 69.96%\n",
            "\n",
            "Reducing learning rate from 4.00e-05 to 8.00e-06\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 28/100 | Time: 158.78s | Train Loss: 0.7596, Train Acc: 72.45% | Val Loss: 0.8356, Val Acc: 69.98%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 29/100 | Time: 160.92s | Train Loss: 0.7609, Train Acc: 72.45% | Val Loss: 0.8339, Val Acc: 70.03%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 30/100 | Time: 158.12s | Train Loss: 0.7576, Train Acc: 72.62% | Val Loss: 0.8368, Val Acc: 69.89%\n",
            "\n",
            "Reducing learning rate from 8.00e-06 to 1.60e-06\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 31/100 | Time: 158.69s | Train Loss: 0.7565, Train Acc: 72.66% | Val Loss: 0.8387, Val Acc: 69.93%\n",
            "\n",
            "Early stopping at epoch 31\n",
            "Restored model weights from the end of the best epoch: 24\n",
            "--- Training Finished ---\n",
            "\n",
            "--- Starting Comprehensive Evaluation on Test Set ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating: 100%|██████████| 370/370 [00:18<00:00, 20.08it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Top-K Accuracy Results ---\n",
            "Top-1 Accuracy: 69.88%\n",
            "Top-3 Accuracy: 93.92%\n",
            "Top-5 Accuracy: 98.64%\n",
            "\n",
            "--- Additional Performance Metrics ---\n",
            "Macro Average Precision: 0.6318\n",
            "Macro Average Recall: 0.6125\n",
            "Macro Average F1-Score: 0.6175\n",
            "Weighted Average Precision: 0.6959\n",
            "Weighted Average Recall: 0.6988\n",
            "Weighted Average F1-Score: 0.6945\n",
            "\n",
            "--- Model Confidence & Calibration Metrics ---\n",
            "Average Prediction Confidence: 0.7381\n",
            "Confidence Standard Deviation: 0.2084\n",
            "Average Prediction Entropy: 0.7242\n",
            "Expected Calibration Error: 0.0393\n",
            "\n",
            "Final Test Loss: 0.8346\n",
            "===============================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hybrid Transformer on ResNet-18 (RoPE)"
      ],
      "metadata": {
        "id": "Ls_GmrMQOQF6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.datasets import ImageFolder\n",
        "import os\n",
        "import requests\n",
        "import zipfile\n",
        "import shutil\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "import sys\n",
        "import math\n",
        "import numpy as np\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "import medmnist\n",
        "from medmnist import INFO\n",
        "\n",
        "# --- Configuration Parameters ---\n",
        "CONFIG = {\n",
        "    \"batch_size\": 128,\n",
        "    \"num_epochs\": 100,\n",
        "    \"learning_rate\": 0.001,\n",
        "    \"weight_decay\": 1e-4,\n",
        "    \"split_seed\": 42,\n",
        "    \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
        "    \"checkpoint_path\": \"./best_model.pth\",\n",
        "}\n",
        "\n",
        "# --- Transformer Configuration ---\n",
        "TRANSFORMER_CONFIG = {\n",
        "    \"embedding_dim\": 256,\n",
        "    \"nhead\": 8,\n",
        "    \"num_encoder_layers\": 3,\n",
        "    \"dim_feedforward\": 512,\n",
        "    \"dropout\": 0.1,\n",
        "}\n",
        "\n",
        "# ==========================================\n",
        "# Callback Classes\n",
        "# ==========================================\n",
        "\n",
        "class Callback:\n",
        "    \"\"\"Base callback class\"\"\"\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        pass\n",
        "\n",
        "    def on_training_end(self):\n",
        "        pass\n",
        "\n",
        "class ReduceLROnPlateau(Callback):\n",
        "    \"\"\"Reduce learning rate when a metric has stopped improving\"\"\"\n",
        "    def __init__(self, optimizer, monitor='val_accuracy', factor=0.2, patience=3, min_lr=1e-7, verbose=1):\n",
        "        self.optimizer = optimizer\n",
        "        self.monitor = monitor\n",
        "        self.factor = factor\n",
        "        self.patience = patience\n",
        "        self.min_lr = min_lr\n",
        "        self.verbose = verbose\n",
        "        self.wait = 0\n",
        "        self.best = None\n",
        "        self.mode = 'min' if 'loss' in monitor else 'max'\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        if logs is None:\n",
        "            return\n",
        "\n",
        "        current = logs.get(self.monitor)\n",
        "        if current is None:\n",
        "            return\n",
        "\n",
        "        if self.best is None:\n",
        "            self.best = current\n",
        "            return\n",
        "\n",
        "        if self.mode == 'min':\n",
        "            improved = current < self.best\n",
        "        else:\n",
        "            improved = current > self.best\n",
        "\n",
        "        if improved:\n",
        "            self.best = current\n",
        "            self.wait = 0\n",
        "        else:\n",
        "            self.wait += 1\n",
        "            if self.wait >= self.patience:\n",
        "                old_lr = self.optimizer.param_groups[0]['lr']\n",
        "                new_lr = max(old_lr * self.factor, self.min_lr)\n",
        "                if new_lr != old_lr:\n",
        "                    self.optimizer.param_groups[0]['lr'] = new_lr\n",
        "                    if self.verbose:\n",
        "                        print(f\"\\nReducing learning rate from {old_lr:.2e} to {new_lr:.2e}\")\n",
        "                    self.wait = 0\n",
        "\n",
        "class EarlyStopping(Callback):\n",
        "    \"\"\"Stop training when a monitored metric has stopped improving\"\"\"\n",
        "    def __init__(self, monitor='val_accuracy', patience=7, restore_best_weights=True, verbose=1):\n",
        "        self.monitor = monitor\n",
        "        self.patience = patience\n",
        "        self.restore_best_weights = restore_best_weights\n",
        "        self.verbose = verbose\n",
        "        self.wait = 0\n",
        "        self.best = None\n",
        "        self.best_weights = None\n",
        "        self.mode = 'min' if 'loss' in monitor else 'max'\n",
        "        self.stopped_epoch = 0\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None, model=None):\n",
        "        if logs is None:\n",
        "            return False\n",
        "\n",
        "        current = logs.get(self.monitor)\n",
        "        if current is None:\n",
        "            return False\n",
        "\n",
        "        if self.best is None:\n",
        "            self.best = current\n",
        "            if model is not None and self.restore_best_weights:\n",
        "                self.best_weights = model.state_dict().copy()\n",
        "            return False\n",
        "\n",
        "        if self.mode == 'min':\n",
        "            improved = current < self.best\n",
        "        else:\n",
        "            improved = current > self.best\n",
        "\n",
        "        if improved:\n",
        "            self.best = current\n",
        "            self.wait = 0\n",
        "            if model is not None and self.restore_best_weights:\n",
        "                self.best_weights = model.state_dict().copy()\n",
        "        else:\n",
        "            self.wait += 1\n",
        "            if self.wait >= self.patience:\n",
        "                self.stopped_epoch = epoch\n",
        "                if self.verbose:\n",
        "                    print(f\"\\nEarly stopping at epoch {epoch + 1}\")\n",
        "                return True\n",
        "        return False\n",
        "\n",
        "    def on_training_end(self, model=None):\n",
        "        if self.stopped_epoch > 0 and self.verbose:\n",
        "            print(f\"Restored model weights from the end of the best epoch: {self.stopped_epoch + 1 - self.patience}\")\n",
        "        if model is not None and self.restore_best_weights and self.best_weights is not None:\n",
        "            model.load_state_dict(self.best_weights)\n",
        "\n",
        "class ModelCheckpoint(Callback):\n",
        "    \"\"\"Save the model after every epoch\"\"\"\n",
        "    def __init__(self, filepath, save_best_only=True, monitor='val_accuracy', verbose=1):\n",
        "        self.filepath = filepath\n",
        "        self.save_best_only = save_best_only\n",
        "        self.monitor = monitor\n",
        "        self.verbose = verbose\n",
        "        self.best = None\n",
        "        self.mode = 'min' if 'loss' in monitor else 'max'\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None, model=None):\n",
        "        if logs is None or model is None:\n",
        "            return\n",
        "\n",
        "        current = logs.get(self.monitor)\n",
        "        if current is None:\n",
        "            return\n",
        "\n",
        "        if not self.save_best_only:\n",
        "            filepath = self.filepath.replace('.pth', f'_epoch_{epoch + 1}.pth')\n",
        "            torch.save(model.state_dict(), filepath)\n",
        "            if self.verbose:\n",
        "                print(f\"\\nModel saved to {filepath}\")\n",
        "            return\n",
        "\n",
        "        if self.best is None:\n",
        "            self.best = current\n",
        "            torch.save(model.state_dict(), self.filepath)\n",
        "            if self.verbose:\n",
        "                print(f\"\\nModel saved to {self.filepath}\")\n",
        "            return\n",
        "\n",
        "        if self.mode == 'min':\n",
        "            improved = current < self.best\n",
        "        else:\n",
        "            improved = current > self.best\n",
        "\n",
        "        if improved:\n",
        "            self.best = current\n",
        "            torch.save(model.state_dict(), self.filepath)\n",
        "            if self.verbose:\n",
        "                print(f\"\\nModel saved to {self.filepath}\")\n",
        "\n",
        "# ==========================================\n",
        "# Data Preprocessing\n",
        "# ==========================================\n",
        "\n",
        "def get_dataloaders(config):\n",
        "    \"\"\"Downloads, prepares MedMNIST data with augmentation, returning DataLoaders.\"\"\"\n",
        "\n",
        "    # MedMNIST dataset configuration\n",
        "    dataset_name = \"tissuemnist\"  # Options: pathmnist, chestmnist, dermamnist, octmnist, pneumoniamnist, retinamnist, breastmnist, bloodmnist, tissuemnist, organamnist, organcmnist, organsmnist\n",
        "\n",
        "    # Get dataset info\n",
        "    info = INFO[dataset_name]\n",
        "    task = info['task']\n",
        "    n_channels = info['n_channels']\n",
        "    n_classes = len(info['label'])\n",
        "\n",
        "    print(f\"Dataset: {dataset_name}\")\n",
        "    print(f\"Task: {task}\")\n",
        "    print(f\"Number of channels: {n_channels}\")\n",
        "    print(f\"Number of classes: {n_classes}\")\n",
        "\n",
        "    # Update config with dataset info\n",
        "    config[\"num_classes\"] = n_classes\n",
        "\n",
        "    # Get the dataset class\n",
        "    DataClass = getattr(medmnist, info['python_class'])\n",
        "\n",
        "    # Define transforms\n",
        "    # Training transforms (with augmentation)\n",
        "    train_transform = transforms.Compose([\n",
        "        transforms.Resize((64, 64)),  # Resize to match original Tiny ImageNet size\n",
        "        transforms.RandomRotation(degrees=10),\n",
        "        transforms.RandomHorizontalFlip(p=0.5),\n",
        "        transforms.ColorJitter(brightness=0.1, contrast=0.1),\n",
        "        transforms.ToTensor(),\n",
        "        # Convert grayscale to RGB if needed\n",
        "        transforms.Lambda(lambda x: x.repeat(3, 1, 1) if x.size(0) == 1 else x),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "    # Validation/Test transforms (no augmentation)\n",
        "    val_test_transform = transforms.Compose([\n",
        "        transforms.Resize((64, 64)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Lambda(lambda x: x.repeat(3, 1, 1) if x.size(0) == 1 else x),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "    print(f\"Loading {dataset_name} dataset...\")\n",
        "\n",
        "    # Load datasets - MedMNIST provides predefined train/val/test splits\n",
        "    train_dataset = DataClass(\n",
        "        split='train',\n",
        "        transform=train_transform,\n",
        "        download=True,  # Will download if not present\n",
        "        as_rgb=False    # We handle RGB conversion in transforms\n",
        "    )\n",
        "\n",
        "    val_dataset = DataClass(\n",
        "        split='val',\n",
        "        transform=val_test_transform,\n",
        "        download=True,\n",
        "        as_rgb=False\n",
        "    )\n",
        "\n",
        "    test_dataset = DataClass(\n",
        "        split='test',\n",
        "        transform=val_test_transform,\n",
        "        download=True,\n",
        "        as_rgb=False\n",
        "    )\n",
        "\n",
        "    print(f\"Dataset splits - Train: {len(train_dataset)}, Val: {len(val_dataset)}, Test: {len(test_dataset)}\")\n",
        "\n",
        "    # Handle different task types\n",
        "    if task == 'multi-label, binary-class':\n",
        "        def collate_fn(batch):\n",
        "            images = torch.stack([item[0] for item in batch])\n",
        "            # Fix: Convert to numpy array first, then to tensor\n",
        "            labels_np = np.array([item[1] for item in batch])\n",
        "            labels = torch.tensor(labels_np, dtype=torch.float32).squeeze()\n",
        "            return images, labels\n",
        "    else:\n",
        "        def collate_fn(batch):\n",
        "            images = torch.stack([item[0] for item in batch])\n",
        "            # Fix: Convert to numpy array first, then to tensor\n",
        "            labels_np = np.array([item[1] for item in batch])\n",
        "            labels = torch.tensor(labels_np, dtype=torch.long).squeeze()\n",
        "            return images, labels\n",
        "\n",
        "    # Set worker init function for reproducible DataLoader behavior\n",
        "    def worker_init_fn(worker_id):\n",
        "        np.random.seed(config[\"split_seed\"] + worker_id)\n",
        "\n",
        "    # Create DataLoaders\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=config[\"batch_size\"],\n",
        "        shuffle=True,\n",
        "        num_workers=2,\n",
        "        pin_memory=True,\n",
        "        worker_init_fn=worker_init_fn,\n",
        "        collate_fn=collate_fn,\n",
        "        generator=torch.Generator().manual_seed(config[\"split_seed\"])\n",
        "    )\n",
        "\n",
        "    val_loader = DataLoader(\n",
        "        val_dataset,\n",
        "        batch_size=config[\"batch_size\"],\n",
        "        shuffle=False,\n",
        "        num_workers=2,\n",
        "        pin_memory=True,\n",
        "        worker_init_fn=worker_init_fn,\n",
        "        collate_fn=collate_fn\n",
        "    )\n",
        "\n",
        "    test_loader = DataLoader(\n",
        "        test_dataset,\n",
        "        batch_size=config[\"batch_size\"],\n",
        "        shuffle=False,\n",
        "        num_workers=2,\n",
        "        pin_memory=True,\n",
        "        worker_init_fn=worker_init_fn,\n",
        "        collate_fn=collate_fn\n",
        "    )\n",
        "\n",
        "    return train_loader, val_loader, test_loader\n",
        "\n",
        "# ==========================================\n",
        "# Model Architecture and Training\n",
        "# ==========================================\n",
        "\n",
        "class RotaryEmbedding(nn.Module):\n",
        "    \"\"\"\n",
        "    The Rotary Positional Embedding (RoPE) module.\n",
        "    This implementation is based on the paper \"RoFormer: Enhanced Transformer with Rotary Position Embedding\".\n",
        "    \"\"\"\n",
        "    def __init__(self, dim, base=10000):\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "        self.base = base\n",
        "        # Create inverse frequencies and register as a buffer\n",
        "        inv_freq = 1.0 / (self.base ** (torch.arange(0, dim, 2).float() / dim))\n",
        "        self.register_buffer(\"inv_freq\", inv_freq)\n",
        "        self.seq_len_cached = None\n",
        "        self.cos_cached = None\n",
        "        self.sin_cached = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        seq_len = x.shape[1]\n",
        "        # Check if we need to recompute the cache\n",
        "        if seq_len != self.seq_len_cached:\n",
        "            self.seq_len_cached = seq_len\n",
        "            t = torch.arange(seq_len, device=x.device).type_as(self.inv_freq)\n",
        "            freqs = torch.einsum(\"i,j->ij\", t, self.inv_freq)\n",
        "            emb = torch.cat((freqs, freqs), dim=-1).to(x.device)\n",
        "            self.cos_cached = emb.cos()[:, None, :]\n",
        "            self.sin_cached = emb.sin()[:, None, :]\n",
        "\n",
        "        # Apply the rotation\n",
        "        return self.cos_cached, self.sin_cached\n",
        "\n",
        "def rotate_half(x):\n",
        "    x1, x2 = x[..., : x.shape[-1] // 2], x[..., x.shape[-1] // 2 :]\n",
        "    return torch.cat((-x2, x1), dim=x1.ndim - 1)\n",
        "\n",
        "def apply_rotary_pos_emb(q, k, cos, sin):\n",
        "    return (q * cos) + (rotate_half(q) * sin), (k * cos) + (rotate_half(k) * sin)\n",
        "\n",
        "class MultiHeadAttentionWithRoPE(nn.Module):\n",
        "    \"\"\"Custom Multi-Head Attention with RoPE support\"\"\"\n",
        "    def __init__(self, d_model, nhead, dropout=0.1, batch_first=True):\n",
        "        super().__init__()\n",
        "        assert d_model % nhead == 0\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.nhead = nhead\n",
        "        self.d_k = d_model // nhead\n",
        "        self.batch_first = batch_first  # Add this attribute\n",
        "\n",
        "        self.w_q = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.w_k = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.w_v = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.w_o = nn.Linear(d_model, d_model)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.rotary_emb = RotaryEmbedding(self.d_k)\n",
        "\n",
        "    def forward(self, query, key, value, key_padding_mask=None, attn_mask=None, need_weights=False, is_causal=False):\n",
        "        batch_size, seq_len, _ = query.size()\n",
        "\n",
        "        # Linear transformations\n",
        "        Q = self.w_q(query)  # [batch_size, seq_len, d_model]\n",
        "        K = self.w_k(key)    # [batch_size, seq_len, d_model]\n",
        "        V = self.w_v(value)  # [batch_size, seq_len, d_model]\n",
        "\n",
        "        # Reshape for multi-head attention\n",
        "        Q = Q.view(batch_size, seq_len, self.nhead, self.d_k)  # [batch, seq, heads, d_k]\n",
        "        K = K.view(batch_size, seq_len, self.nhead, self.d_k)\n",
        "        V = V.view(batch_size, seq_len, self.nhead, self.d_k)\n",
        "\n",
        "        # Apply RoPE to Q and K\n",
        "        cos, sin = self.rotary_emb(query)\n",
        "        Q, K = apply_rotary_pos_emb(Q, K, cos, sin)\n",
        "\n",
        "        # Transpose for attention computation: [batch, heads, seq, d_k]\n",
        "        Q = Q.transpose(1, 2)\n",
        "        K = K.transpose(1, 2)\n",
        "        V = V.transpose(1, 2)\n",
        "\n",
        "        # Scaled dot-product attention\n",
        "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
        "\n",
        "        # Apply masks if provided\n",
        "        if attn_mask is not None:\n",
        "            scores = scores.masked_fill(attn_mask == 0, -1e9)\n",
        "\n",
        "        if key_padding_mask is not None:\n",
        "            # key_padding_mask: [batch_size, seq_len], True for padding positions\n",
        "            key_padding_mask = key_padding_mask.unsqueeze(1).unsqueeze(2)  # [batch, 1, 1, seq]\n",
        "            scores = scores.masked_fill(key_padding_mask, -1e9)\n",
        "\n",
        "        attention_weights = torch.softmax(scores, dim=-1)\n",
        "        attention_weights = self.dropout(attention_weights)\n",
        "\n",
        "        # Apply attention to values\n",
        "        attention_output = torch.matmul(attention_weights, V)  # [batch, heads, seq, d_k]\n",
        "\n",
        "        # Concatenate heads\n",
        "        attention_output = attention_output.transpose(1, 2).contiguous().view(\n",
        "            batch_size, seq_len, self.d_model\n",
        "        )\n",
        "\n",
        "        # Final linear transformation\n",
        "        output = self.w_o(attention_output)\n",
        "\n",
        "        if need_weights:\n",
        "            return output, attention_weights.mean(dim=1)  # Average over heads for compatibility\n",
        "        return output\n",
        "\n",
        "class TransformerEncoderLayerWithRoPE(nn.Module):\n",
        "    \"\"\"\n",
        "    A custom Transformer Encoder Layer that incorporates RoPE.\n",
        "    \"\"\"\n",
        "    def __init__(self, d_model, nhead, dim_feedforward, dropout, batch_first=True):\n",
        "        super().__init__()\n",
        "        self.self_attn = MultiHeadAttentionWithRoPE(d_model, nhead, dropout, batch_first)\n",
        "        self.linear1 = nn.Linear(d_model, dim_feedforward)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.linear2 = nn.Linear(dim_feedforward, d_model)\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "        self.activation = nn.ReLU(inplace=True)\n",
        "        self.batch_first = batch_first  # Add this attribute\n",
        "\n",
        "    def forward(self, src, src_mask=None, src_key_padding_mask=None, is_causal=False):\n",
        "        # Self-attention with RoPE\n",
        "        src2 = self.self_attn(src, src, src, key_padding_mask=src_key_padding_mask, attn_mask=src_mask)\n",
        "        src = src + self.dropout1(src2)\n",
        "        src = self.norm1(src)\n",
        "\n",
        "        # Feed Forward\n",
        "        src2 = self.linear2(self.dropout(self.activation(self.linear1(src))))\n",
        "        src = src + self.dropout2(src2)\n",
        "        src = self.norm2(src)\n",
        "        return src\n",
        "\n",
        "class NonResidualBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, stride=1):\n",
        "        super(NonResidualBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        out = self.relu(out)\n",
        "        return out\n",
        "\n",
        "class ResNetTransformer(nn.Module):\n",
        "    def __init__(self, block, layers, num_classes, t_config):\n",
        "        super(ResNetTransformer, self).__init__()\n",
        "        self.in_channels = 64\n",
        "        self.embedding_dim = t_config[\"embedding_dim\"]\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "        self.layer1 = self._make_layer(block, 64, layers[0], stride=1)\n",
        "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n",
        "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n",
        "\n",
        "        self.projections = nn.ModuleList([\n",
        "            self._create_projection(64, self.embedding_dim),\n",
        "            self._create_projection(64, self.embedding_dim),\n",
        "            self._create_projection(128, self.embedding_dim),\n",
        "            self._create_projection(256, self.embedding_dim),\n",
        "            self._create_projection(512, self.embedding_dim)\n",
        "        ])\n",
        "\n",
        "        # Create transformer encoder layers manually\n",
        "        self.transformer_layers = nn.ModuleList([\n",
        "            TransformerEncoderLayerWithRoPE(\n",
        "                d_model=self.embedding_dim,\n",
        "                nhead=t_config[\"nhead\"],\n",
        "                dim_feedforward=t_config[\"dim_feedforward\"],\n",
        "                dropout=t_config[\"dropout\"],\n",
        "                batch_first=True\n",
        "            ) for _ in range(t_config[\"num_encoder_layers\"])\n",
        "        ])\n",
        "\n",
        "        self.classifier = nn.Linear(self.embedding_dim, num_classes)\n",
        "\n",
        "    def _create_projection(self, in_features, out_features):\n",
        "        return nn.Sequential(\n",
        "            nn.AdaptiveAvgPool2d((1, 1)),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(in_features, out_features)\n",
        "        )\n",
        "\n",
        "    def _make_layer(self, block, out_channels, num_blocks, stride):\n",
        "        strides = [stride] + [1] * (num_blocks - 1)\n",
        "        layers = []\n",
        "        current_in_channels = self.in_channels\n",
        "        for s in strides:\n",
        "            layers.append(block(current_in_channels, out_channels, s))\n",
        "            current_in_channels = out_channels\n",
        "        self.in_channels = out_channels\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        features = []\n",
        "        x = self.maxpool(self.relu(self.bn1(self.conv1(x))))\n",
        "        features.append(x)\n",
        "        x = self.layer1(x)\n",
        "        features.append(x)\n",
        "        x = self.layer2(x)\n",
        "        features.append(x)\n",
        "        x = self.layer3(x)\n",
        "        features.append(x)\n",
        "        x = self.layer4(x)\n",
        "        features.append(x)\n",
        "\n",
        "        tokens = [self.projections[i](feat) for i, feat in enumerate(features)]\n",
        "        token_sequence = torch.stack(tokens, dim=1)\n",
        "\n",
        "        # Apply transformer layers manually\n",
        "        for layer in self.transformer_layers:\n",
        "            token_sequence = layer(token_sequence)\n",
        "\n",
        "        aggregated_vector = token_sequence.mean(dim=1)\n",
        "        logits = self.classifier(aggregated_vector)\n",
        "        return logits\n",
        "\n",
        "def evaluate_model(model, data_loader, criterion, device):\n",
        "    \"\"\"Evaluates the model on a given dataset.\"\"\"\n",
        "    model.eval()\n",
        "    total_loss, correct, total = 0.0, 0, 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in data_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            total_loss += loss.item() * images.size(0)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    avg_loss = total_loss / total\n",
        "    accuracy = 100 * correct / total\n",
        "    return avg_loss, accuracy\n",
        "\n",
        "def train_and_validate(model, train_loader, val_loader, config):\n",
        "    \"\"\"Main training loop with callbacks.\"\"\"\n",
        "    device = config[\"device\"]\n",
        "    model.to(device)\n",
        "\n",
        "    # Set seeds for reproducible training\n",
        "    torch.manual_seed(config[\"split_seed\"])\n",
        "    np.random.seed(config[\"split_seed\"])\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(config[\"split_seed\"])\n",
        "        torch.cuda.manual_seed_all(config[\"split_seed\"])\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=config[\"learning_rate\"], weight_decay=config[\"weight_decay\"])\n",
        "\n",
        "    # Initialize callbacks\n",
        "    callbacks = [\n",
        "        ReduceLROnPlateau(\n",
        "            optimizer=optimizer,\n",
        "            monitor='val_accuracy',\n",
        "            factor=0.2,\n",
        "            patience=3,\n",
        "            min_lr=1e-7,\n",
        "            verbose=1\n",
        "        ),\n",
        "        EarlyStopping(\n",
        "            monitor='val_accuracy',\n",
        "            patience=7,\n",
        "            restore_best_weights=True,\n",
        "            verbose=1\n",
        "        ),\n",
        "        ModelCheckpoint(\n",
        "            filepath=config[\"checkpoint_path\"],\n",
        "            save_best_only=True,\n",
        "            monitor='val_accuracy',\n",
        "            verbose=1\n",
        "        )\n",
        "    ]\n",
        "\n",
        "    print(\"\\n--- Starting Training ---\")\n",
        "    early_stop = False\n",
        "\n",
        "    for epoch in range(config[\"num_epochs\"]):\n",
        "        if early_stop:\n",
        "            break\n",
        "\n",
        "        model.train()\n",
        "        start_time = time.time()\n",
        "        running_loss, train_correct, train_total = 0.0, 0, 0\n",
        "\n",
        "        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{config['num_epochs']} [Train]\", leave=False)\n",
        "        for images, labels in progress_bar:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item() * images.size(0)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            train_total += labels.size(0)\n",
        "            train_correct += (predicted == labels).sum().item()\n",
        "            progress_bar.set_postfix(loss=loss.item())\n",
        "\n",
        "        train_loss = running_loss / train_total\n",
        "        train_acc = 100 * train_correct / train_total\n",
        "        val_loss, val_acc = evaluate_model(model, val_loader, criterion, device)\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{config['num_epochs']} | Time: {time.time() - start_time:.2f}s | \"\n",
        "              f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}% | \"\n",
        "              f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%\")\n",
        "\n",
        "        # Prepare logs for callbacks\n",
        "        logs = {\n",
        "            'train_loss': train_loss,\n",
        "            'train_accuracy': train_acc,\n",
        "            'val_loss': val_loss,\n",
        "            'val_accuracy': val_acc\n",
        "        }\n",
        "\n",
        "        # Execute callbacks\n",
        "        for callback in callbacks:\n",
        "            if isinstance(callback, EarlyStopping):\n",
        "                if callback.on_epoch_end(epoch, logs, model):\n",
        "                    early_stop = True\n",
        "                    break\n",
        "            elif isinstance(callback, ModelCheckpoint):\n",
        "                callback.on_epoch_end(epoch, logs, model)\n",
        "            else:\n",
        "                callback.on_epoch_end(epoch, logs)\n",
        "\n",
        "    # Execute callback cleanup\n",
        "    for callback in callbacks:\n",
        "        if isinstance(callback, EarlyStopping):\n",
        "            callback.on_training_end(model)\n",
        "        else:\n",
        "            callback.on_training_end()\n",
        "\n",
        "    print(\"--- Training Finished ---\\n\")\n",
        "    return model\n",
        "\n",
        "# ==========================================\n",
        "# Evaluation\n",
        "# ==========================================\n",
        "\n",
        "def calculate_top_k_accuracy(outputs, labels, k_values=[1, 3, 5]):\n",
        "    \"\"\"Calculate top-k accuracy for given k values.\"\"\"\n",
        "    batch_size = labels.size(0)\n",
        "    _, pred = outputs.topk(max(k_values), 1, True, True)\n",
        "    pred = pred.t()\n",
        "    correct = pred.eq(labels.view(1, -1).expand_as(pred))\n",
        "\n",
        "    top_k_accuracies = {}\n",
        "    for k in k_values:\n",
        "        correct_k = correct[:k].reshape(-1).float().sum(0, keepdim=True)\n",
        "        top_k_accuracies[k] = correct_k.item() / batch_size\n",
        "\n",
        "    return top_k_accuracies\n",
        "\n",
        "def calculate_entropy(probs):\n",
        "    \"\"\"Calculate entropy of probability distributions.\"\"\"\n",
        "    # Add small epsilon to avoid log(0)\n",
        "    epsilon = 1e-8\n",
        "    probs = probs + epsilon\n",
        "    entropy = -torch.sum(probs * torch.log(probs), dim=1)\n",
        "    return entropy\n",
        "\n",
        "def calculate_ece(confidences, accuracies, n_bins=10):\n",
        "    \"\"\"Calculate Expected Calibration Error.\"\"\"\n",
        "    bin_boundaries = torch.linspace(0, 1, n_bins + 1)\n",
        "    bin_lowers = bin_boundaries[:-1]\n",
        "    bin_uppers = bin_boundaries[1:]\n",
        "\n",
        "    ece = 0\n",
        "    for bin_lower, bin_upper in zip(bin_lowers, bin_uppers):\n",
        "        in_bin = (confidences > bin_lower.item()) & (confidences <= bin_upper.item())\n",
        "        prop_in_bin = in_bin.float().mean()\n",
        "\n",
        "        if prop_in_bin.item() > 0:\n",
        "            accuracy_in_bin = accuracies[in_bin].float().mean()\n",
        "            avg_confidence_in_bin = confidences[in_bin].mean()\n",
        "            ece += torch.abs(avg_confidence_in_bin - accuracy_in_bin) * prop_in_bin\n",
        "\n",
        "    return ece.item()\n",
        "\n",
        "def comprehensive_evaluation(model, test_loader, criterion, device):\n",
        "    \"\"\"Comprehensive evaluation with all requested metrics.\"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    all_predictions = []\n",
        "    all_labels = []\n",
        "    all_confidences = []\n",
        "    all_entropies = []\n",
        "    all_top_k_results = {1: [], 3: [], 5: []}\n",
        "    total_loss = 0.0\n",
        "    total_samples = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in tqdm(test_loader, desc=\"Evaluating\"):\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            # Convert outputs to probabilities\n",
        "            probs = torch.softmax(outputs, dim=1)\n",
        "\n",
        "            # Get predictions and confidences\n",
        "            confidences, predictions = torch.max(probs, dim=1)\n",
        "\n",
        "            # Calculate entropy\n",
        "            entropies = calculate_entropy(probs)\n",
        "\n",
        "            # Calculate top-k accuracies\n",
        "            top_k_accs = calculate_top_k_accuracy(outputs, labels, [1, 3, 5])\n",
        "\n",
        "            # Store results\n",
        "            all_predictions.extend(predictions.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "            all_confidences.extend(confidences.cpu().numpy())\n",
        "            all_entropies.extend(entropies.cpu().numpy())\n",
        "\n",
        "            for k in [1, 3, 5]:\n",
        "                all_top_k_results[k].extend([top_k_accs[k]] * labels.size(0))\n",
        "\n",
        "            total_loss += loss.item() * images.size(0)\n",
        "            total_samples += images.size(0)\n",
        "\n",
        "    # Convert to numpy arrays\n",
        "    all_predictions = np.array(all_predictions)\n",
        "    all_labels = np.array(all_labels)\n",
        "    all_confidences = np.array(all_confidences)\n",
        "    all_entropies = np.array(all_entropies)\n",
        "\n",
        "    # Calculate basic metrics\n",
        "    avg_loss = total_loss / total_samples\n",
        "\n",
        "    # Calculate top-k accuracies\n",
        "    top1 = np.mean([pred == label for pred, label in zip(all_predictions, all_labels)])\n",
        "    top3 = np.mean(all_top_k_results[3])\n",
        "    top5 = np.mean(all_top_k_results[5])\n",
        "\n",
        "    # Calculate precision, recall, f1-score\n",
        "    precision_macro, recall_macro, f1_macro, _ = precision_recall_fscore_support(\n",
        "        all_labels, all_predictions, average='macro', zero_division=0\n",
        "    )\n",
        "    precision_weighted, recall_weighted, f1_weighted, _ = precision_recall_fscore_support(\n",
        "        all_labels, all_predictions, average='weighted', zero_division=0\n",
        "    )\n",
        "\n",
        "    # Calculate confidence and calibration metrics\n",
        "    avg_confidence = np.mean(all_confidences)\n",
        "    std_confidence = np.std(all_confidences)\n",
        "    avg_entropy = np.mean(all_entropies)\n",
        "\n",
        "    # Calculate ECE\n",
        "    accuracies = (all_predictions == all_labels).astype(float)\n",
        "    ece = calculate_ece(torch.tensor(all_confidences), torch.tensor(accuracies))\n",
        "\n",
        "    return {\n",
        "        'test_loss': avg_loss,\n",
        "        'top1': top1,\n",
        "        'top3': top3,\n",
        "        'top5': top5,\n",
        "        'precision_macro': precision_macro,\n",
        "        'recall_macro': recall_macro,\n",
        "        'f1_macro': f1_macro,\n",
        "        'precision_weighted': precision_weighted,\n",
        "        'recall_weighted': recall_weighted,\n",
        "        'f1_weighted': f1_weighted,\n",
        "        'avg_confidence': avg_confidence,\n",
        "        'std_confidence': std_confidence,\n",
        "        'avg_entropy': avg_entropy,\n",
        "        'ece': ece\n",
        "    }\n",
        "\n",
        "# ==========================================\n",
        "# Main Execution\n",
        "# ==========================================\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # Set global seeds for full reproducibility\n",
        "    torch.manual_seed(CONFIG[\"split_seed\"])\n",
        "    np.random.seed(CONFIG[\"split_seed\"])\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(CONFIG[\"split_seed\"])\n",
        "        torch.cuda.manual_seed_all(CONFIG[\"split_seed\"])\n",
        "        # Ensure deterministic behavior on CUDA\n",
        "        torch.backends.cudnn.deterministic = True\n",
        "        torch.backends.cudnn.benchmark = False\n",
        "\n",
        "    print(f\"Using device: {CONFIG['device']}\")\n",
        "    print(f\"Reproducibility seed: {CONFIG['split_seed']}\")\n",
        "\n",
        "    # Get data loaders\n",
        "    dataloaders = get_dataloaders(CONFIG)\n",
        "    if dataloaders is None:\n",
        "        print(\"Could not prepare data. Halting execution.\", file=sys.stderr)\n",
        "        sys.exit(1)  # Exit if data preparation failed\n",
        "\n",
        "    train_loader, val_loader, test_loader = dataloaders\n",
        "\n",
        "    # Initialize and train model\n",
        "    model = ResNetTransformer(\n",
        "        block=NonResidualBlock,\n",
        "        layers=[2, 2, 2, 2],\n",
        "        num_classes=CONFIG[\"num_classes\"],\n",
        "        t_config=TRANSFORMER_CONFIG\n",
        "    )\n",
        "    trained_model = train_and_validate(model, train_loader, val_loader, CONFIG)\n",
        "\n",
        "    # Comprehensive evaluation on the test set\n",
        "    print(\"--- Starting Comprehensive Evaluation on Test Set ---\")\n",
        "    results = comprehensive_evaluation(trained_model, test_loader, nn.CrossEntropyLoss(), CONFIG[\"device\"])\n",
        "\n",
        "    # Print all requested metrics\n",
        "    print(\"\\n--- Top-K Accuracy Results ---\")\n",
        "    print(f\"Top-1 Accuracy: {results['top1'] * 100:.2f}%\")\n",
        "    print(f\"Top-3 Accuracy: {results['top3'] * 100:.2f}%\")\n",
        "    print(f\"Top-5 Accuracy: {results['top5'] * 100:.2f}%\")\n",
        "\n",
        "    print(\"\\n--- Additional Performance Metrics ---\")\n",
        "    print(f\"Macro Average Precision: {results['precision_macro']:.4f}\")\n",
        "    print(f\"Macro Average Recall: {results['recall_macro']:.4f}\")\n",
        "    print(f\"Macro Average F1-Score: {results['f1_macro']:.4f}\")\n",
        "    print(f\"Weighted Average Precision: {results['precision_weighted']:.4f}\")\n",
        "    print(f\"Weighted Average Recall: {results['recall_weighted']:.4f}\")\n",
        "    print(f\"Weighted Average F1-Score: {results['f1_weighted']:.4f}\")\n",
        "\n",
        "    print(\"\\n--- Model Confidence & Calibration Metrics ---\")\n",
        "    print(f\"Average Prediction Confidence: {results['avg_confidence']:.4f}\")\n",
        "    print(f\"Confidence Standard Deviation: {results['std_confidence']:.4f}\")\n",
        "    print(f\"Average Prediction Entropy: {results['avg_entropy']:.4f}\")\n",
        "    print(f\"Expected Calibration Error: {results['ece']:.4f}\")\n",
        "\n",
        "    print(f\"\\nFinal Test Loss: {results['test_loss']:.4f}\")\n",
        "    print(\"===============================================\")"
      ],
      "metadata": {
        "id": "mUvJ4ukOOTQb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e252dad-e16d-4a21-a9dc-5ebd61e209f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Reproducibility seed: 42\n",
            "Dataset: tissuemnist\n",
            "Task: multi-class\n",
            "Number of channels: 1\n",
            "Number of classes: 8\n",
            "Loading tissuemnist dataset...\n",
            "Dataset splits - Train: 165466, Val: 23640, Test: 47280\n",
            "\n",
            "--- Starting Training ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100 | Time: 154.24s | Train Loss: 1.2416, Train Acc: 52.98% | Val Loss: 1.1574, Val Acc: 55.54%\n",
            "\n",
            "Model saved to ./best_model.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/100 | Time: 152.81s | Train Loss: 1.0925, Train Acc: 59.43% | Val Loss: 1.3759, Val Acc: 52.12%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/100 | Time: 154.10s | Train Loss: 1.0295, Train Acc: 61.90% | Val Loss: 1.0860, Val Acc: 60.44%\n",
            "\n",
            "Model saved to ./best_model.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4/100 | Time: 152.31s | Train Loss: 0.9991, Train Acc: 63.14% | Val Loss: 0.9812, Val Acc: 64.41%\n",
            "\n",
            "Model saved to ./best_model.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5/100 | Time: 156.21s | Train Loss: 0.9770, Train Acc: 64.00% | Val Loss: 1.0835, Val Acc: 59.98%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6/100 | Time: 156.84s | Train Loss: 0.9592, Train Acc: 64.78% | Val Loss: 1.0048, Val Acc: 63.79%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7/100 | Time: 156.23s | Train Loss: 0.9482, Train Acc: 65.24% | Val Loss: 1.0917, Val Acc: 58.94%\n",
            "\n",
            "Reducing learning rate from 1.00e-03 to 2.00e-04\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8/100 | Time: 154.11s | Train Loss: 0.8822, Train Acc: 67.69% | Val Loss: 0.9096, Val Acc: 66.54%\n",
            "\n",
            "Model saved to ./best_model.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9/100 | Time: 154.87s | Train Loss: 0.8657, Train Acc: 68.46% | Val Loss: 0.8764, Val Acc: 67.80%\n",
            "\n",
            "Model saved to ./best_model.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10/100 | Time: 153.81s | Train Loss: 0.8555, Train Acc: 68.77% | Val Loss: 0.8548, Val Acc: 69.15%\n",
            "\n",
            "Model saved to ./best_model.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 11/100 | Time: 151.22s | Train Loss: 0.8473, Train Acc: 69.04% | Val Loss: 0.8659, Val Acc: 68.43%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 12/100 | Time: 155.12s | Train Loss: 0.8426, Train Acc: 69.30% | Val Loss: 0.8672, Val Acc: 68.36%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 13/100 | Time: 153.18s | Train Loss: 0.8361, Train Acc: 69.51% | Val Loss: 0.8690, Val Acc: 68.22%\n",
            "\n",
            "Reducing learning rate from 2.00e-04 to 4.00e-05\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 14/100 | Time: 153.01s | Train Loss: 0.8116, Train Acc: 70.60% | Val Loss: 0.8444, Val Acc: 69.24%\n",
            "\n",
            "Model saved to ./best_model.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 15/100 | Time: 152.55s | Train Loss: 0.8073, Train Acc: 70.71% | Val Loss: 0.8392, Val Acc: 69.42%\n",
            "\n",
            "Model saved to ./best_model.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 16/100 | Time: 154.65s | Train Loss: 0.8019, Train Acc: 70.95% | Val Loss: 0.8319, Val Acc: 69.89%\n",
            "\n",
            "Model saved to ./best_model.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 17/100 | Time: 151.36s | Train Loss: 0.7987, Train Acc: 71.01% | Val Loss: 0.8382, Val Acc: 69.60%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 18/100 | Time: 151.49s | Train Loss: 0.7973, Train Acc: 70.96% | Val Loss: 0.8326, Val Acc: 69.70%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 19/100 | Time: 150.75s | Train Loss: 0.7958, Train Acc: 71.11% | Val Loss: 0.8337, Val Acc: 69.78%\n",
            "\n",
            "Reducing learning rate from 4.00e-05 to 8.00e-06\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 20/100 | Time: 149.99s | Train Loss: 0.7870, Train Acc: 71.56% | Val Loss: 0.8329, Val Acc: 69.79%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 21/100 | Time: 151.74s | Train Loss: 0.7870, Train Acc: 71.46% | Val Loss: 0.8302, Val Acc: 69.90%\n",
            "\n",
            "Model saved to ./best_model.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 22/100 | Time: 151.16s | Train Loss: 0.7848, Train Acc: 71.59% | Val Loss: 0.8278, Val Acc: 69.99%\n",
            "\n",
            "Model saved to ./best_model.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 23/100 | Time: 151.05s | Train Loss: 0.7855, Train Acc: 71.45% | Val Loss: 0.8282, Val Acc: 69.95%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 24/100 | Time: 149.66s | Train Loss: 0.7841, Train Acc: 71.60% | Val Loss: 0.8291, Val Acc: 70.02%\n",
            "\n",
            "Model saved to ./best_model.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 25/100 | Time: 151.90s | Train Loss: 0.7835, Train Acc: 71.65% | Val Loss: 0.8290, Val Acc: 69.92%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 26/100 | Time: 150.96s | Train Loss: 0.7823, Train Acc: 71.61% | Val Loss: 0.8301, Val Acc: 69.97%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 27/100 | Time: 152.74s | Train Loss: 0.7812, Train Acc: 71.68% | Val Loss: 0.8283, Val Acc: 69.99%\n",
            "\n",
            "Reducing learning rate from 8.00e-06 to 1.60e-06\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 28/100 | Time: 151.40s | Train Loss: 0.7798, Train Acc: 71.68% | Val Loss: 0.8305, Val Acc: 69.95%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 29/100 | Time: 147.63s | Train Loss: 0.7821, Train Acc: 71.56% | Val Loss: 0.8279, Val Acc: 69.99%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 30/100 | Time: 148.73s | Train Loss: 0.7807, Train Acc: 71.62% | Val Loss: 0.8302, Val Acc: 69.86%\n",
            "\n",
            "Reducing learning rate from 1.60e-06 to 3.20e-07\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 31/100 | Time: 147.17s | Train Loss: 0.7804, Train Acc: 71.71% | Val Loss: 0.8336, Val Acc: 69.76%\n",
            "\n",
            "Early stopping at epoch 31\n",
            "Restored model weights from the end of the best epoch: 24\n",
            "--- Training Finished ---\n",
            "\n",
            "--- Starting Comprehensive Evaluation on Test Set ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating: 100%|██████████| 370/370 [00:17<00:00, 20.76it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Top-K Accuracy Results ---\n",
            "Top-1 Accuracy: 69.72%\n",
            "Top-3 Accuracy: 93.98%\n",
            "Top-5 Accuracy: 98.63%\n",
            "\n",
            "--- Additional Performance Metrics ---\n",
            "Macro Average Precision: 0.6324\n",
            "Macro Average Recall: 0.6095\n",
            "Macro Average F1-Score: 0.6157\n",
            "Weighted Average Precision: 0.6945\n",
            "Weighted Average Recall: 0.6972\n",
            "Weighted Average F1-Score: 0.6924\n",
            "\n",
            "--- Model Confidence & Calibration Metrics ---\n",
            "Average Prediction Confidence: 0.7298\n",
            "Confidence Standard Deviation: 0.2087\n",
            "Average Prediction Entropy: 0.7488\n",
            "Expected Calibration Error: 0.0326\n",
            "\n",
            "Final Test Loss: 0.8342\n",
            "===============================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FAevt-InaHTw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}