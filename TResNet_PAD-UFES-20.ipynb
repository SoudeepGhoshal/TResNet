{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SoudeepGhoshal/TResNet/blob/main/TResNet_PAD-UFES-20.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PAD-UFES-20"
      ],
      "metadata": {
        "id": "QsIwmzhFp62d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ResNet-18"
      ],
      "metadata": {
        "id": "4lh3eD0Vp8Oj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lTD9SzyhmvoA",
        "outputId": "49cf3f5a-23b9-4cd6-b974-7342e45f90d9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Reproducibility seed: 42\n",
            "Downloading PAD-UFES-20 dataset from Kaggle...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "PAD-UFES-20: 100%|██████████| 3.60G/3.60G [00:44<00:00, 80.7MiB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting...\n",
            "Loading metadata from: ./pad-ufes-20/metadata.csv\n",
            "Total samples: 2298\n",
            "Diagnostic classes: diagnostic\n",
            "BCC    845\n",
            "ACK    730\n",
            "NEV    244\n",
            "SEK    235\n",
            "SCC    192\n",
            "MEL     52\n",
            "Name: count, dtype: int64\n",
            "✅ Dataset verification complete\n",
            "Found 6 classes: ['ACK', 'BCC', 'MEL', 'NEV', 'SCC', 'SEK']\n",
            "Dataset splits - Train: 1608, Val: 344, Test: 346, Total: 2298\n",
            "Found 6 classes: ['ACK', 'BCC', 'MEL', 'NEV', 'SCC', 'SEK']\n",
            "Found 6 classes: ['ACK', 'BCC', 'MEL', 'NEV', 'SCC', 'SEK']\n",
            "Found 6 classes: ['ACK', 'BCC', 'MEL', 'NEV', 'SCC', 'SEK']\n",
            "\n",
            "--- Starting Training ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100 | Time: 78.61s | Train Loss: 1.7392, Train Acc: 32.77% | Val Loss: 2.4749, Val Acc: 35.47%\n",
            "\n",
            "Model saved to ./best_model_pad_ufes.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/100 | Time: 77.79s | Train Loss: 1.5052, Train Acc: 37.81% | Val Loss: 1.4946, Val Acc: 35.47%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/100 | Time: 78.41s | Train Loss: 1.5331, Train Acc: 34.95% | Val Loss: 1.5019, Val Acc: 37.21%\n",
            "\n",
            "Model saved to ./best_model_pad_ufes.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4/100 | Time: 76.71s | Train Loss: 1.4889, Train Acc: 37.06% | Val Loss: 1.5013, Val Acc: 40.99%\n",
            "\n",
            "Model saved to ./best_model_pad_ufes.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5/100 | Time: 77.71s | Train Loss: 1.4685, Train Acc: 38.81% | Val Loss: 1.4709, Val Acc: 37.79%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6/100 | Time: 76.86s | Train Loss: 1.4701, Train Acc: 38.56% | Val Loss: 1.5206, Val Acc: 41.28%\n",
            "\n",
            "Model saved to ./best_model_pad_ufes.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7/100 | Time: 77.19s | Train Loss: 1.4269, Train Acc: 39.80% | Val Loss: 1.4787, Val Acc: 38.37%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8/100 | Time: 77.21s | Train Loss: 1.4134, Train Acc: 41.11% | Val Loss: 1.4918, Val Acc: 39.24%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9/100 | Time: 75.88s | Train Loss: 1.4073, Train Acc: 40.92% | Val Loss: 1.4598, Val Acc: 42.44%\n",
            "\n",
            "Model saved to ./best_model_pad_ufes.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10/100 | Time: 76.66s | Train Loss: 1.3778, Train Acc: 42.60% | Val Loss: 1.3838, Val Acc: 40.41%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 11/100 | Time: 76.41s | Train Loss: 1.3562, Train Acc: 43.16% | Val Loss: 1.3721, Val Acc: 44.48%\n",
            "\n",
            "Model saved to ./best_model_pad_ufes.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 12/100 | Time: 75.95s | Train Loss: 1.3746, Train Acc: 42.35% | Val Loss: 1.3887, Val Acc: 44.19%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 13/100 | Time: 76.50s | Train Loss: 1.3521, Train Acc: 41.98% | Val Loss: 1.5303, Val Acc: 38.66%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 14/100 | Time: 76.29s | Train Loss: 1.3398, Train Acc: 43.28% | Val Loss: 1.3895, Val Acc: 41.86%\n",
            "\n",
            "Reducing learning rate from 1.00e-03 to 2.00e-04\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 15/100 | Time: 76.41s | Train Loss: 1.2834, Train Acc: 48.94% | Val Loss: 1.2824, Val Acc: 48.84%\n",
            "\n",
            "Model saved to ./best_model_pad_ufes.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 16/100 | Time: 77.15s | Train Loss: 1.2588, Train Acc: 48.32% | Val Loss: 1.2722, Val Acc: 51.45%\n",
            "\n",
            "Model saved to ./best_model_pad_ufes.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 17/100 | Time: 77.08s | Train Loss: 1.2217, Train Acc: 49.50% | Val Loss: 1.2180, Val Acc: 53.20%\n",
            "\n",
            "Model saved to ./best_model_pad_ufes.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 18/100 | Time: 76.85s | Train Loss: 1.2270, Train Acc: 50.87% | Val Loss: 1.2189, Val Acc: 54.94%\n",
            "\n",
            "Model saved to ./best_model_pad_ufes.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 19/100 | Time: 76.84s | Train Loss: 1.2208, Train Acc: 50.19% | Val Loss: 1.2153, Val Acc: 52.33%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 20/100 | Time: 77.12s | Train Loss: 1.2163, Train Acc: 49.69% | Val Loss: 1.2395, Val Acc: 51.74%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 21/100 | Time: 77.38s | Train Loss: 1.2013, Train Acc: 51.74% | Val Loss: 1.2139, Val Acc: 53.49%\n",
            "\n",
            "Reducing learning rate from 2.00e-04 to 4.00e-05\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 22/100 | Time: 76.97s | Train Loss: 1.1879, Train Acc: 53.11% | Val Loss: 1.1553, Val Acc: 56.40%\n",
            "\n",
            "Model saved to ./best_model_pad_ufes.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 23/100 | Time: 76.44s | Train Loss: 1.1599, Train Acc: 54.04% | Val Loss: 1.1562, Val Acc: 56.98%\n",
            "\n",
            "Model saved to ./best_model_pad_ufes.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 24/100 | Time: 77.15s | Train Loss: 1.1596, Train Acc: 54.04% | Val Loss: 1.1472, Val Acc: 54.65%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 25/100 | Time: 77.47s | Train Loss: 1.1661, Train Acc: 53.54% | Val Loss: 1.1380, Val Acc: 56.10%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 26/100 | Time: 76.53s | Train Loss: 1.1705, Train Acc: 52.18% | Val Loss: 1.1537, Val Acc: 56.98%\n",
            "\n",
            "Reducing learning rate from 4.00e-05 to 8.00e-06\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 27/100 | Time: 77.98s | Train Loss: 1.1396, Train Acc: 54.10% | Val Loss: 1.1452, Val Acc: 57.56%\n",
            "\n",
            "Model saved to ./best_model_pad_ufes.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 28/100 | Time: 76.96s | Train Loss: 1.1350, Train Acc: 54.91% | Val Loss: 1.1503, Val Acc: 56.98%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 29/100 | Time: 76.75s | Train Loss: 1.1458, Train Acc: 54.42% | Val Loss: 1.1473, Val Acc: 55.81%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 30/100 | Time: 76.25s | Train Loss: 1.1393, Train Acc: 54.29% | Val Loss: 1.1449, Val Acc: 56.10%\n",
            "\n",
            "Reducing learning rate from 8.00e-06 to 1.60e-06\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 31/100 | Time: 75.88s | Train Loss: 1.1460, Train Acc: 54.42% | Val Loss: 1.1409, Val Acc: 57.27%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 32/100 | Time: 75.63s | Train Loss: 1.1388, Train Acc: 54.91% | Val Loss: 1.1435, Val Acc: 56.40%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 33/100 | Time: 74.95s | Train Loss: 1.1477, Train Acc: 54.73% | Val Loss: 1.1453, Val Acc: 56.10%\n",
            "\n",
            "Reducing learning rate from 1.60e-06 to 3.20e-07\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 34/100 | Time: 75.64s | Train Loss: 1.1326, Train Acc: 54.29% | Val Loss: 1.1407, Val Acc: 55.81%\n",
            "\n",
            "Early stopping at epoch 34\n",
            "Restored model weights from the end of the best epoch: 27\n",
            "--- Training Finished ---\n",
            "\n",
            "--- Starting Comprehensive Evaluation on Test Set ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating: 100%|██████████| 6/6 [00:12<00:00,  2.09s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Top-K Accuracy Results ---\n",
            "Top-1 Accuracy: 55.49%\n",
            "Top-3 Accuracy: 89.60%\n",
            "Top-5 Accuracy: 99.13%\n",
            "\n",
            "--- Additional Performance Metrics ---\n",
            "Macro Average Precision: 0.4061\n",
            "Macro Average Recall: 0.3559\n",
            "Macro Average F1-Score: 0.3600\n",
            "Weighted Average Precision: 0.4882\n",
            "Weighted Average Recall: 0.5549\n",
            "Weighted Average F1-Score: 0.5086\n",
            "\n",
            "--- Model Confidence & Calibration Metrics ---\n",
            "Average Prediction Confidence: 0.5558\n",
            "Confidence Standard Deviation: 0.1389\n",
            "Average Prediction Entropy: 1.1394\n",
            "Expected Calibration Error: 0.0239\n",
            "\n",
            "Final Test Loss: 1.2002\n",
            "===============================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "import torchvision.transforms as transforms\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "import os\n",
        "import requests\n",
        "import zipfile\n",
        "import shutil\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "import sys\n",
        "import numpy as np\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "\n",
        "# --- Configuration Parameters ---\n",
        "CONFIG = {\n",
        "    \"data_path\": \"./pad-ufes-20/\",\n",
        "    \"dataset_url\": \"https://www.kaggle.com/api/v1/datasets/download/maxjen/pad-ufes-20\",  # Kaggle API URL\n",
        "    \"batch_size\": 64,  # Reduced due to smaller dataset\n",
        "    \"num_epochs\": 100,\n",
        "    \"learning_rate\": 0.001,\n",
        "    \"weight_decay\": 1e-4,\n",
        "    \"split_seed\": 42,\n",
        "    \"train_split\": 0.70,\n",
        "    \"val_split\": 0.15,\n",
        "    \"test_split\": 0.15,\n",
        "    \"num_classes\": 6,  # PAD-UFES-20 has 6 diagnostic classes\n",
        "    \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
        "    \"checkpoint_path\": \"./best_model_pad_ufes.pth\",\n",
        "}\n",
        "\n",
        "# ==========================================\n",
        "# Callback Classes\n",
        "# ==========================================\n",
        "\n",
        "class Callback:\n",
        "    \"\"\"Base callback class\"\"\"\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        pass\n",
        "\n",
        "    def on_training_end(self):\n",
        "        pass\n",
        "\n",
        "class ReduceLROnPlateau(Callback):\n",
        "    \"\"\"Reduce learning rate when a metric has stopped improving\"\"\"\n",
        "    def __init__(self, optimizer, monitor='val_accuracy', factor=0.2, patience=3, min_lr=1e-7, verbose=1):\n",
        "        self.optimizer = optimizer\n",
        "        self.monitor = monitor\n",
        "        self.factor = factor\n",
        "        self.patience = patience\n",
        "        self.min_lr = min_lr\n",
        "        self.verbose = verbose\n",
        "        self.wait = 0\n",
        "        self.best = None\n",
        "        self.mode = 'min' if 'loss' in monitor else 'max'\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        if logs is None:\n",
        "            return\n",
        "\n",
        "        current = logs.get(self.monitor)\n",
        "        if current is None:\n",
        "            return\n",
        "\n",
        "        if self.best is None:\n",
        "            self.best = current\n",
        "            return\n",
        "\n",
        "        if self.mode == 'min':\n",
        "            improved = current < self.best\n",
        "        else:\n",
        "            improved = current > self.best\n",
        "\n",
        "        if improved:\n",
        "            self.best = current\n",
        "            self.wait = 0\n",
        "        else:\n",
        "            self.wait += 1\n",
        "            if self.wait >= self.patience:\n",
        "                old_lr = self.optimizer.param_groups[0]['lr']\n",
        "                new_lr = max(old_lr * self.factor, self.min_lr)\n",
        "                if new_lr != old_lr:\n",
        "                    self.optimizer.param_groups[0]['lr'] = new_lr\n",
        "                    if self.verbose:\n",
        "                        print(f\"\\nReducing learning rate from {old_lr:.2e} to {new_lr:.2e}\")\n",
        "                    self.wait = 0\n",
        "\n",
        "class EarlyStopping(Callback):\n",
        "    \"\"\"Stop training when a monitored metric has stopped improving\"\"\"\n",
        "    def __init__(self, monitor='val_accuracy', patience=7, restore_best_weights=True, verbose=1):\n",
        "        self.monitor = monitor\n",
        "        self.patience = patience\n",
        "        self.restore_best_weights = restore_best_weights\n",
        "        self.verbose = verbose\n",
        "        self.wait = 0\n",
        "        self.best = None\n",
        "        self.best_weights = None\n",
        "        self.mode = 'min' if 'loss' in monitor else 'max'\n",
        "        self.stopped_epoch = 0\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None, model=None):\n",
        "        if logs is None:\n",
        "            return False\n",
        "\n",
        "        current = logs.get(self.monitor)\n",
        "        if current is None:\n",
        "            return False\n",
        "\n",
        "        if self.best is None:\n",
        "            self.best = current\n",
        "            if model is not None and self.restore_best_weights:\n",
        "                self.best_weights = model.state_dict().copy()\n",
        "            return False\n",
        "\n",
        "        if self.mode == 'min':\n",
        "            improved = current < self.best\n",
        "        else:\n",
        "            improved = current > self.best\n",
        "\n",
        "        if improved:\n",
        "            self.best = current\n",
        "            self.wait = 0\n",
        "            if model is not None and self.restore_best_weights:\n",
        "                self.best_weights = model.state_dict().copy()\n",
        "        else:\n",
        "            self.wait += 1\n",
        "            if self.wait >= self.patience:\n",
        "                self.stopped_epoch = epoch\n",
        "                if self.verbose:\n",
        "                    print(f\"\\nEarly stopping at epoch {epoch + 1}\")\n",
        "                return True\n",
        "        return False\n",
        "\n",
        "    def on_training_end(self, model=None):\n",
        "        if self.stopped_epoch > 0 and self.verbose:\n",
        "            print(f\"Restored model weights from the end of the best epoch: {self.stopped_epoch + 1 - self.patience}\")\n",
        "        if model is not None and self.restore_best_weights and self.best_weights is not None:\n",
        "            model.load_state_dict(self.best_weights)\n",
        "\n",
        "class ModelCheckpoint(Callback):\n",
        "    \"\"\"Save the model after every epoch\"\"\"\n",
        "    def __init__(self, filepath, save_best_only=True, monitor='val_accuracy', verbose=1):\n",
        "        self.filepath = filepath\n",
        "        self.save_best_only = save_best_only\n",
        "        self.monitor = monitor\n",
        "        self.verbose = verbose\n",
        "        self.best = None\n",
        "        self.mode = 'min' if 'loss' in monitor else 'max'\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None, model=None):\n",
        "        if logs is None or model is None:\n",
        "            return\n",
        "\n",
        "        current = logs.get(self.monitor)\n",
        "        if current is None:\n",
        "            return\n",
        "\n",
        "        if not self.save_best_only:\n",
        "            filepath = self.filepath.replace('.pth', f'_epoch_{epoch + 1}.pth')\n",
        "            torch.save(model.state_dict(), filepath)\n",
        "            if self.verbose:\n",
        "                print(f\"\\nModel saved to {filepath}\")\n",
        "            return\n",
        "\n",
        "        if self.best is None:\n",
        "            self.best = current\n",
        "            torch.save(model.state_dict(), self.filepath)\n",
        "            if self.verbose:\n",
        "                print(f\"\\nModel saved to {self.filepath}\")\n",
        "            return\n",
        "\n",
        "        if self.mode == 'min':\n",
        "            improved = current < self.best\n",
        "        else:\n",
        "            improved = current > self.best\n",
        "\n",
        "        if improved:\n",
        "            self.best = current\n",
        "            torch.save(model.state_dict(), self.filepath)\n",
        "            if self.verbose:\n",
        "                print(f\"\\nModel saved to {self.filepath}\")\n",
        "\n",
        "# ==========================================\n",
        "# Data Preprocessing\n",
        "# ==========================================\n",
        "\n",
        "def download_and_extract_pad_ufes(url, dest_path):\n",
        "    \"\"\"Downloads and extracts PAD-UFES-20 dataset from Kaggle with progress and robust error handling.\"\"\"\n",
        "    if os.path.exists(dest_path):\n",
        "        print(\"Dataset directory already exists.\")\n",
        "        return True\n",
        "\n",
        "    zip_path = dest_path.rstrip('/') + \".zip\"\n",
        "    extract_to_dir = os.path.abspath(os.path.join(dest_path, os.pardir))\n",
        "\n",
        "    print(f\"Downloading PAD-UFES-20 dataset from Kaggle...\")\n",
        "    try:\n",
        "        # Add headers to mimic browser request\n",
        "        headers = {\n",
        "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
        "        }\n",
        "\n",
        "        with requests.get(url, stream=True, timeout=60, headers=headers) as r:\n",
        "            r.raise_for_status()\n",
        "            total_size = int(r.headers.get('content-length', 0))\n",
        "            with open(zip_path, 'wb') as f, tqdm(\n",
        "                total=total_size, unit='iB', unit_scale=True, desc=\"PAD-UFES-20\"\n",
        "            ) as progress_bar:\n",
        "                for chunk in r.iter_content(chunk_size=8192):\n",
        "                    f.write(chunk)\n",
        "                    progress_bar.update(len(chunk))\n",
        "\n",
        "        print(\"Extracting...\")\n",
        "        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "            zip_ref.extractall(extract_to_dir)\n",
        "\n",
        "        # Check if extraction created nested folders and fix if needed\n",
        "        extracted_folders = [f for f in os.listdir(extract_to_dir) if os.path.isdir(os.path.join(extract_to_dir, f))]\n",
        "        if 'pad-ufes-20' in extracted_folders:\n",
        "            # Dataset extracted correctly\n",
        "            pass\n",
        "        else:\n",
        "            # Look for the actual dataset folder\n",
        "            for folder in extracted_folders:\n",
        "                folder_path = os.path.join(extract_to_dir, folder)\n",
        "                if os.path.exists(os.path.join(folder_path, 'metadata.csv')):\n",
        "                    # This is our dataset folder, rename it\n",
        "                    os.rename(folder_path, dest_path)\n",
        "                    break\n",
        "\n",
        "        return True\n",
        "\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"\\nError downloading file: {e}\", file=sys.stderr)\n",
        "        print(\"Please download manually from: https://www.kaggle.com/datasets/maxjen/pad-ufes-20\", file=sys.stderr)\n",
        "        return False\n",
        "\n",
        "    finally:\n",
        "        if os.path.exists(zip_path):\n",
        "            os.remove(zip_path)\n",
        "\n",
        "class PADUFESDataset(Dataset):\n",
        "    \"\"\"Custom dataset class for PAD-UFES-20.\"\"\"\n",
        "    def __init__(self, data_dir, metadata_df, transform=None):\n",
        "        self.data_dir = data_dir\n",
        "        self.metadata_df = metadata_df.reset_index(drop=True)\n",
        "        self.transform = transform\n",
        "\n",
        "        # Create label mapping\n",
        "        self.classes = sorted(metadata_df['diagnostic'].unique())\n",
        "        self.class_to_idx = {cls_name: idx for idx, cls_name in enumerate(self.classes)}\n",
        "\n",
        "        print(f\"Found {len(self.classes)} classes: {self.classes}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.metadata_df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.metadata_df.iloc[idx]\n",
        "\n",
        "        # Handle different possible image path formats\n",
        "        img_name = row['img_id']\n",
        "        if not img_name.endswith('.png'):\n",
        "            img_name += '.png'\n",
        "\n",
        "        # Try different possible folder structures\n",
        "        possible_paths = [\n",
        "            os.path.join(self.data_dir, 'images', img_name),\n",
        "            os.path.join(self.data_dir, 'imgs', img_name),\n",
        "            os.path.join(self.data_dir, img_name),\n",
        "        ]\n",
        "\n",
        "        img_path = None\n",
        "        for path in possible_paths:\n",
        "            if os.path.exists(path):\n",
        "                img_path = path\n",
        "                break\n",
        "\n",
        "        if img_path is None:\n",
        "            # Last resort: search for the file\n",
        "            for root, dirs, files in os.walk(self.data_dir):\n",
        "                if img_name in files:\n",
        "                    img_path = os.path.join(root, img_name)\n",
        "                    break\n",
        "\n",
        "        if img_path is None:\n",
        "            print(f\"Warning: Image {img_name} not found, using black placeholder\")\n",
        "            image = Image.new('RGB', (224, 224), color='black')\n",
        "        else:\n",
        "            try:\n",
        "                image = Image.open(img_path).convert('RGB')\n",
        "            except Exception as e:\n",
        "                print(f\"Error loading image {img_path}: {e}\")\n",
        "                image = Image.new('RGB', (224, 224), color='black')\n",
        "\n",
        "        label = self.class_to_idx[row['diagnostic']]\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, label\n",
        "\n",
        "def prepare_pad_ufes_data(data_path):\n",
        "    \"\"\"Prepare PAD-UFES-20 data by reading metadata and organizing structure.\"\"\"\n",
        "\n",
        "    # Try different possible metadata file locations\n",
        "    possible_metadata_paths = [\n",
        "        os.path.join(data_path, 'metadata.csv'),\n",
        "        os.path.join(data_path, 'PAD-UFES-20_metadata.csv'),\n",
        "    ]\n",
        "\n",
        "    metadata_path = None\n",
        "    for path in possible_metadata_paths:\n",
        "        if os.path.exists(path):\n",
        "            metadata_path = path\n",
        "            break\n",
        "\n",
        "    if metadata_path is None:\n",
        "        # Search for any CSV file that might be the metadata\n",
        "        for root, dirs, files in os.walk(data_path):\n",
        "            for file in files:\n",
        "                if file.endswith('.csv') and 'metadata' in file.lower():\n",
        "                    metadata_path = os.path.join(root, file)\n",
        "                    break\n",
        "            if metadata_path:\n",
        "                break\n",
        "\n",
        "    if metadata_path is None:\n",
        "        print(f\"Metadata file not found in {data_path}\")\n",
        "        return None\n",
        "\n",
        "    print(f\"Loading metadata from: {metadata_path}\")\n",
        "    try:\n",
        "        metadata_df = pd.read_csv(metadata_path)\n",
        "    except Exception as e:\n",
        "        print(f\"Error reading metadata file: {e}\")\n",
        "        return None\n",
        "\n",
        "    # Print dataset info\n",
        "    print(f\"Total samples: {len(metadata_df)}\")\n",
        "    print(f\"Diagnostic classes: {metadata_df['diagnostic'].value_counts()}\")\n",
        "\n",
        "    # Verify dataset integrity\n",
        "    expected_samples = 2298\n",
        "    expected_classes = 6\n",
        "\n",
        "    actual_classes = len(metadata_df['diagnostic'].unique())\n",
        "    if len(metadata_df) != expected_samples:\n",
        "        print(f\"⚠️  Warning: Expected {expected_samples} samples, got {len(metadata_df)}\")\n",
        "    if actual_classes != expected_classes:\n",
        "        print(f\"⚠️  Warning: Expected {expected_classes} classes, got {actual_classes}\")\n",
        "\n",
        "    print(\"✅ Dataset verification complete\")\n",
        "    return metadata_df\n",
        "\n",
        "def get_dataloaders(config):\n",
        "    \"\"\"Downloads, prepares, and splits the PAD-UFES-20 data, returning DataLoaders.\"\"\"\n",
        "\n",
        "    # Try to download if URL is provided and dataset doesn't exist\n",
        "    if config.get(\"dataset_url\") and not os.path.exists(config[\"data_path\"]):\n",
        "        if not download_and_extract_pad_ufes(config[\"dataset_url\"], config[\"data_path\"]):\n",
        "            print(\"\\nAutomatic download failed. Please download manually:\")\n",
        "            print(\"1. Visit: https://www.kaggle.com/datasets/maxjen/pad-ufes-20\")\n",
        "            print(\"2. Download the dataset\")\n",
        "            print(\"3. Extract to:\", config[\"data_path\"])\n",
        "            return None\n",
        "\n",
        "    # Check if dataset exists\n",
        "    if not os.path.exists(config[\"data_path\"]):\n",
        "        print(f\"\\nDataset not found at {config['data_path']}\")\n",
        "        print(\"Please download the PAD-UFES-20 dataset:\")\n",
        "        print(\"1. Visit: https://www.kaggle.com/datasets/maxjen/pad-ufes-20\")\n",
        "        print(\"2. Download the dataset\")\n",
        "        print(\"3. Extract to:\", config[\"data_path\"])\n",
        "        print(\"4. Ensure the structure includes metadata.csv and image files\")\n",
        "        return None\n",
        "\n",
        "    # Prepare metadata\n",
        "    metadata_df = prepare_pad_ufes_data(config[\"data_path\"])\n",
        "    if metadata_df is None:\n",
        "        return None\n",
        "\n",
        "    # Define transforms\n",
        "    train_transform = transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.RandomHorizontalFlip(p=0.5),\n",
        "        transforms.RandomRotation(degrees=10),\n",
        "        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
        "        transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "    ])\n",
        "\n",
        "    val_test_transform = transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "    ])\n",
        "\n",
        "    # Create full dataset\n",
        "    full_dataset = PADUFESDataset(config[\"data_path\"], metadata_df, transform=None)\n",
        "\n",
        "    # Ensure reproducible splits\n",
        "    torch.manual_seed(config[\"split_seed\"])\n",
        "    np.random.seed(config[\"split_seed\"])\n",
        "\n",
        "    total_size = len(full_dataset)\n",
        "    train_size = int(config[\"train_split\"] * total_size)\n",
        "    val_size = int(config[\"val_split\"] * total_size)\n",
        "    test_size = int(config[\"test_split\"] * total_size)\n",
        "\n",
        "    # Adjust sizes to ensure they sum to total_size\n",
        "    actual_total = train_size + val_size + test_size\n",
        "    if actual_total != total_size:\n",
        "        test_size += (total_size - actual_total)\n",
        "\n",
        "    print(f\"Dataset splits - Train: {train_size}, Val: {val_size}, Test: {test_size}, Total: {total_size}\")\n",
        "\n",
        "    # Create indices for splitting\n",
        "    indices = list(range(total_size))\n",
        "    np.random.shuffle(indices)\n",
        "\n",
        "    train_indices = indices[:train_size]\n",
        "    val_indices = indices[train_size:train_size + val_size]\n",
        "    test_indices = indices[train_size + val_size:]\n",
        "\n",
        "    # Create subset datasets with appropriate transforms\n",
        "    train_dataset = PADUFESDataset(\n",
        "        config[\"data_path\"],\n",
        "        metadata_df.iloc[train_indices],\n",
        "        transform=train_transform\n",
        "    )\n",
        "    val_dataset = PADUFESDataset(\n",
        "        config[\"data_path\"],\n",
        "        metadata_df.iloc[val_indices],\n",
        "        transform=val_test_transform\n",
        "    )\n",
        "    test_dataset = PADUFESDataset(\n",
        "        config[\"data_path\"],\n",
        "        metadata_df.iloc[test_indices],\n",
        "        transform=val_test_transform\n",
        "    )\n",
        "\n",
        "    # Set worker init function for reproducible DataLoader behavior\n",
        "    def worker_init_fn(worker_id):\n",
        "        np.random.seed(config[\"split_seed\"] + worker_id)\n",
        "\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=config[\"batch_size\"],\n",
        "        shuffle=True,\n",
        "        num_workers=2,\n",
        "        pin_memory=True,\n",
        "        worker_init_fn=worker_init_fn,\n",
        "        generator=torch.Generator().manual_seed(config[\"split_seed\"])\n",
        "    )\n",
        "    val_loader = DataLoader(\n",
        "        val_dataset,\n",
        "        batch_size=config[\"batch_size\"],\n",
        "        shuffle=False,\n",
        "        num_workers=2,\n",
        "        pin_memory=True,\n",
        "        worker_init_fn=worker_init_fn\n",
        "    )\n",
        "    test_loader = DataLoader(\n",
        "        test_dataset,\n",
        "        batch_size=config[\"batch_size\"],\n",
        "        shuffle=False,\n",
        "        num_workers=2,\n",
        "        pin_memory=True,\n",
        "        worker_init_fn=worker_init_fn\n",
        "    )\n",
        "\n",
        "    return train_loader, val_loader, test_loader\n",
        "\n",
        "# ==========================================\n",
        "# Model Architecture and Training\n",
        "# ==========================================\n",
        "\n",
        "class ResidualBlock(nn.Module):\n",
        "    \"\"\"A residual block, the fundamental building block of ResNet.\"\"\"\n",
        "    def __init__(self, in_channels, out_channels, stride=1):\n",
        "        super(ResidualBlock, self).__init__()\n",
        "        # Main path\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "\n",
        "        # Shortcut path (for matching dimensions)\n",
        "        self.downsample = nn.Sequential()\n",
        "        if stride != 1 or in_channels != out_channels:\n",
        "            self.downsample = nn.Sequential(\n",
        "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(out_channels)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = self.downsample(x)\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        out += identity\n",
        "        out = self.relu(out)\n",
        "        return out\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "    \"\"\"A modular ResNet implementation.\"\"\"\n",
        "    def __init__(self, block, layers, num_classes=200):\n",
        "        super(ResNet, self).__init__()\n",
        "        self.in_channels = 64\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "        self.layer1 = self._make_layer(block, 64, layers[0], stride=1)\n",
        "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n",
        "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Linear(512, num_classes)\n",
        "\n",
        "    def _make_layer(self, block, out_channels, num_blocks, stride):\n",
        "        strides = [stride] + [1] * (num_blocks - 1)\n",
        "        layers = []\n",
        "        for s in strides:\n",
        "            layers.append(block(self.in_channels, out_channels, s))\n",
        "            self.in_channels = out_channels\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "        x = self.avgpool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "def ResNet18(num_classes=200):\n",
        "    return ResNet(ResidualBlock, [2, 2, 2, 2], num_classes)\n",
        "\n",
        "def evaluate_model(model, data_loader, criterion, device):\n",
        "    \"\"\"Evaluates the model on a given dataset.\"\"\"\n",
        "    model.eval()\n",
        "    total_loss, correct, total = 0.0, 0, 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in data_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            total_loss += loss.item() * images.size(0)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    avg_loss = total_loss / total\n",
        "    accuracy = 100 * correct / total\n",
        "    return avg_loss, accuracy\n",
        "\n",
        "def train_and_validate(model, train_loader, val_loader, config):\n",
        "    \"\"\"Main training loop with callbacks.\"\"\"\n",
        "    device = config[\"device\"]\n",
        "    model.to(device)\n",
        "\n",
        "    # Set seeds for reproducible training\n",
        "    torch.manual_seed(config[\"split_seed\"])\n",
        "    np.random.seed(config[\"split_seed\"])\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(config[\"split_seed\"])\n",
        "        torch.cuda.manual_seed_all(config[\"split_seed\"])\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=config[\"learning_rate\"], weight_decay=config[\"weight_decay\"])\n",
        "\n",
        "    # Initialize callbacks\n",
        "    callbacks = [\n",
        "        ReduceLROnPlateau(\n",
        "            optimizer=optimizer,\n",
        "            monitor='val_accuracy',\n",
        "            factor=0.2,\n",
        "            patience=3,\n",
        "            min_lr=1e-7,\n",
        "            verbose=1\n",
        "        ),\n",
        "        EarlyStopping(\n",
        "            monitor='val_accuracy',\n",
        "            patience=7,\n",
        "            restore_best_weights=True,\n",
        "            verbose=1\n",
        "        ),\n",
        "        ModelCheckpoint(\n",
        "            filepath=config[\"checkpoint_path\"],\n",
        "            save_best_only=True,\n",
        "            monitor='val_accuracy',\n",
        "            verbose=1\n",
        "        )\n",
        "    ]\n",
        "\n",
        "    print(\"\\n--- Starting Training ---\")\n",
        "    early_stop = False\n",
        "\n",
        "    for epoch in range(config[\"num_epochs\"]):\n",
        "        if early_stop:\n",
        "            break\n",
        "\n",
        "        model.train()\n",
        "        start_time = time.time()\n",
        "        running_loss, train_correct, train_total = 0.0, 0, 0\n",
        "\n",
        "        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{config['num_epochs']} [Train]\", leave=False)\n",
        "        for images, labels in progress_bar:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item() * images.size(0)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            train_total += labels.size(0)\n",
        "            train_correct += (predicted == labels).sum().item()\n",
        "            progress_bar.set_postfix(loss=loss.item())\n",
        "\n",
        "        train_loss = running_loss / train_total\n",
        "        train_acc = 100 * train_correct / train_total\n",
        "        val_loss, val_acc = evaluate_model(model, val_loader, criterion, device)\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{config['num_epochs']} | Time: {time.time() - start_time:.2f}s | \"\n",
        "              f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}% | \"\n",
        "              f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%\")\n",
        "\n",
        "        # Prepare logs for callbacks\n",
        "        logs = {\n",
        "            'train_loss': train_loss,\n",
        "            'train_accuracy': train_acc,\n",
        "            'val_loss': val_loss,\n",
        "            'val_accuracy': val_acc\n",
        "        }\n",
        "\n",
        "        # Execute callbacks\n",
        "        for callback in callbacks:\n",
        "            if isinstance(callback, EarlyStopping):\n",
        "                if callback.on_epoch_end(epoch, logs, model):\n",
        "                    early_stop = True\n",
        "                    break\n",
        "            elif isinstance(callback, ModelCheckpoint):\n",
        "                callback.on_epoch_end(epoch, logs, model)\n",
        "            else:\n",
        "                callback.on_epoch_end(epoch, logs)\n",
        "\n",
        "    # Execute callback cleanup\n",
        "    for callback in callbacks:\n",
        "        if isinstance(callback, EarlyStopping):\n",
        "            callback.on_training_end(model)\n",
        "        else:\n",
        "            callback.on_training_end()\n",
        "\n",
        "    print(\"--- Training Finished ---\\n\")\n",
        "    return model\n",
        "\n",
        "# ==========================================\n",
        "# Evaluation\n",
        "# ==========================================\n",
        "\n",
        "def calculate_top_k_accuracy(outputs, labels, k_values=[1, 3, 5]):\n",
        "    \"\"\"Calculate top-k accuracy for given k values.\"\"\"\n",
        "    batch_size = labels.size(0)\n",
        "    _, pred = outputs.topk(max(k_values), 1, True, True)\n",
        "    pred = pred.t()\n",
        "    correct = pred.eq(labels.view(1, -1).expand_as(pred))\n",
        "\n",
        "    top_k_accuracies = {}\n",
        "    for k in k_values:\n",
        "        correct_k = correct[:k].reshape(-1).float().sum(0, keepdim=True)\n",
        "        top_k_accuracies[k] = correct_k.item() / batch_size\n",
        "\n",
        "    return top_k_accuracies\n",
        "\n",
        "def calculate_entropy(probs):\n",
        "    \"\"\"Calculate entropy of probability distributions.\"\"\"\n",
        "    # Add small epsilon to avoid log(0)\n",
        "    epsilon = 1e-8\n",
        "    probs = probs + epsilon\n",
        "    entropy = -torch.sum(probs * torch.log(probs), dim=1)\n",
        "    return entropy\n",
        "\n",
        "def calculate_ece(confidences, accuracies, n_bins=10):\n",
        "    \"\"\"Calculate Expected Calibration Error.\"\"\"\n",
        "    bin_boundaries = torch.linspace(0, 1, n_bins + 1)\n",
        "    bin_lowers = bin_boundaries[:-1]\n",
        "    bin_uppers = bin_boundaries[1:]\n",
        "\n",
        "    ece = 0\n",
        "    for bin_lower, bin_upper in zip(bin_lowers, bin_uppers):\n",
        "        in_bin = (confidences > bin_lower.item()) & (confidences <= bin_upper.item())\n",
        "        prop_in_bin = in_bin.float().mean()\n",
        "\n",
        "        if prop_in_bin.item() > 0:\n",
        "            accuracy_in_bin = accuracies[in_bin].float().mean()\n",
        "            avg_confidence_in_bin = confidences[in_bin].mean()\n",
        "            ece += torch.abs(avg_confidence_in_bin - accuracy_in_bin) * prop_in_bin\n",
        "\n",
        "    return ece.item()\n",
        "\n",
        "def comprehensive_evaluation(model, test_loader, criterion, device):\n",
        "    \"\"\"Comprehensive evaluation with all requested metrics.\"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    all_predictions = []\n",
        "    all_labels = []\n",
        "    all_confidences = []\n",
        "    all_entropies = []\n",
        "    all_top_k_results = {1: [], 3: [], 5: []}\n",
        "    total_loss = 0.0\n",
        "    total_samples = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in tqdm(test_loader, desc=\"Evaluating\"):\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            # Convert outputs to probabilities\n",
        "            probs = torch.softmax(outputs, dim=1)\n",
        "\n",
        "            # Get predictions and confidences\n",
        "            confidences, predictions = torch.max(probs, dim=1)\n",
        "\n",
        "            # Calculate entropy\n",
        "            entropies = calculate_entropy(probs)\n",
        "\n",
        "            # Calculate top-k accuracies\n",
        "            top_k_accs = calculate_top_k_accuracy(outputs, labels, [1, 3, 5])\n",
        "\n",
        "            # Store results\n",
        "            all_predictions.extend(predictions.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "            all_confidences.extend(confidences.cpu().numpy())\n",
        "            all_entropies.extend(entropies.cpu().numpy())\n",
        "\n",
        "            for k in [1, 3, 5]:\n",
        "                all_top_k_results[k].extend([top_k_accs[k]] * labels.size(0))\n",
        "\n",
        "            total_loss += loss.item() * images.size(0)\n",
        "            total_samples += images.size(0)\n",
        "\n",
        "    # Convert to numpy arrays\n",
        "    all_predictions = np.array(all_predictions)\n",
        "    all_labels = np.array(all_labels)\n",
        "    all_confidences = np.array(all_confidences)\n",
        "    all_entropies = np.array(all_entropies)\n",
        "\n",
        "    # Calculate basic metrics\n",
        "    avg_loss = total_loss / total_samples\n",
        "\n",
        "    # Calculate top-k accuracies\n",
        "    top1 = np.mean([pred == label for pred, label in zip(all_predictions, all_labels)])\n",
        "    top3 = np.mean(all_top_k_results[3])\n",
        "    top5 = np.mean(all_top_k_results[5])\n",
        "\n",
        "    # Calculate precision, recall, f1-score\n",
        "    precision_macro, recall_macro, f1_macro, _ = precision_recall_fscore_support(\n",
        "        all_labels, all_predictions, average='macro', zero_division=0\n",
        "    )\n",
        "    precision_weighted, recall_weighted, f1_weighted, _ = precision_recall_fscore_support(\n",
        "        all_labels, all_predictions, average='weighted', zero_division=0\n",
        "    )\n",
        "\n",
        "    # Calculate confidence and calibration metrics\n",
        "    avg_confidence = np.mean(all_confidences)\n",
        "    std_confidence = np.std(all_confidences)\n",
        "    avg_entropy = np.mean(all_entropies)\n",
        "\n",
        "    # Calculate ECE\n",
        "    accuracies = (all_predictions == all_labels).astype(float)\n",
        "    ece = calculate_ece(torch.tensor(all_confidences), torch.tensor(accuracies))\n",
        "\n",
        "    return {\n",
        "        'test_loss': avg_loss,\n",
        "        'top1': top1,\n",
        "        'top3': top3,\n",
        "        'top5': top5,\n",
        "        'precision_macro': precision_macro,\n",
        "        'recall_macro': recall_macro,\n",
        "        'f1_macro': f1_macro,\n",
        "        'precision_weighted': precision_weighted,\n",
        "        'recall_weighted': recall_weighted,\n",
        "        'f1_weighted': f1_weighted,\n",
        "        'avg_confidence': avg_confidence,\n",
        "        'std_confidence': std_confidence,\n",
        "        'avg_entropy': avg_entropy,\n",
        "        'ece': ece\n",
        "    }\n",
        "\n",
        "# ==========================================\n",
        "# Main Execution\n",
        "# ==========================================\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # Set global seeds for full reproducibility\n",
        "    torch.manual_seed(CONFIG[\"split_seed\"])\n",
        "    np.random.seed(CONFIG[\"split_seed\"])\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(CONFIG[\"split_seed\"])\n",
        "        torch.cuda.manual_seed_all(CONFIG[\"split_seed\"])\n",
        "        # Ensure deterministic behavior on CUDA\n",
        "        torch.backends.cudnn.deterministic = True\n",
        "        torch.backends.cudnn.benchmark = False\n",
        "\n",
        "    print(f\"Using device: {CONFIG['device']}\")\n",
        "    print(f\"Reproducibility seed: {CONFIG['split_seed']}\")\n",
        "\n",
        "    # Get data loaders\n",
        "    dataloaders = get_dataloaders(CONFIG)\n",
        "    if dataloaders is None:\n",
        "        print(\"Could not prepare data. Halting execution.\", file=sys.stderr)\n",
        "        sys.exit(1) # Exit if data preparation failed\n",
        "\n",
        "    train_loader, val_loader, test_loader = dataloaders\n",
        "\n",
        "    # Initialize and train model\n",
        "    model = ResNet18(num_classes=CONFIG[\"num_classes\"])\n",
        "    trained_model = train_and_validate(model, train_loader, val_loader, CONFIG)\n",
        "\n",
        "    # Comprehensive evaluation on the test set\n",
        "    print(\"--- Starting Comprehensive Evaluation on Test Set ---\")\n",
        "    results = comprehensive_evaluation(trained_model, test_loader, nn.CrossEntropyLoss(), CONFIG[\"device\"])\n",
        "\n",
        "    # Print all requested metrics\n",
        "    print(\"\\n--- Top-K Accuracy Results ---\")\n",
        "    print(f\"Top-1 Accuracy: {results['top1'] * 100:.2f}%\")\n",
        "    print(f\"Top-3 Accuracy: {results['top3'] * 100:.2f}%\")\n",
        "    print(f\"Top-5 Accuracy: {results['top5'] * 100:.2f}%\")\n",
        "\n",
        "    print(\"\\n--- Additional Performance Metrics ---\")\n",
        "    print(f\"Macro Average Precision: {results['precision_macro']:.4f}\")\n",
        "    print(f\"Macro Average Recall: {results['recall_macro']:.4f}\")\n",
        "    print(f\"Macro Average F1-Score: {results['f1_macro']:.4f}\")\n",
        "    print(f\"Weighted Average Precision: {results['precision_weighted']:.4f}\")\n",
        "    print(f\"Weighted Average Recall: {results['recall_weighted']:.4f}\")\n",
        "    print(f\"Weighted Average F1-Score: {results['f1_weighted']:.4f}\")\n",
        "\n",
        "    print(\"\\n--- Model Confidence & Calibration Metrics ---\")\n",
        "    print(f\"Average Prediction Confidence: {results['avg_confidence']:.4f}\")\n",
        "    print(f\"Confidence Standard Deviation: {results['std_confidence']:.4f}\")\n",
        "    print(f\"Average Prediction Entropy: {results['avg_entropy']:.4f}\")\n",
        "    print(f\"Expected Calibration Error: {results['ece']:.4f}\")\n",
        "\n",
        "    print(f\"\\nFinal Test Loss: {results['test_loss']:.4f}\")\n",
        "    print(\"===============================================\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hybrid Transformer on ResNet-18 (No PE)"
      ],
      "metadata": {
        "id": "8tNjeYCnqE1O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "import torchvision.transforms as transforms\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "import os\n",
        "import requests\n",
        "import zipfile\n",
        "import shutil\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "import sys\n",
        "import numpy as np\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "\n",
        "# --- Configuration Parameters ---\n",
        "CONFIG = {\n",
        "    \"data_path\": \"./pad-ufes-20/\",\n",
        "    \"dataset_url\": \"https://www.kaggle.com/api/v1/datasets/download/maxjen/pad-ufes-20\",  # Kaggle API URL\n",
        "    \"batch_size\": 64,  # Reduced due to smaller dataset\n",
        "    \"num_epochs\": 100,\n",
        "    \"learning_rate\": 0.001,\n",
        "    \"weight_decay\": 1e-4,\n",
        "    \"split_seed\": 42,\n",
        "    \"train_split\": 0.70,\n",
        "    \"val_split\": 0.15,\n",
        "    \"test_split\": 0.15,\n",
        "    \"num_classes\": 6,  # PAD-UFES-20 has 6 diagnostic classes\n",
        "    \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
        "    \"checkpoint_path\": \"./best_model_pad_ufes.pth\",\n",
        "}\n",
        "\n",
        "# --- Transformer Configuration ---\n",
        "TRANSFORMER_CONFIG = {\n",
        "    \"embedding_dim\": 32,\n",
        "    \"nhead\": 16,\n",
        "    \"num_encoder_layers\": 3,\n",
        "    \"dim_feedforward\": 2,\n",
        "    \"dropout\": 0.1,\n",
        "}\n",
        "\n",
        "# ==========================================\n",
        "# Callback Classes\n",
        "# ==========================================\n",
        "\n",
        "class Callback:\n",
        "    \"\"\"Base callback class\"\"\"\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        pass\n",
        "\n",
        "    def on_training_end(self):\n",
        "        pass\n",
        "\n",
        "class ReduceLROnPlateau(Callback):\n",
        "    \"\"\"Reduce learning rate when a metric has stopped improving\"\"\"\n",
        "    def __init__(self, optimizer, monitor='val_loss', factor=0.2, patience=3, min_lr=1e-7, verbose=1):\n",
        "        self.optimizer = optimizer\n",
        "        self.monitor = monitor\n",
        "        self.factor = factor\n",
        "        self.patience = patience\n",
        "        self.min_lr = min_lr\n",
        "        self.verbose = verbose\n",
        "        self.wait = 0\n",
        "        self.best = None\n",
        "        self.mode = 'min' if 'loss' in monitor else 'max'\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        if logs is None:\n",
        "            return\n",
        "\n",
        "        current = logs.get(self.monitor)\n",
        "        if current is None:\n",
        "            return\n",
        "\n",
        "        if self.best is None:\n",
        "            self.best = current\n",
        "            return\n",
        "\n",
        "        if self.mode == 'min':\n",
        "            improved = current < self.best\n",
        "        else:\n",
        "            improved = current > self.best\n",
        "\n",
        "        if improved:\n",
        "            self.best = current\n",
        "            self.wait = 0\n",
        "        else:\n",
        "            self.wait += 1\n",
        "            if self.wait >= self.patience:\n",
        "                old_lr = self.optimizer.param_groups[0]['lr']\n",
        "                new_lr = max(old_lr * self.factor, self.min_lr)\n",
        "                if new_lr != old_lr:\n",
        "                    self.optimizer.param_groups[0]['lr'] = new_lr\n",
        "                    if self.verbose:\n",
        "                        print(f\"\\nReducing learning rate from {old_lr:.2e} to {new_lr:.2e}\")\n",
        "                    self.wait = 0\n",
        "\n",
        "class EarlyStopping(Callback):\n",
        "    \"\"\"Stop training when a monitored metric has stopped improving\"\"\"\n",
        "    def __init__(self, monitor='val_loss', patience=7, restore_best_weights=True, verbose=1):\n",
        "        self.monitor = monitor\n",
        "        self.patience = patience\n",
        "        self.restore_best_weights = restore_best_weights\n",
        "        self.verbose = verbose\n",
        "        self.wait = 0\n",
        "        self.best = None\n",
        "        self.best_weights = None\n",
        "        self.mode = 'min' if 'loss' in monitor else 'max'\n",
        "        self.stopped_epoch = 0\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None, model=None):\n",
        "        if logs is None:\n",
        "            return False\n",
        "\n",
        "        current = logs.get(self.monitor)\n",
        "        if current is None:\n",
        "            return False\n",
        "\n",
        "        if self.best is None:\n",
        "            self.best = current\n",
        "            if model is not None and self.restore_best_weights:\n",
        "                self.best_weights = model.state_dict().copy()\n",
        "            return False\n",
        "\n",
        "        if self.mode == 'min':\n",
        "            improved = current < self.best\n",
        "        else:\n",
        "            improved = current > self.best\n",
        "\n",
        "        if improved:\n",
        "            self.best = current\n",
        "            self.wait = 0\n",
        "            if model is not None and self.restore_best_weights:\n",
        "                self.best_weights = model.state_dict().copy()\n",
        "        else:\n",
        "            self.wait += 1\n",
        "            if self.wait >= self.patience:\n",
        "                self.stopped_epoch = epoch\n",
        "                if self.verbose:\n",
        "                    print(f\"\\nEarly stopping at epoch {epoch + 1}\")\n",
        "                return True\n",
        "        return False\n",
        "\n",
        "    def on_training_end(self, model=None):\n",
        "        if self.stopped_epoch > 0 and self.verbose:\n",
        "            print(f\"Restored model weights from the end of the best epoch: {self.stopped_epoch + 1 - self.patience}\")\n",
        "        if model is not None and self.restore_best_weights and self.best_weights is not None:\n",
        "            model.load_state_dict(self.best_weights)\n",
        "\n",
        "class ModelCheckpoint(Callback):\n",
        "    \"\"\"Save the model after every epoch\"\"\"\n",
        "    def __init__(self, filepath, save_best_only=True, monitor='val_accuracy', verbose=1):\n",
        "        self.filepath = filepath\n",
        "        self.save_best_only = save_best_only\n",
        "        self.monitor = monitor\n",
        "        self.verbose = verbose\n",
        "        self.best = None\n",
        "        self.mode = 'min' if 'loss' in monitor else 'max'\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None, model=None):\n",
        "        if logs is None or model is None:\n",
        "            return\n",
        "\n",
        "        current = logs.get(self.monitor)\n",
        "        if current is None:\n",
        "            return\n",
        "\n",
        "        if not self.save_best_only:\n",
        "            filepath = self.filepath.replace('.pth', f'_epoch_{epoch + 1}.pth')\n",
        "            torch.save(model.state_dict(), filepath)\n",
        "            if self.verbose:\n",
        "                print(f\"\\nModel saved to {filepath}\")\n",
        "            return\n",
        "\n",
        "        if self.best is None:\n",
        "            self.best = current\n",
        "            torch.save(model.state_dict(), self.filepath)\n",
        "            if self.verbose:\n",
        "                print(f\"\\nModel saved to {self.filepath}\")\n",
        "            return\n",
        "\n",
        "        if self.mode == 'min':\n",
        "            improved = current < self.best\n",
        "        else:\n",
        "            improved = current > self.best\n",
        "\n",
        "        if improved:\n",
        "            self.best = current\n",
        "            torch.save(model.state_dict(), self.filepath)\n",
        "            if self.verbose:\n",
        "                print(f\"\\nModel saved to {self.filepath}\")\n",
        "\n",
        "# ==========================================\n",
        "# Data Preprocessing\n",
        "# ==========================================\n",
        "\n",
        "def download_and_extract_pad_ufes(url, dest_path):\n",
        "    \"\"\"Downloads and extracts PAD-UFES-20 dataset from Kaggle with progress and robust error handling.\"\"\"\n",
        "    if os.path.exists(dest_path):\n",
        "        print(\"Dataset directory already exists.\")\n",
        "        return True\n",
        "\n",
        "    zip_path = dest_path.rstrip('/') + \".zip\"\n",
        "    extract_to_dir = os.path.abspath(os.path.join(dest_path, os.pardir))\n",
        "\n",
        "    print(f\"Downloading PAD-UFES-20 dataset from Kaggle...\")\n",
        "    try:\n",
        "        # Add headers to mimic browser request\n",
        "        headers = {\n",
        "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
        "        }\n",
        "\n",
        "        with requests.get(url, stream=True, timeout=60, headers=headers) as r:\n",
        "            r.raise_for_status()\n",
        "            total_size = int(r.headers.get('content-length', 0))\n",
        "            with open(zip_path, 'wb') as f, tqdm(\n",
        "                total=total_size, unit='iB', unit_scale=True, desc=\"PAD-UFES-20\"\n",
        "            ) as progress_bar:\n",
        "                for chunk in r.iter_content(chunk_size=8192):\n",
        "                    f.write(chunk)\n",
        "                    progress_bar.update(len(chunk))\n",
        "\n",
        "        print(\"Extracting...\")\n",
        "        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "            zip_ref.extractall(extract_to_dir)\n",
        "\n",
        "        # Check if extraction created nested folders and fix if needed\n",
        "        extracted_folders = [f for f in os.listdir(extract_to_dir) if os.path.isdir(os.path.join(extract_to_dir, f))]\n",
        "        if 'pad-ufes-20' in extracted_folders:\n",
        "            # Dataset extracted correctly\n",
        "            pass\n",
        "        else:\n",
        "            # Look for the actual dataset folder\n",
        "            for folder in extracted_folders:\n",
        "                folder_path = os.path.join(extract_to_dir, folder)\n",
        "                if os.path.exists(os.path.join(folder_path, 'metadata.csv')):\n",
        "                    # This is our dataset folder, rename it\n",
        "                    os.rename(folder_path, dest_path)\n",
        "                    break\n",
        "\n",
        "        return True\n",
        "\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"\\nError downloading file: {e}\", file=sys.stderr)\n",
        "        print(\"Please download manually from: https://www.kaggle.com/datasets/maxjen/pad-ufes-20\", file=sys.stderr)\n",
        "        return False\n",
        "\n",
        "    finally:\n",
        "        if os.path.exists(zip_path):\n",
        "            os.remove(zip_path)\n",
        "\n",
        "class PADUFESDataset(Dataset):\n",
        "    \"\"\"Custom dataset class for PAD-UFES-20.\"\"\"\n",
        "    def __init__(self, data_dir, metadata_df, transform=None):\n",
        "        self.data_dir = data_dir\n",
        "        self.metadata_df = metadata_df.reset_index(drop=True)\n",
        "        self.transform = transform\n",
        "\n",
        "        # Create label mapping\n",
        "        self.classes = sorted(metadata_df['diagnostic'].unique())\n",
        "        self.class_to_idx = {cls_name: idx for idx, cls_name in enumerate(self.classes)}\n",
        "\n",
        "        print(f\"Found {len(self.classes)} classes: {self.classes}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.metadata_df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.metadata_df.iloc[idx]\n",
        "\n",
        "        # Handle different possible image path formats\n",
        "        img_name = row['img_id']\n",
        "        if not img_name.endswith('.png'):\n",
        "            img_name += '.png'\n",
        "\n",
        "        # Try different possible folder structures\n",
        "        possible_paths = [\n",
        "            os.path.join(self.data_dir, 'images', img_name),\n",
        "            os.path.join(self.data_dir, 'imgs', img_name),\n",
        "            os.path.join(self.data_dir, img_name),\n",
        "        ]\n",
        "\n",
        "        img_path = None\n",
        "        for path in possible_paths:\n",
        "            if os.path.exists(path):\n",
        "                img_path = path\n",
        "                break\n",
        "\n",
        "        if img_path is None:\n",
        "            # Last resort: search for the file\n",
        "            for root, dirs, files in os.walk(self.data_dir):\n",
        "                if img_name in files:\n",
        "                    img_path = os.path.join(root, img_name)\n",
        "                    break\n",
        "\n",
        "        if img_path is None:\n",
        "            print(f\"Warning: Image {img_name} not found, using black placeholder\")\n",
        "            image = Image.new('RGB', (224, 224), color='black')\n",
        "        else:\n",
        "            try:\n",
        "                image = Image.open(img_path).convert('RGB')\n",
        "            except Exception as e:\n",
        "                print(f\"Error loading image {img_path}: {e}\")\n",
        "                image = Image.new('RGB', (224, 224), color='black')\n",
        "\n",
        "        label = self.class_to_idx[row['diagnostic']]\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, label\n",
        "\n",
        "def prepare_pad_ufes_data(data_path):\n",
        "    \"\"\"Prepare PAD-UFES-20 data by reading metadata and organizing structure.\"\"\"\n",
        "\n",
        "    # Try different possible metadata file locations\n",
        "    possible_metadata_paths = [\n",
        "        os.path.join(data_path, 'metadata.csv'),\n",
        "        os.path.join(data_path, 'PAD-UFES-20_metadata.csv'),\n",
        "    ]\n",
        "\n",
        "    metadata_path = None\n",
        "    for path in possible_metadata_paths:\n",
        "        if os.path.exists(path):\n",
        "            metadata_path = path\n",
        "            break\n",
        "\n",
        "    if metadata_path is None:\n",
        "        # Search for any CSV file that might be the metadata\n",
        "        for root, dirs, files in os.walk(data_path):\n",
        "            for file in files:\n",
        "                if file.endswith('.csv') and 'metadata' in file.lower():\n",
        "                    metadata_path = os.path.join(root, file)\n",
        "                    break\n",
        "            if metadata_path:\n",
        "                break\n",
        "\n",
        "    if metadata_path is None:\n",
        "        print(f\"Metadata file not found in {data_path}\")\n",
        "        return None\n",
        "\n",
        "    print(f\"Loading metadata from: {metadata_path}\")\n",
        "    try:\n",
        "        metadata_df = pd.read_csv(metadata_path)\n",
        "    except Exception as e:\n",
        "        print(f\"Error reading metadata file: {e}\")\n",
        "        return None\n",
        "\n",
        "    # Print dataset info\n",
        "    print(f\"Total samples: {len(metadata_df)}\")\n",
        "    print(f\"Diagnostic classes: {metadata_df['diagnostic'].value_counts()}\")\n",
        "\n",
        "    # Verify dataset integrity\n",
        "    expected_samples = 2298\n",
        "    expected_classes = 6\n",
        "\n",
        "    actual_classes = len(metadata_df['diagnostic'].unique())\n",
        "    if len(metadata_df) != expected_samples:\n",
        "        print(f\"⚠️  Warning: Expected {expected_samples} samples, got {len(metadata_df)}\")\n",
        "    if actual_classes != expected_classes:\n",
        "        print(f\"⚠️  Warning: Expected {expected_classes} classes, got {actual_classes}\")\n",
        "\n",
        "    print(\"✅ Dataset verification complete\")\n",
        "    return metadata_df\n",
        "\n",
        "def get_dataloaders(config):\n",
        "    \"\"\"Downloads, prepares, and splits the PAD-UFES-20 data, returning DataLoaders.\"\"\"\n",
        "\n",
        "    # Try to download if URL is provided and dataset doesn't exist\n",
        "    if config.get(\"dataset_url\") and not os.path.exists(config[\"data_path\"]):\n",
        "        if not download_and_extract_pad_ufes(config[\"dataset_url\"], config[\"data_path\"]):\n",
        "            print(\"\\nAutomatic download failed. Please download manually:\")\n",
        "            print(\"1. Visit: https://www.kaggle.com/datasets/maxjen/pad-ufes-20\")\n",
        "            print(\"2. Download the dataset\")\n",
        "            print(\"3. Extract to:\", config[\"data_path\"])\n",
        "            return None\n",
        "\n",
        "    # Check if dataset exists\n",
        "    if not os.path.exists(config[\"data_path\"]):\n",
        "        print(f\"\\nDataset not found at {config['data_path']}\")\n",
        "        print(\"Please download the PAD-UFES-20 dataset:\")\n",
        "        print(\"1. Visit: https://www.kaggle.com/datasets/maxjen/pad-ufes-20\")\n",
        "        print(\"2. Download the dataset\")\n",
        "        print(\"3. Extract to:\", config[\"data_path\"])\n",
        "        print(\"4. Ensure the structure includes metadata.csv and image files\")\n",
        "        return None\n",
        "\n",
        "    # Prepare metadata\n",
        "    metadata_df = prepare_pad_ufes_data(config[\"data_path\"])\n",
        "    if metadata_df is None:\n",
        "        return None\n",
        "\n",
        "    # Define transforms\n",
        "    train_transform = transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.RandomHorizontalFlip(p=0.5),\n",
        "        transforms.RandomRotation(degrees=10),\n",
        "        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
        "        transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "    ])\n",
        "\n",
        "    val_test_transform = transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "    ])\n",
        "\n",
        "    # Create full dataset\n",
        "    full_dataset = PADUFESDataset(config[\"data_path\"], metadata_df, transform=None)\n",
        "\n",
        "    # Ensure reproducible splits\n",
        "    torch.manual_seed(config[\"split_seed\"])\n",
        "    np.random.seed(config[\"split_seed\"])\n",
        "\n",
        "    total_size = len(full_dataset)\n",
        "    train_size = int(config[\"train_split\"] * total_size)\n",
        "    val_size = int(config[\"val_split\"] * total_size)\n",
        "    test_size = int(config[\"test_split\"] * total_size)\n",
        "\n",
        "    # Adjust sizes to ensure they sum to total_size\n",
        "    actual_total = train_size + val_size + test_size\n",
        "    if actual_total != total_size:\n",
        "        test_size += (total_size - actual_total)\n",
        "\n",
        "    print(f\"Dataset splits - Train: {train_size}, Val: {val_size}, Test: {test_size}, Total: {total_size}\")\n",
        "\n",
        "    # Create indices for splitting\n",
        "    indices = list(range(total_size))\n",
        "    np.random.shuffle(indices)\n",
        "\n",
        "    train_indices = indices[:train_size]\n",
        "    val_indices = indices[train_size:train_size + val_size]\n",
        "    test_indices = indices[train_size + val_size:]\n",
        "\n",
        "    # Create subset datasets with appropriate transforms\n",
        "    train_dataset = PADUFESDataset(\n",
        "        config[\"data_path\"],\n",
        "        metadata_df.iloc[train_indices],\n",
        "        transform=train_transform\n",
        "    )\n",
        "    val_dataset = PADUFESDataset(\n",
        "        config[\"data_path\"],\n",
        "        metadata_df.iloc[val_indices],\n",
        "        transform=val_test_transform\n",
        "    )\n",
        "    test_dataset = PADUFESDataset(\n",
        "        config[\"data_path\"],\n",
        "        metadata_df.iloc[test_indices],\n",
        "        transform=val_test_transform\n",
        "    )\n",
        "\n",
        "    # Set worker init function for reproducible DataLoader behavior\n",
        "    def worker_init_fn(worker_id):\n",
        "        np.random.seed(config[\"split_seed\"] + worker_id)\n",
        "\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=config[\"batch_size\"],\n",
        "        shuffle=True,\n",
        "        num_workers=2,\n",
        "        pin_memory=True,\n",
        "        worker_init_fn=worker_init_fn,\n",
        "        generator=torch.Generator().manual_seed(config[\"split_seed\"])\n",
        "    )\n",
        "    val_loader = DataLoader(\n",
        "        val_dataset,\n",
        "        batch_size=config[\"batch_size\"],\n",
        "        shuffle=False,\n",
        "        num_workers=2,\n",
        "        pin_memory=True,\n",
        "        worker_init_fn=worker_init_fn\n",
        "    )\n",
        "    test_loader = DataLoader(\n",
        "        test_dataset,\n",
        "        batch_size=config[\"batch_size\"],\n",
        "        shuffle=False,\n",
        "        num_workers=2,\n",
        "        pin_memory=True,\n",
        "        worker_init_fn=worker_init_fn\n",
        "    )\n",
        "\n",
        "    return train_loader, val_loader, test_loader\n",
        "\n",
        "# ==========================================\n",
        "# Model Architecture and Training\n",
        "# ==========================================\n",
        "\n",
        "class NonResidualBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    This block is a standard convolutional block WITHOUT the residual connection.\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels, out_channels, stride=1):\n",
        "        super(NonResidualBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        out = self.relu(out)\n",
        "        return out\n",
        "\n",
        "class ResNetTransformer(nn.Module):\n",
        "    \"\"\"\n",
        "    A hybrid architecture combining a non-residual CNN backbone with a Transformer encoder.\n",
        "    \"\"\"\n",
        "    def __init__(self, block, layers, num_classes, t_config):\n",
        "        super(ResNetTransformer, self).__init__()\n",
        "        self.in_channels = 64\n",
        "        self.embedding_dim = t_config[\"embedding_dim\"]\n",
        "\n",
        "        # 1. CNN Backbone (Feature Extractor)\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "        self.layer1 = self._make_layer(block, 64, layers[0], stride=1)\n",
        "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n",
        "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n",
        "\n",
        "        # 2. Projection heads to create tokens from feature maps\n",
        "        self.projections = nn.ModuleList([\n",
        "            self._create_projection(64, self.embedding_dim),   # From initial maxpool\n",
        "            self._create_projection(64, self.embedding_dim),   # From layer1\n",
        "            self._create_projection(128, self.embedding_dim),  # From layer2\n",
        "            self._create_projection(256, self.embedding_dim),  # From layer3\n",
        "            self._create_projection(512, self.embedding_dim)   # From layer4\n",
        "        ])\n",
        "\n",
        "        # 3. Transformer Encoder\n",
        "        encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=self.embedding_dim,\n",
        "            nhead=t_config[\"nhead\"],\n",
        "            dim_feedforward=t_config[\"dim_feedforward\"],\n",
        "            dropout=t_config[\"dropout\"],\n",
        "            batch_first=True\n",
        "        )\n",
        "        self.transformer_encoder = nn.TransformerEncoder(\n",
        "            encoder_layer,\n",
        "            num_layers=t_config[\"num_encoder_layers\"]\n",
        "        )\n",
        "\n",
        "        # 4. Final Classifier\n",
        "        self.classifier = nn.Linear(self.embedding_dim, num_classes)\n",
        "\n",
        "    def _create_projection(self, in_features, out_features):\n",
        "        return nn.Sequential(\n",
        "            nn.AdaptiveAvgPool2d((1, 1)),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(in_features, out_features)\n",
        "        )\n",
        "\n",
        "    def _make_layer(self, block, out_channels, num_blocks, stride):\n",
        "        strides = [stride] + [1] * (num_blocks - 1)\n",
        "        layers = []\n",
        "        current_in_channels = self.in_channels\n",
        "        for s in strides:\n",
        "            layers.append(block(current_in_channels, out_channels, s))\n",
        "            current_in_channels = out_channels\n",
        "        self.in_channels = out_channels\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # 1. Pass through CNN backbone and capture features\n",
        "        features = []\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "        features.append(x)\n",
        "\n",
        "        x = self.layer1(x); features.append(x)\n",
        "        x = self.layer2(x); features.append(x)\n",
        "        x = self.layer3(x); features.append(x)\n",
        "        x = self.layer4(x); features.append(x)\n",
        "\n",
        "        # 2. Project features to tokens\n",
        "        tokens = []\n",
        "        for i, feature_map in enumerate(features):\n",
        "            tokens.append(self.projections[i](feature_map))\n",
        "\n",
        "        # 3. Stack tokens and pass through Transformer\n",
        "        token_sequence = torch.stack(tokens, dim=1)\n",
        "        transformer_out = self.transformer_encoder(token_sequence)\n",
        "\n",
        "        # 4. Aggregate and classify\n",
        "        aggregated_vector = transformer_out.mean(dim=1)\n",
        "        logits = self.classifier(aggregated_vector)\n",
        "\n",
        "        return logits\n",
        "\n",
        "def ResNetTransformer18(num_classes=200, t_config=TRANSFORMER_CONFIG):\n",
        "    return ResNetTransformer(NonResidualBlock, [2, 2, 2, 2], num_classes, t_config)\n",
        "\n",
        "def evaluate_model(model, data_loader, criterion, device):\n",
        "    \"\"\"Evaluates the model on a given dataset.\"\"\"\n",
        "    model.eval()\n",
        "    total_loss, correct, total = 0.0, 0, 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in data_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            total_loss += loss.item() * images.size(0)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    avg_loss = total_loss / total\n",
        "    accuracy = 100 * correct / total\n",
        "    return avg_loss, accuracy\n",
        "\n",
        "def train_and_validate(model, train_loader, val_loader, config):\n",
        "    \"\"\"Main training loop with callbacks.\"\"\"\n",
        "    device = config[\"device\"]\n",
        "    model.to(device)\n",
        "\n",
        "    # Set seeds for reproducible training\n",
        "    torch.manual_seed(config[\"split_seed\"])\n",
        "    np.random.seed(config[\"split_seed\"])\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(config[\"split_seed\"])\n",
        "        torch.cuda.manual_seed_all(config[\"split_seed\"])\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=config[\"learning_rate\"], weight_decay=config[\"weight_decay\"])\n",
        "\n",
        "    # Initialize callbacks\n",
        "    callbacks = [\n",
        "        ReduceLROnPlateau(\n",
        "            optimizer=optimizer,\n",
        "            monitor='val_accuracy',\n",
        "            factor=0.2,\n",
        "            patience=3,\n",
        "            min_lr=1e-7,\n",
        "            verbose=1\n",
        "        ),\n",
        "        EarlyStopping(\n",
        "            monitor='val_accuracy',\n",
        "            patience=7,\n",
        "            restore_best_weights=True,\n",
        "            verbose=1\n",
        "        ),\n",
        "        ModelCheckpoint(\n",
        "            filepath=config[\"checkpoint_path\"],\n",
        "            save_best_only=True,\n",
        "            monitor='val_accuracy',\n",
        "            verbose=1\n",
        "        )\n",
        "    ]\n",
        "\n",
        "    print(\"\\n--- Starting Training ---\")\n",
        "    early_stop = False\n",
        "\n",
        "    for epoch in range(config[\"num_epochs\"]):\n",
        "        if early_stop:\n",
        "            break\n",
        "\n",
        "        model.train()\n",
        "        start_time = time.time()\n",
        "        running_loss, train_correct, train_total = 0.0, 0, 0\n",
        "\n",
        "        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{config['num_epochs']} [Train]\", leave=False)\n",
        "        for images, labels in progress_bar:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item() * images.size(0)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            train_total += labels.size(0)\n",
        "            train_correct += (predicted == labels).sum().item()\n",
        "            progress_bar.set_postfix(loss=loss.item())\n",
        "\n",
        "        train_loss = running_loss / train_total\n",
        "        train_acc = 100 * train_correct / train_total\n",
        "        val_loss, val_acc = evaluate_model(model, val_loader, criterion, device)\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{config['num_epochs']} | Time: {time.time() - start_time:.2f}s | \"\n",
        "              f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}% | \"\n",
        "              f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%\")\n",
        "\n",
        "        # Prepare logs for callbacks\n",
        "        logs = {\n",
        "            'train_loss': train_loss,\n",
        "            'train_accuracy': train_acc,\n",
        "            'val_loss': val_loss,\n",
        "            'val_accuracy': val_acc\n",
        "        }\n",
        "\n",
        "        # Execute callbacks\n",
        "        for callback in callbacks:\n",
        "            if isinstance(callback, EarlyStopping):\n",
        "                if callback.on_epoch_end(epoch, logs, model):\n",
        "                    early_stop = True\n",
        "                    break\n",
        "            elif isinstance(callback, ModelCheckpoint):\n",
        "                callback.on_epoch_end(epoch, logs, model)\n",
        "            else:\n",
        "                callback.on_epoch_end(epoch, logs)\n",
        "\n",
        "    # Execute callback cleanup\n",
        "    for callback in callbacks:\n",
        "        if isinstance(callback, EarlyStopping):\n",
        "            callback.on_training_end(model)\n",
        "        else:\n",
        "            callback.on_training_end()\n",
        "\n",
        "    print(\"--- Training Finished ---\\n\")\n",
        "    return model\n",
        "\n",
        "# ==========================================\n",
        "# Evaluation\n",
        "# ==========================================\n",
        "\n",
        "def calculate_top_k_accuracy(outputs, labels, k_values=[1, 3, 5]):\n",
        "    \"\"\"Calculate top-k accuracy for given k values.\"\"\"\n",
        "    batch_size = labels.size(0)\n",
        "    _, pred = outputs.topk(max(k_values), 1, True, True)\n",
        "    pred = pred.t()\n",
        "    correct = pred.eq(labels.view(1, -1).expand_as(pred))\n",
        "\n",
        "    top_k_accuracies = {}\n",
        "    for k in k_values:\n",
        "        correct_k = correct[:k].reshape(-1).float().sum(0, keepdim=True)\n",
        "        top_k_accuracies[k] = correct_k.item() / batch_size\n",
        "\n",
        "    return top_k_accuracies\n",
        "\n",
        "def calculate_entropy(probs):\n",
        "    \"\"\"Calculate entropy of probability distributions.\"\"\"\n",
        "    # Add small epsilon to avoid log(0)\n",
        "    epsilon = 1e-8\n",
        "    probs = probs + epsilon\n",
        "    entropy = -torch.sum(probs * torch.log(probs), dim=1)\n",
        "    return entropy\n",
        "\n",
        "def calculate_ece(confidences, accuracies, n_bins=10):\n",
        "    \"\"\"Calculate Expected Calibration Error.\"\"\"\n",
        "    bin_boundaries = torch.linspace(0, 1, n_bins + 1)\n",
        "    bin_lowers = bin_boundaries[:-1]\n",
        "    bin_uppers = bin_boundaries[1:]\n",
        "\n",
        "    ece = 0\n",
        "    for bin_lower, bin_upper in zip(bin_lowers, bin_uppers):\n",
        "        in_bin = (confidences > bin_lower.item()) & (confidences <= bin_upper.item())\n",
        "        prop_in_bin = in_bin.float().mean()\n",
        "\n",
        "        if prop_in_bin.item() > 0:\n",
        "            accuracy_in_bin = accuracies[in_bin].float().mean()\n",
        "            avg_confidence_in_bin = confidences[in_bin].mean()\n",
        "            ece += torch.abs(avg_confidence_in_bin - accuracy_in_bin) * prop_in_bin\n",
        "\n",
        "    return ece.item()\n",
        "\n",
        "def comprehensive_evaluation(model, test_loader, criterion, device):\n",
        "    \"\"\"Comprehensive evaluation with all requested metrics.\"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    all_predictions = []\n",
        "    all_labels = []\n",
        "    all_confidences = []\n",
        "    all_entropies = []\n",
        "    all_top_k_results = {1: [], 3: [], 5: []}\n",
        "    total_loss = 0.0\n",
        "    total_samples = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in tqdm(test_loader, desc=\"Evaluating\"):\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            # Convert outputs to probabilities\n",
        "            probs = torch.softmax(outputs, dim=1)\n",
        "\n",
        "            # Get predictions and confidences\n",
        "            confidences, predictions = torch.max(probs, dim=1)\n",
        "\n",
        "            # Calculate entropy\n",
        "            entropies = calculate_entropy(probs)\n",
        "\n",
        "            # Calculate top-k accuracies\n",
        "            top_k_accs = calculate_top_k_accuracy(outputs, labels, [1, 3, 5])\n",
        "\n",
        "            # Store results\n",
        "            all_predictions.extend(predictions.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "            all_confidences.extend(confidences.cpu().numpy())\n",
        "            all_entropies.extend(entropies.cpu().numpy())\n",
        "\n",
        "            for k in [1, 3, 5]:\n",
        "                all_top_k_results[k].extend([top_k_accs[k]] * labels.size(0))\n",
        "\n",
        "            total_loss += loss.item() * images.size(0)\n",
        "            total_samples += images.size(0)\n",
        "\n",
        "    # Convert to numpy arrays\n",
        "    all_predictions = np.array(all_predictions)\n",
        "    all_labels = np.array(all_labels)\n",
        "    all_confidences = np.array(all_confidences)\n",
        "    all_entropies = np.array(all_entropies)\n",
        "\n",
        "    # Calculate basic metrics\n",
        "    avg_loss = total_loss / total_samples\n",
        "\n",
        "    # Calculate top-k accuracies\n",
        "    top1 = np.mean([pred == label for pred, label in zip(all_predictions, all_labels)])\n",
        "    top3 = np.mean(all_top_k_results[3])\n",
        "    top5 = np.mean(all_top_k_results[5])\n",
        "\n",
        "    # Calculate precision, recall, f1-score\n",
        "    precision_macro, recall_macro, f1_macro, _ = precision_recall_fscore_support(\n",
        "        all_labels, all_predictions, average='macro', zero_division=0\n",
        "    )\n",
        "    precision_weighted, recall_weighted, f1_weighted, _ = precision_recall_fscore_support(\n",
        "        all_labels, all_predictions, average='weighted', zero_division=0\n",
        "    )\n",
        "\n",
        "    # Calculate confidence and calibration metrics\n",
        "    avg_confidence = np.mean(all_confidences)\n",
        "    std_confidence = np.std(all_confidences)\n",
        "    avg_entropy = np.mean(all_entropies)\n",
        "\n",
        "    # Calculate ECE\n",
        "    accuracies = (all_predictions == all_labels).astype(float)\n",
        "    ece = calculate_ece(torch.tensor(all_confidences), torch.tensor(accuracies))\n",
        "\n",
        "    return {\n",
        "        'test_loss': avg_loss,\n",
        "        'top1': top1,\n",
        "        'top3': top3,\n",
        "        'top5': top5,\n",
        "        'precision_macro': precision_macro,\n",
        "        'recall_macro': recall_macro,\n",
        "        'f1_macro': f1_macro,\n",
        "        'precision_weighted': precision_weighted,\n",
        "        'recall_weighted': recall_weighted,\n",
        "        'f1_weighted': f1_weighted,\n",
        "        'avg_confidence': avg_confidence,\n",
        "        'std_confidence': std_confidence,\n",
        "        'avg_entropy': avg_entropy,\n",
        "        'ece': ece\n",
        "    }\n",
        "\n",
        "# ==========================================\n",
        "# Main Execution\n",
        "# ==========================================\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # Set global seeds for full reproducibility\n",
        "    torch.manual_seed(CONFIG[\"split_seed\"])\n",
        "    np.random.seed(CONFIG[\"split_seed\"])\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(CONFIG[\"split_seed\"])\n",
        "        torch.cuda.manual_seed_all(CONFIG[\"split_seed\"])\n",
        "        # Ensure deterministic behavior on CUDA\n",
        "        torch.backends.cudnn.deterministic = True\n",
        "        torch.backends.cudnn.benchmark = False\n",
        "\n",
        "    print(f\"Using device: {CONFIG['device']}\")\n",
        "    print(f\"Reproducibility seed: {CONFIG['split_seed']}\")\n",
        "\n",
        "    # Get data loaders\n",
        "    dataloaders = get_dataloaders(CONFIG)\n",
        "    if dataloaders is None:\n",
        "        print(\"Could not prepare data. Halting execution.\", file=sys.stderr)\n",
        "        sys.exit(1)  # Exit if data preparation failed\n",
        "\n",
        "    train_loader, val_loader, test_loader = dataloaders\n",
        "\n",
        "    # Initialize and train model\n",
        "    model = ResNetTransformer18(num_classes=CONFIG[\"num_classes\"], t_config=TRANSFORMER_CONFIG)\n",
        "    trained_model = train_and_validate(model, train_loader, val_loader, CONFIG)\n",
        "\n",
        "    # Comprehensive evaluation on the test set\n",
        "    print(\"--- Starting Comprehensive Evaluation on Test Set ---\")\n",
        "    results = comprehensive_evaluation(trained_model, test_loader, nn.CrossEntropyLoss(), CONFIG[\"device\"])\n",
        "\n",
        "    # Print all requested metrics\n",
        "    print(\"\\n--- Top-K Accuracy Results ---\")\n",
        "    print(f\"Top-1 Accuracy: {results['top1'] * 100:.2f}%\")\n",
        "    print(f\"Top-3 Accuracy: {results['top3'] * 100:.2f}%\")\n",
        "    print(f\"Top-5 Accuracy: {results['top5'] * 100:.2f}%\")\n",
        "\n",
        "    print(\"\\n--- Additional Performance Metrics ---\")\n",
        "    print(f\"Macro Average Precision: {results['precision_macro']:.4f}\")\n",
        "    print(f\"Macro Average Recall: {results['recall_macro']:.4f}\")\n",
        "    print(f\"Macro Average F1-Score: {results['f1_macro']:.4f}\")\n",
        "    print(f\"Weighted Average Precision: {results['precision_weighted']:.4f}\")\n",
        "    print(f\"Weighted Average Recall: {results['recall_weighted']:.4f}\")\n",
        "    print(f\"Weighted Average F1-Score: {results['f1_weighted']:.4f}\")\n",
        "\n",
        "    print(\"\\n--- Model Confidence & Calibration Metrics ---\")\n",
        "    print(f\"Average Prediction Confidence: {results['avg_confidence']:.4f}\")\n",
        "    print(f\"Confidence Standard Deviation: {results['std_confidence']:.4f}\")\n",
        "    print(f\"Average Prediction Entropy: {results['avg_entropy']:.4f}\")\n",
        "    print(f\"Expected Calibration Error: {results['ece']:.4f}\")\n",
        "\n",
        "    print(f\"\\nFinal Test Loss: {results['test_loss']:.4f}\")\n",
        "    print(\"===============================================\")"
      ],
      "metadata": {
        "id": "igAyIeYUpMC2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c954fdf4-056d-4715-9652-19c66b74c05f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Reproducibility seed: 42\n",
            "Loading metadata from: ./pad-ufes-20/metadata.csv\n",
            "Total samples: 2298\n",
            "Diagnostic classes: diagnostic\n",
            "BCC    845\n",
            "ACK    730\n",
            "NEV    244\n",
            "SEK    235\n",
            "SCC    192\n",
            "MEL     52\n",
            "Name: count, dtype: int64\n",
            "✅ Dataset verification complete\n",
            "Found 6 classes: ['ACK', 'BCC', 'MEL', 'NEV', 'SCC', 'SEK']\n",
            "Dataset splits - Train: 1608, Val: 344, Test: 346, Total: 2298\n",
            "Found 6 classes: ['ACK', 'BCC', 'MEL', 'NEV', 'SCC', 'SEK']\n",
            "Found 6 classes: ['ACK', 'BCC', 'MEL', 'NEV', 'SCC', 'SEK']\n",
            "Found 6 classes: ['ACK', 'BCC', 'MEL', 'NEV', 'SCC', 'SEK']\n",
            "\n",
            "--- Starting Training ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100 | Time: 75.83s | Train Loss: 1.5568, Train Acc: 37.69% | Val Loss: 1.4853, Val Acc: 39.83%\n",
            "\n",
            "Model saved to ./best_model_pad_ufes.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/100 | Time: 76.14s | Train Loss: 1.4967, Train Acc: 37.31% | Val Loss: 1.4784, Val Acc: 36.34%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/100 | Time: 76.21s | Train Loss: 1.5075, Train Acc: 33.46% | Val Loss: 1.4731, Val Acc: 36.34%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4/100 | Time: 75.53s | Train Loss: 1.4852, Train Acc: 37.44% | Val Loss: 1.4105, Val Acc: 47.97%\n",
            "\n",
            "Model saved to ./best_model_pad_ufes.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5/100 | Time: 76.02s | Train Loss: 1.4840, Train Acc: 38.18% | Val Loss: 1.4612, Val Acc: 35.47%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6/100 | Time: 75.26s | Train Loss: 1.4579, Train Acc: 37.75% | Val Loss: 1.4297, Val Acc: 40.41%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7/100 | Time: 75.02s | Train Loss: 1.4435, Train Acc: 38.50% | Val Loss: 1.4247, Val Acc: 42.44%\n",
            "\n",
            "Reducing learning rate from 1.00e-03 to 2.00e-04\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8/100 | Time: 76.09s | Train Loss: 1.4559, Train Acc: 38.81% | Val Loss: 1.4201, Val Acc: 42.73%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9/100 | Time: 75.82s | Train Loss: 1.4322, Train Acc: 39.18% | Val Loss: 1.4250, Val Acc: 40.70%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10/100 | Time: 76.25s | Train Loss: 1.4129, Train Acc: 38.74% | Val Loss: 1.4322, Val Acc: 43.90%\n",
            "\n",
            "Reducing learning rate from 2.00e-04 to 4.00e-05\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 11/100 | Time: 76.60s | Train Loss: 1.4064, Train Acc: 41.54% | Val Loss: 1.3808, Val Acc: 44.19%\n",
            "\n",
            "Early stopping at epoch 11\n",
            "Restored model weights from the end of the best epoch: 4\n",
            "--- Training Finished ---\n",
            "\n",
            "--- Starting Comprehensive Evaluation on Test Set ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating: 100%|██████████| 6/6 [00:12<00:00,  2.06s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Top-K Accuracy Results ---\n",
            "Top-1 Accuracy: 44.51%\n",
            "Top-3 Accuracy: 84.97%\n",
            "Top-5 Accuracy: 98.84%\n",
            "\n",
            "--- Additional Performance Metrics ---\n",
            "Macro Average Precision: 0.1954\n",
            "Macro Average Recall: 0.2172\n",
            "Macro Average F1-Score: 0.1941\n",
            "Weighted Average Precision: 0.3527\n",
            "Weighted Average Recall: 0.4451\n",
            "Weighted Average F1-Score: 0.3787\n",
            "\n",
            "--- Model Confidence & Calibration Metrics ---\n",
            "Average Prediction Confidence: 0.3865\n",
            "Confidence Standard Deviation: 0.0523\n",
            "Average Prediction Entropy: 1.4136\n",
            "Expected Calibration Error: 0.0586\n",
            "\n",
            "Final Test Loss: 1.3770\n",
            "===============================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hybrid Transformer on ResNet-18 (Learnable PE)"
      ],
      "metadata": {
        "id": "16k7buIdqY5W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "import torchvision.transforms as transforms\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "import os\n",
        "import requests\n",
        "import zipfile\n",
        "import shutil\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "import sys\n",
        "import numpy as np\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# --- Configuration Parameters ---\n",
        "CONFIG = {\n",
        "    \"data_path\": \"./pad-ufes-20/\",\n",
        "    \"dataset_url\": \"https://www.kaggle.com/api/v1/datasets/download/maxjen/pad-ufes-20\",  # Kaggle API URL\n",
        "    \"batch_size\": 64,  # Reduced due to smaller dataset\n",
        "    \"num_epochs\": 100,\n",
        "    \"learning_rate\": 0.001,\n",
        "    \"weight_decay\": 1e-4,\n",
        "    \"split_seed\": 42,\n",
        "    \"train_split\": 0.70,\n",
        "    \"val_split\": 0.15,\n",
        "    \"test_split\": 0.15,\n",
        "    \"num_classes\": 6,  # PAD-UFES-20 has 6 diagnostic classes\n",
        "    \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
        "    \"checkpoint_path\": \"./best_model_pad_ufes.pth\",\n",
        "}\n",
        "\n",
        "# --- Transformer Configuration ---\n",
        "TRANSFORMER_CONFIG = {\n",
        "    \"embedding_dim\": 256,       # Dimension of the tokens fed to the transformer\n",
        "    \"nhead\": 8,                 # Number of attention heads\n",
        "    \"num_encoder_layers\": 3,    # Number of transformer encoder layers\n",
        "    \"dim_feedforward\": 512,     # Hidden dimension in the feed-forward network\n",
        "    \"dropout\": 0.1,\n",
        "}\n",
        "\n",
        "# ==========================================\n",
        "# Callback Classes\n",
        "# ==========================================\n",
        "\n",
        "class Callback:\n",
        "    \"\"\"Base callback class\"\"\"\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        pass\n",
        "\n",
        "    def on_training_end(self):\n",
        "        pass\n",
        "\n",
        "class ReduceLROnPlateau(Callback):\n",
        "    \"\"\"Reduce learning rate when a metric has stopped improving\"\"\"\n",
        "    def __init__(self, optimizer, monitor='val_accuracy', factor=0.2, patience=3, min_lr=1e-7, verbose=1):\n",
        "        self.optimizer = optimizer\n",
        "        self.monitor = monitor\n",
        "        self.factor = factor\n",
        "        self.patience = patience\n",
        "        self.min_lr = min_lr\n",
        "        self.verbose = verbose\n",
        "        self.wait = 0\n",
        "        self.best = None\n",
        "        self.mode = 'min' if 'loss' in monitor else 'max'\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        if logs is None:\n",
        "            return\n",
        "\n",
        "        current = logs.get(self.monitor)\n",
        "        if current is None:\n",
        "            return\n",
        "\n",
        "        if self.best is None:\n",
        "            self.best = current\n",
        "            return\n",
        "\n",
        "        if self.mode == 'min':\n",
        "            improved = current < self.best\n",
        "        else:\n",
        "            improved = current > self.best\n",
        "\n",
        "        if improved:\n",
        "            self.best = current\n",
        "            self.wait = 0\n",
        "        else:\n",
        "            self.wait += 1\n",
        "            if self.wait >= self.patience:\n",
        "                old_lr = self.optimizer.param_groups[0]['lr']\n",
        "                new_lr = max(old_lr * self.factor, self.min_lr)\n",
        "                if new_lr != old_lr:\n",
        "                    self.optimizer.param_groups[0]['lr'] = new_lr\n",
        "                    if self.verbose:\n",
        "                        print(f\"\\nReducing learning rate from {old_lr:.2e} to {new_lr:.2e}\")\n",
        "                    self.wait = 0\n",
        "\n",
        "class EarlyStopping(Callback):\n",
        "    \"\"\"Stop training when a monitored metric has stopped improving\"\"\"\n",
        "    def __init__(self, monitor='val_accuracy', patience=7, restore_best_weights=True, verbose=1):\n",
        "        self.monitor = monitor\n",
        "        self.patience = patience\n",
        "        self.restore_best_weights = restore_best_weights\n",
        "        self.verbose = verbose\n",
        "        self.wait = 0\n",
        "        self.best = None\n",
        "        self.best_weights = None\n",
        "        self.mode = 'min' if 'loss' in monitor else 'max'\n",
        "        self.stopped_epoch = 0\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None, model=None):\n",
        "        if logs is None:\n",
        "            return False\n",
        "\n",
        "        current = logs.get(self.monitor)\n",
        "        if current is None:\n",
        "            return False\n",
        "\n",
        "        if self.best is None:\n",
        "            self.best = current\n",
        "            if model is not None and self.restore_best_weights:\n",
        "                self.best_weights = model.state_dict().copy()\n",
        "            return False\n",
        "\n",
        "        if self.mode == 'min':\n",
        "            improved = current < self.best\n",
        "        else:\n",
        "            improved = current > self.best\n",
        "\n",
        "        if improved:\n",
        "            self.best = current\n",
        "            self.wait = 0\n",
        "            if model is not None and self.restore_best_weights:\n",
        "                self.best_weights = model.state_dict().copy()\n",
        "        else:\n",
        "            self.wait += 1\n",
        "            if self.wait >= self.patience:\n",
        "                self.stopped_epoch = epoch\n",
        "                if self.verbose:\n",
        "                    print(f\"\\nEarly stopping at epoch {epoch + 1}\")\n",
        "                return True\n",
        "        return False\n",
        "\n",
        "    def on_training_end(self, model=None):\n",
        "        if self.stopped_epoch > 0 and self.verbose:\n",
        "            print(f\"Restored model weights from the end of the best epoch: {self.stopped_epoch + 1 - self.patience}\")\n",
        "        if model is not None and self.restore_best_weights and self.best_weights is not None:\n",
        "            model.load_state_dict(self.best_weights)\n",
        "\n",
        "class ModelCheckpoint(Callback):\n",
        "    \"\"\"Save the model after every epoch\"\"\"\n",
        "    def __init__(self, filepath, save_best_only=True, monitor='val_accuracy', verbose=1):\n",
        "        self.filepath = filepath\n",
        "        self.save_best_only = save_best_only\n",
        "        self.monitor = monitor\n",
        "        self.verbose = verbose\n",
        "        self.best = None\n",
        "        self.mode = 'min' if 'loss' in monitor else 'max'\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None, model=None):\n",
        "        if logs is None or model is None:\n",
        "            return\n",
        "\n",
        "        current = logs.get(self.monitor)\n",
        "        if current is None:\n",
        "            return\n",
        "\n",
        "        if not self.save_best_only:\n",
        "            filepath = self.filepath.replace('.pth', f'_epoch_{epoch + 1}.pth')\n",
        "            torch.save(model.state_dict(), filepath)\n",
        "            if self.verbose:\n",
        "                print(f\"\\nModel saved to {filepath}\")\n",
        "            return\n",
        "\n",
        "        if self.best is None:\n",
        "            self.best = current\n",
        "            torch.save(model.state_dict(), self.filepath)\n",
        "            if self.verbose:\n",
        "                print(f\"\\nModel saved to {self.filepath}\")\n",
        "            return\n",
        "\n",
        "        if self.mode == 'min':\n",
        "            improved = current < self.best\n",
        "        else:\n",
        "            improved = current > self.best\n",
        "\n",
        "        if improved:\n",
        "            self.best = current\n",
        "            torch.save(model.state_dict(), self.filepath)\n",
        "            if self.verbose:\n",
        "                print(f\"\\nModel saved to {self.filepath}\")\n",
        "\n",
        "# ==========================================\n",
        "# Data Preprocessing\n",
        "# ==========================================\n",
        "\n",
        "def download_and_extract_pad_ufes(url, dest_path):\n",
        "    \"\"\"Downloads and extracts PAD-UFES-20 dataset from Kaggle with progress and robust error handling.\"\"\"\n",
        "    if os.path.exists(dest_path):\n",
        "        print(\"Dataset directory already exists.\")\n",
        "        return True\n",
        "\n",
        "    zip_path = dest_path.rstrip('/') + \".zip\"\n",
        "    extract_to_dir = os.path.abspath(os.path.join(dest_path, os.pardir))\n",
        "\n",
        "    print(f\"Downloading PAD-UFES-20 dataset from Kaggle...\")\n",
        "    try:\n",
        "        # Add headers to mimic browser request\n",
        "        headers = {\n",
        "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
        "        }\n",
        "\n",
        "        with requests.get(url, stream=True, timeout=60, headers=headers) as r:\n",
        "            r.raise_for_status()\n",
        "            total_size = int(r.headers.get('content-length', 0))\n",
        "            with open(zip_path, 'wb') as f, tqdm(\n",
        "                total=total_size, unit='iB', unit_scale=True, desc=\"PAD-UFES-20\"\n",
        "            ) as progress_bar:\n",
        "                for chunk in r.iter_content(chunk_size=8192):\n",
        "                    f.write(chunk)\n",
        "                    progress_bar.update(len(chunk))\n",
        "\n",
        "        print(\"Extracting...\")\n",
        "        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "            zip_ref.extractall(extract_to_dir)\n",
        "\n",
        "        # Check if extraction created nested folders and fix if needed\n",
        "        extracted_folders = [f for f in os.listdir(extract_to_dir) if os.path.isdir(os.path.join(extract_to_dir, f))]\n",
        "        if 'pad-ufes-20' in extracted_folders:\n",
        "            # Dataset extracted correctly\n",
        "            pass\n",
        "        else:\n",
        "            # Look for the actual dataset folder\n",
        "            for folder in extracted_folders:\n",
        "                folder_path = os.path.join(extract_to_dir, folder)\n",
        "                if os.path.exists(os.path.join(folder_path, 'metadata.csv')):\n",
        "                    # This is our dataset folder, rename it\n",
        "                    os.rename(folder_path, dest_path)\n",
        "                    break\n",
        "\n",
        "        return True\n",
        "\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"\\nError downloading file: {e}\", file=sys.stderr)\n",
        "        print(\"Please download manually from: https://www.kaggle.com/datasets/maxjen/pad-ufes-20\", file=sys.stderr)\n",
        "        return False\n",
        "\n",
        "    finally:\n",
        "        if os.path.exists(zip_path):\n",
        "            os.remove(zip_path)\n",
        "\n",
        "class PADUFESDataset(Dataset):\n",
        "    \"\"\"Custom dataset class for PAD-UFES-20.\"\"\"\n",
        "    def __init__(self, data_dir, metadata_df, transform=None):\n",
        "        self.data_dir = data_dir\n",
        "        self.metadata_df = metadata_df.reset_index(drop=True)\n",
        "        self.transform = transform\n",
        "\n",
        "        # Create label mapping\n",
        "        self.classes = sorted(metadata_df['diagnostic'].unique())\n",
        "        self.class_to_idx = {cls_name: idx for idx, cls_name in enumerate(self.classes)}\n",
        "\n",
        "        print(f\"Found {len(self.classes)} classes: {self.classes}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.metadata_df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.metadata_df.iloc[idx]\n",
        "\n",
        "        # Handle different possible image path formats\n",
        "        img_name = row['img_id']\n",
        "        if not img_name.endswith('.png'):\n",
        "            img_name += '.png'\n",
        "\n",
        "        # Try different possible folder structures\n",
        "        possible_paths = [\n",
        "            os.path.join(self.data_dir, 'images', img_name),\n",
        "            os.path.join(self.data_dir, 'imgs', img_name),\n",
        "            os.path.join(self.data_dir, img_name),\n",
        "        ]\n",
        "\n",
        "        img_path = None\n",
        "        for path in possible_paths:\n",
        "            if os.path.exists(path):\n",
        "                img_path = path\n",
        "                break\n",
        "\n",
        "        if img_path is None:\n",
        "            # Last resort: search for the file\n",
        "            for root, dirs, files in os.walk(self.data_dir):\n",
        "                if img_name in files:\n",
        "                    img_path = os.path.join(root, img_name)\n",
        "                    break\n",
        "\n",
        "        if img_path is None:\n",
        "            print(f\"Warning: Image {img_name} not found, using black placeholder\")\n",
        "            image = Image.new('RGB', (224, 224), color='black')\n",
        "        else:\n",
        "            try:\n",
        "                image = Image.open(img_path).convert('RGB')\n",
        "            except Exception as e:\n",
        "                print(f\"Error loading image {img_path}: {e}\")\n",
        "                image = Image.new('RGB', (224, 224), color='black')\n",
        "\n",
        "        label = self.class_to_idx[row['diagnostic']]\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, label\n",
        "\n",
        "def prepare_pad_ufes_data(data_path):\n",
        "    \"\"\"Prepare PAD-UFES-20 data by reading metadata and organizing structure.\"\"\"\n",
        "\n",
        "    # Try different possible metadata file locations\n",
        "    possible_metadata_paths = [\n",
        "        os.path.join(data_path, 'metadata.csv'),\n",
        "        os.path.join(data_path, 'PAD-UFES-20_metadata.csv'),\n",
        "    ]\n",
        "\n",
        "    metadata_path = None\n",
        "    for path in possible_metadata_paths:\n",
        "        if os.path.exists(path):\n",
        "            metadata_path = path\n",
        "            break\n",
        "\n",
        "    if metadata_path is None:\n",
        "        # Search for any CSV file that might be the metadata\n",
        "        for root, dirs, files in os.walk(data_path):\n",
        "            for file in files:\n",
        "                if file.endswith('.csv') and 'metadata' in file.lower():\n",
        "                    metadata_path = os.path.join(root, file)\n",
        "                    break\n",
        "            if metadata_path:\n",
        "                break\n",
        "\n",
        "    if metadata_path is None:\n",
        "        print(f\"Metadata file not found in {data_path}\")\n",
        "        return None\n",
        "\n",
        "    print(f\"Loading metadata from: {metadata_path}\")\n",
        "    try:\n",
        "        metadata_df = pd.read_csv(metadata_path)\n",
        "    except Exception as e:\n",
        "        print(f\"Error reading metadata file: {e}\")\n",
        "        return None\n",
        "\n",
        "    # Print dataset info\n",
        "    print(f\"Total samples: {len(metadata_df)}\")\n",
        "    print(f\"Diagnostic classes: {metadata_df['diagnostic'].value_counts()}\")\n",
        "\n",
        "    # Verify dataset integrity\n",
        "    expected_samples = 2298\n",
        "    expected_classes = 6\n",
        "\n",
        "    actual_classes = len(metadata_df['diagnostic'].unique())\n",
        "    if len(metadata_df) != expected_samples:\n",
        "        print(f\"⚠️  Warning: Expected {expected_samples} samples, got {len(metadata_df)}\")\n",
        "    if actual_classes != expected_classes:\n",
        "        print(f\"⚠️  Warning: Expected {expected_classes} classes, got {actual_classes}\")\n",
        "\n",
        "    print(\"✅ Dataset verification complete\")\n",
        "    return metadata_df\n",
        "\n",
        "def get_dataloaders(config):\n",
        "    \"\"\"Downloads, prepares, and splits the PAD-UFES-20 data, returning DataLoaders.\"\"\"\n",
        "\n",
        "    # Try to download if URL is provided and dataset doesn't exist\n",
        "    if config.get(\"dataset_url\") and not os.path.exists(config[\"data_path\"]):\n",
        "        if not download_and_extract_pad_ufes(config[\"dataset_url\"], config[\"data_path\"]):\n",
        "            print(\"\\nAutomatic download failed. Please download manually:\")\n",
        "            print(\"1. Visit: https://www.kaggle.com/datasets/maxjen/pad-ufes-20\")\n",
        "            print(\"2. Download the dataset\")\n",
        "            print(\"3. Extract to:\", config[\"data_path\"])\n",
        "            return None\n",
        "\n",
        "    # Check if dataset exists\n",
        "    if not os.path.exists(config[\"data_path\"]):\n",
        "        print(f\"\\nDataset not found at {config['data_path']}\")\n",
        "        print(\"Please download the PAD-UFES-20 dataset:\")\n",
        "        print(\"1. Visit: https://www.kaggle.com/datasets/maxjen/pad-ufes-20\")\n",
        "        print(\"2. Download the dataset\")\n",
        "        print(\"3. Extract to:\", config[\"data_path\"])\n",
        "        print(\"4. Ensure the structure includes metadata.csv and image files\")\n",
        "        return None\n",
        "\n",
        "    # Prepare metadata\n",
        "    metadata_df = prepare_pad_ufes_data(config[\"data_path\"])\n",
        "    if metadata_df is None:\n",
        "        return None\n",
        "\n",
        "    # Define transforms\n",
        "    train_transform = transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.RandomHorizontalFlip(p=0.5),\n",
        "        transforms.RandomRotation(degrees=10),\n",
        "        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
        "        transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "    ])\n",
        "\n",
        "    val_test_transform = transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "    ])\n",
        "\n",
        "    # Create full dataset\n",
        "    full_dataset = PADUFESDataset(config[\"data_path\"], metadata_df, transform=None)\n",
        "\n",
        "    # Ensure reproducible splits\n",
        "    torch.manual_seed(config[\"split_seed\"])\n",
        "    np.random.seed(config[\"split_seed\"])\n",
        "\n",
        "    total_size = len(full_dataset)\n",
        "    train_size = int(config[\"train_split\"] * total_size)\n",
        "    val_size = int(config[\"val_split\"] * total_size)\n",
        "    test_size = int(config[\"test_split\"] * total_size)\n",
        "\n",
        "    # Adjust sizes to ensure they sum to total_size\n",
        "    actual_total = train_size + val_size + test_size\n",
        "    if actual_total != total_size:\n",
        "        test_size += (total_size - actual_total)\n",
        "\n",
        "    print(f\"Dataset splits - Train: {train_size}, Val: {val_size}, Test: {test_size}, Total: {total_size}\")\n",
        "\n",
        "    # Create indices for splitting\n",
        "    indices = list(range(total_size))\n",
        "    np.random.shuffle(indices)\n",
        "\n",
        "    train_indices = indices[:train_size]\n",
        "    val_indices = indices[train_size:train_size + val_size]\n",
        "    test_indices = indices[train_size + val_size:]\n",
        "\n",
        "    # Create subset datasets with appropriate transforms\n",
        "    train_dataset = PADUFESDataset(\n",
        "        config[\"data_path\"],\n",
        "        metadata_df.iloc[train_indices],\n",
        "        transform=train_transform\n",
        "    )\n",
        "    val_dataset = PADUFESDataset(\n",
        "        config[\"data_path\"],\n",
        "        metadata_df.iloc[val_indices],\n",
        "        transform=val_test_transform\n",
        "    )\n",
        "    test_dataset = PADUFESDataset(\n",
        "        config[\"data_path\"],\n",
        "        metadata_df.iloc[test_indices],\n",
        "        transform=val_test_transform\n",
        "    )\n",
        "\n",
        "    # Set worker init function for reproducible DataLoader behavior\n",
        "    def worker_init_fn(worker_id):\n",
        "        np.random.seed(config[\"split_seed\"] + worker_id)\n",
        "\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=config[\"batch_size\"],\n",
        "        shuffle=True,\n",
        "        num_workers=2,\n",
        "        pin_memory=True,\n",
        "        worker_init_fn=worker_init_fn,\n",
        "        generator=torch.Generator().manual_seed(config[\"split_seed\"])\n",
        "    )\n",
        "    val_loader = DataLoader(\n",
        "        val_dataset,\n",
        "        batch_size=config[\"batch_size\"],\n",
        "        shuffle=False,\n",
        "        num_workers=2,\n",
        "        pin_memory=True,\n",
        "        worker_init_fn=worker_init_fn\n",
        "    )\n",
        "    test_loader = DataLoader(\n",
        "        test_dataset,\n",
        "        batch_size=config[\"batch_size\"],\n",
        "        shuffle=False,\n",
        "        num_workers=2,\n",
        "        pin_memory=True,\n",
        "        worker_init_fn=worker_init_fn\n",
        "    )\n",
        "\n",
        "    return train_loader, val_loader, test_loader\n",
        "\n",
        "# ==========================================\n",
        "# Model Architecture and Training\n",
        "# ==========================================\n",
        "\n",
        "class NonResidualBlock(nn.Module):\n",
        "    \"\"\"A standard convolutional block WITHOUT the residual connection.\"\"\"\n",
        "    def __init__(self, in_channels, out_channels, stride=1):\n",
        "        super(NonResidualBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        out = self.relu(out)\n",
        "        return out\n",
        "\n",
        "class ResNetTransformer(nn.Module):\n",
        "    \"\"\"\n",
        "    A hybrid architecture combining a non-residual CNN backbone with a Transformer encoder,\n",
        "    including positional embeddings.\n",
        "    \"\"\"\n",
        "    def __init__(self, block, layers, num_classes, t_config):\n",
        "        super(ResNetTransformer, self).__init__()\n",
        "        self.in_channels = 64\n",
        "        self.embedding_dim = t_config[\"embedding_dim\"]\n",
        "        self.num_tokens = len(layers) + 1  # 4 layers + 1 initial capture\n",
        "\n",
        "        # 1. CNN Backbone (Feature Extractor)\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "        self.layer1 = self._make_layer(block, 64, layers[0], stride=1)\n",
        "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n",
        "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n",
        "\n",
        "        # 2. Projection heads to create tokens from feature maps\n",
        "        self.projections = nn.ModuleList([\n",
        "            self._create_projection(64, self.embedding_dim),\n",
        "            self._create_projection(64, self.embedding_dim),\n",
        "            self._create_projection(128, self.embedding_dim),\n",
        "            self._create_projection(256, self.embedding_dim),\n",
        "            self._create_projection(512, self.embedding_dim)\n",
        "        ])\n",
        "\n",
        "        # 3. Learnable Positional Embedding\n",
        "        self.positional_embedding = nn.Parameter(torch.randn(1, self.num_tokens, self.embedding_dim))\n",
        "\n",
        "        # 4. Transformer Encoder\n",
        "        encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=self.embedding_dim,\n",
        "            nhead=t_config[\"nhead\"],\n",
        "            dim_feedforward=t_config[\"dim_feedforward\"],\n",
        "            dropout=t_config[\"dropout\"],\n",
        "            batch_first=True\n",
        "        )\n",
        "        self.transformer_encoder = nn.TransformerEncoder(\n",
        "            encoder_layer,\n",
        "            num_layers=t_config[\"num_encoder_layers\"]\n",
        "        )\n",
        "\n",
        "        # 5. Final Classifier\n",
        "        self.classifier = nn.Linear(self.embedding_dim, num_classes)\n",
        "\n",
        "    def _create_projection(self, in_features, out_features):\n",
        "        return nn.Sequential(\n",
        "            nn.AdaptiveAvgPool2d((1, 1)),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(in_features, out_features)\n",
        "        )\n",
        "\n",
        "    def _make_layer(self, block, out_channels, num_blocks, stride):\n",
        "        strides = [stride] + [1] * (num_blocks - 1)\n",
        "        layers = []\n",
        "        current_in_channels = self.in_channels\n",
        "        for s in strides:\n",
        "            layers.append(block(current_in_channels, out_channels, s))\n",
        "            current_in_channels = out_channels\n",
        "        self.in_channels = out_channels\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # 1. Pass through CNN backbone and capture features\n",
        "        features = []\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "        features.append(x)  # Capture 1: After initial maxpool\n",
        "\n",
        "        x = self.layer1(x); features.append(x)  # Capture 2: After layer1\n",
        "        x = self.layer2(x); features.append(x)  # Capture 3: After layer2\n",
        "        x = self.layer3(x); features.append(x)  # Capture 4: After layer3\n",
        "        x = self.layer4(x); features.append(x)  # Capture 5: After layer4\n",
        "\n",
        "        # 2. Project features to tokens\n",
        "        tokens = [self.projections[i](feature_map) for i, feature_map in enumerate(features)]\n",
        "\n",
        "        # 3. Stack tokens into a sequence\n",
        "        token_sequence = torch.stack(tokens, dim=1)\n",
        "\n",
        "        # 4. Add positional embedding\n",
        "        token_sequence += self.positional_embedding\n",
        "\n",
        "        # 5. Pass through Transformer\n",
        "        transformer_out = self.transformer_encoder(token_sequence)\n",
        "\n",
        "        # 6. Aggregate and classify\n",
        "        aggregated_vector = transformer_out.mean(dim=1)\n",
        "        logits = self.classifier(aggregated_vector)\n",
        "\n",
        "        return logits\n",
        "\n",
        "def evaluate_model(model, data_loader, criterion, device):\n",
        "    \"\"\"Evaluates the model on a given dataset.\"\"\"\n",
        "    model.eval()\n",
        "    total_loss, correct, total = 0.0, 0, 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in data_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            total_loss += loss.item() * images.size(0)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    avg_loss = total_loss / total\n",
        "    accuracy = 100 * correct / total\n",
        "    return avg_loss, accuracy\n",
        "\n",
        "def train_and_validate(model, train_loader, val_loader, config):\n",
        "    \"\"\"Main training loop with callbacks.\"\"\"\n",
        "    device = config[\"device\"]\n",
        "    model.to(device)\n",
        "\n",
        "    # Set seeds for reproducible training\n",
        "    torch.manual_seed(config[\"split_seed\"])\n",
        "    np.random.seed(config[\"split_seed\"])\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(config[\"split_seed\"])\n",
        "        torch.cuda.manual_seed_all(config[\"split_seed\"])\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=config[\"learning_rate\"], weight_decay=config[\"weight_decay\"])\n",
        "\n",
        "    # Initialize callbacks\n",
        "    callbacks = [\n",
        "        ReduceLROnPlateau(\n",
        "            optimizer=optimizer,\n",
        "            monitor='val_accuracy',\n",
        "            factor=0.2,\n",
        "            patience=3,\n",
        "            min_lr=1e-7,\n",
        "            verbose=1\n",
        "        ),\n",
        "        EarlyStopping(\n",
        "            monitor='val_accuracy',\n",
        "            patience=7,\n",
        "            restore_best_weights=True,\n",
        "            verbose=1\n",
        "        ),\n",
        "        ModelCheckpoint(\n",
        "            filepath=config[\"checkpoint_path\"],\n",
        "            save_best_only=True,\n",
        "            monitor='val_accuracy',\n",
        "            verbose=1\n",
        "        )\n",
        "    ]\n",
        "\n",
        "    print(\"\\n--- Starting Training ---\")\n",
        "    early_stop = False\n",
        "\n",
        "    for epoch in range(config[\"num_epochs\"]):\n",
        "        if early_stop:\n",
        "            break\n",
        "\n",
        "        model.train()\n",
        "        start_time = time.time()\n",
        "        running_loss, train_correct, train_total = 0.0, 0, 0\n",
        "\n",
        "        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{config['num_epochs']} [Train]\", leave=False)\n",
        "        for images, labels in progress_bar:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item() * images.size(0)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            train_total += labels.size(0)\n",
        "            train_correct += (predicted == labels).sum().item()\n",
        "            progress_bar.set_postfix(loss=loss.item())\n",
        "\n",
        "        train_loss = running_loss / train_total\n",
        "        train_acc = 100 * train_correct / train_total\n",
        "        val_loss, val_acc = evaluate_model(model, val_loader, criterion, device)\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{config['num_epochs']} | Time: {time.time() - start_time:.2f}s | \"\n",
        "              f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}% | \"\n",
        "              f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%\")\n",
        "\n",
        "        # Prepare logs for callbacks\n",
        "        logs = {\n",
        "            'train_loss': train_loss,\n",
        "            'train_accuracy': train_acc,\n",
        "            'val_loss': val_loss,\n",
        "            'val_accuracy': val_acc\n",
        "        }\n",
        "\n",
        "        # Execute callbacks\n",
        "        for callback in callbacks:\n",
        "            if isinstance(callback, EarlyStopping):\n",
        "                if callback.on_epoch_end(epoch, logs, model):\n",
        "                    early_stop = True\n",
        "                    break\n",
        "            elif isinstance(callback, ModelCheckpoint):\n",
        "                callback.on_epoch_end(epoch, logs, model)\n",
        "            else:\n",
        "                callback.on_epoch_end(epoch, logs)\n",
        "\n",
        "    # Execute callback cleanup\n",
        "    for callback in callbacks:\n",
        "        if isinstance(callback, EarlyStopping):\n",
        "            callback.on_training_end(model)\n",
        "        else:\n",
        "            callback.on_training_end()\n",
        "\n",
        "    print(\"--- Training Finished ---\\n\")\n",
        "    return model\n",
        "\n",
        "# ==========================================\n",
        "# Evaluation\n",
        "# ==========================================\n",
        "\n",
        "def calculate_top_k_accuracy(outputs, labels, k_values=[1, 3, 5]):\n",
        "    \"\"\"Calculate top-k accuracy for given k values.\"\"\"\n",
        "    batch_size = labels.size(0)\n",
        "    _, pred = outputs.topk(max(k_values), 1, True, True)\n",
        "    pred = pred.t()\n",
        "    correct = pred.eq(labels.view(1, -1).expand_as(pred))\n",
        "\n",
        "    top_k_accuracies = {}\n",
        "    for k in k_values:\n",
        "        correct_k = correct[:k].reshape(-1).float().sum(0, keepdim=True)\n",
        "        top_k_accuracies[k] = correct_k.item() / batch_size\n",
        "\n",
        "    return top_k_accuracies\n",
        "\n",
        "def calculate_entropy(probs):\n",
        "    \"\"\"Calculate entropy of probability distributions.\"\"\"\n",
        "    # Add small epsilon to avoid log(0)\n",
        "    epsilon = 1e-8\n",
        "    probs = probs + epsilon\n",
        "    entropy = -torch.sum(probs * torch.log(probs), dim=1)\n",
        "    return entropy\n",
        "\n",
        "def calculate_ece(confidences, accuracies, n_bins=10):\n",
        "    \"\"\"Calculate Expected Calibration Error.\"\"\"\n",
        "    bin_boundaries = torch.linspace(0, 1, n_bins + 1)\n",
        "    bin_lowers = bin_boundaries[:-1]\n",
        "    bin_uppers = bin_boundaries[1:]\n",
        "\n",
        "    ece = 0\n",
        "    for bin_lower, bin_upper in zip(bin_lowers, bin_uppers):\n",
        "        in_bin = (confidences > bin_lower.item()) & (confidences <= bin_upper.item())\n",
        "        prop_in_bin = in_bin.float().mean()\n",
        "\n",
        "        if prop_in_bin.item() > 0:\n",
        "            accuracy_in_bin = accuracies[in_bin].float().mean()\n",
        "            avg_confidence_in_bin = confidences[in_bin].mean()\n",
        "            ece += torch.abs(avg_confidence_in_bin - accuracy_in_bin) * prop_in_bin\n",
        "\n",
        "    return ece.item()\n",
        "\n",
        "def comprehensive_evaluation(model, test_loader, criterion, device):\n",
        "    \"\"\"Comprehensive evaluation with all requested metrics.\"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    all_predictions = []\n",
        "    all_labels = []\n",
        "    all_confidences = []\n",
        "    all_entropies = []\n",
        "    all_top_k_results = {1: [], 3: [], 5: []}\n",
        "    total_loss = 0.0\n",
        "    total_samples = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in tqdm(test_loader, desc=\"Evaluating\"):\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            # Convert outputs to probabilities\n",
        "            probs = torch.softmax(outputs, dim=1)\n",
        "\n",
        "            # Get predictions and confidences\n",
        "            confidences, predictions = torch.max(probs, dim=1)\n",
        "\n",
        "            # Calculate entropy\n",
        "            entropies = calculate_entropy(probs)\n",
        "\n",
        "            # Calculate top-k accuracies\n",
        "            top_k_accs = calculate_top_k_accuracy(outputs, labels, [1, 3, 5])\n",
        "\n",
        "            # Store results\n",
        "            all_predictions.extend(predictions.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "            all_confidences.extend(confidences.cpu().numpy())\n",
        "            all_entropies.extend(entropies.cpu().numpy())\n",
        "\n",
        "            for k in [1, 3, 5]:\n",
        "                all_top_k_results[k].extend([top_k_accs[k]] * labels.size(0))\n",
        "\n",
        "            total_loss += loss.item() * images.size(0)\n",
        "            total_samples += images.size(0)\n",
        "\n",
        "    # Convert to numpy arrays\n",
        "    all_predictions = np.array(all_predictions)\n",
        "    all_labels = np.array(all_labels)\n",
        "    all_confidences = np.array(all_confidences)\n",
        "    all_entropies = np.array(all_entropies)\n",
        "\n",
        "    # Calculate basic metrics\n",
        "    avg_loss = total_loss / total_samples\n",
        "\n",
        "    # Calculate top-k accuracies\n",
        "    top1 = np.mean([pred == label for pred, label in zip(all_predictions, all_labels)])\n",
        "    top3 = np.mean(all_top_k_results[3])\n",
        "    top5 = np.mean(all_top_k_results[5])\n",
        "\n",
        "    # Calculate precision, recall, f1-score\n",
        "    precision_macro, recall_macro, f1_macro, _ = precision_recall_fscore_support(\n",
        "        all_labels, all_predictions, average='macro', zero_division=0\n",
        "    )\n",
        "    precision_weighted, recall_weighted, f1_weighted, _ = precision_recall_fscore_support(\n",
        "        all_labels, all_predictions, average='weighted', zero_division=0\n",
        "    )\n",
        "\n",
        "    # Calculate confidence and calibration metrics\n",
        "    avg_confidence = np.mean(all_confidences)\n",
        "    std_confidence = np.std(all_confidences)\n",
        "    avg_entropy = np.mean(all_entropies)\n",
        "\n",
        "    # Calculate ECE\n",
        "    accuracies = (all_predictions == all_labels).astype(float)\n",
        "    ece = calculate_ece(torch.tensor(all_confidences), torch.tensor(accuracies))\n",
        "\n",
        "    return {\n",
        "        'test_loss': avg_loss,\n",
        "        'top1': top1,\n",
        "        'top3': top3,\n",
        "        'top5': top5,\n",
        "        'precision_macro': precision_macro,\n",
        "        'recall_macro': recall_macro,\n",
        "        'f1_macro': f1_macro,\n",
        "        'precision_weighted': precision_weighted,\n",
        "        'recall_weighted': recall_weighted,\n",
        "        'f1_weighted': f1_weighted,\n",
        "        'avg_confidence': avg_confidence,\n",
        "        'std_confidence': std_confidence,\n",
        "        'avg_entropy': avg_entropy,\n",
        "        'ece': ece\n",
        "    }\n",
        "\n",
        "# ==========================================\n",
        "# Main Execution\n",
        "# ==========================================\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # Set global seeds for full reproducibility\n",
        "    torch.manual_seed(CONFIG[\"split_seed\"])\n",
        "    np.random.seed(CONFIG[\"split_seed\"])\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(CONFIG[\"split_seed\"])\n",
        "        torch.cuda.manual_seed_all(CONFIG[\"split_seed\"])\n",
        "        # Ensure deterministic behavior on CUDA\n",
        "        torch.backends.cudnn.deterministic = True\n",
        "        torch.backends.cudnn.benchmark = False\n",
        "\n",
        "    print(f\"Using device: {CONFIG['device']}\")\n",
        "    print(f\"Reproducibility seed: {CONFIG['split_seed']}\")\n",
        "\n",
        "    # Get data loaders\n",
        "    dataloaders = get_dataloaders(CONFIG)\n",
        "    if dataloaders is None:\n",
        "        print(\"Could not prepare data. Halting execution.\", file=sys.stderr)\n",
        "        sys.exit(1)  # Exit if data preparation failed\n",
        "\n",
        "    train_loader, val_loader, test_loader = dataloaders\n",
        "\n",
        "    # Initialize and train model with ResNetTransformer\n",
        "    model = ResNetTransformer(\n",
        "        block=NonResidualBlock,\n",
        "        layers=[2, 2, 2, 2],\n",
        "        num_classes=CONFIG[\"num_classes\"],\n",
        "        t_config=TRANSFORMER_CONFIG\n",
        "    )\n",
        "\n",
        "    print(f\"Model has {sum(p.numel() for p in model.parameters())} parameters\")\n",
        "    print(f\"Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad)}\")\n",
        "\n",
        "    trained_model = train_and_validate(model, train_loader, val_loader, CONFIG)\n",
        "\n",
        "    # Comprehensive evaluation on the test set\n",
        "    print(\"--- Starting Comprehensive Evaluation on Test Set ---\")\n",
        "    results = comprehensive_evaluation(trained_model, test_loader, nn.CrossEntropyLoss(), CONFIG[\"device\"])\n",
        "\n",
        "    # Print all requested metrics\n",
        "    print(\"\\n--- Top-K Accuracy Results ---\")\n",
        "    print(f\"Top-1 Accuracy: {results['top1'] * 100:.2f}%\")\n",
        "    print(f\"Top-3 Accuracy: {results['top3'] * 100:.2f}%\")\n",
        "    print(f\"Top-5 Accuracy: {results['top5'] * 100:.2f}%\")\n",
        "\n",
        "    print(\"\\n--- Additional Performance Metrics ---\")\n",
        "    print(f\"Macro Average Precision: {results['precision_macro']:.4f}\")\n",
        "    print(f\"Macro Average Recall: {results['recall_macro']:.4f}\")\n",
        "    print(f\"Macro Average F1-Score: {results['f1_macro']:.4f}\")\n",
        "    print(f\"Weighted Average Precision: {results['precision_weighted']:.4f}\")\n",
        "    print(f\"Weighted Average Recall: {results['recall_weighted']:.4f}\")\n",
        "    print(f\"Weighted Average F1-Score: {results['f1_weighted']:.4f}\")\n",
        "\n",
        "    print(\"\\n--- Model Confidence & Calibration Metrics ---\")\n",
        "    print(f\"Average Prediction Confidence: {results['avg_confidence']:.4f}\")\n",
        "    print(f\"Confidence Standard Deviation: {results['std_confidence']:.4f}\")\n",
        "    print(f\"Average Prediction Entropy: {results['avg_entropy']:.4f}\")\n",
        "    print(f\"Expected Calibration Error: {results['ece']:.4f}\")\n",
        "\n",
        "    print(f\"\\nFinal Test Loss: {results['test_loss']:.4f}\")\n",
        "    print(\"===============================================\")"
      ],
      "metadata": {
        "id": "tdq72qCqqbPa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ab07bbd-2d7b-4ad2-c4fd-223deeb622eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Reproducibility seed: 42\n",
            "Loading metadata from: ./pad-ufes-20/metadata.csv\n",
            "Total samples: 2298\n",
            "Diagnostic classes: diagnostic\n",
            "BCC    845\n",
            "ACK    730\n",
            "NEV    244\n",
            "SEK    235\n",
            "SCC    192\n",
            "MEL     52\n",
            "Name: count, dtype: int64\n",
            "✅ Dataset verification complete\n",
            "Found 6 classes: ['ACK', 'BCC', 'MEL', 'NEV', 'SCC', 'SEK']\n",
            "Dataset splits - Train: 1608, Val: 344, Test: 346, Total: 2298\n",
            "Found 6 classes: ['ACK', 'BCC', 'MEL', 'NEV', 'SCC', 'SEK']\n",
            "Found 6 classes: ['ACK', 'BCC', 'MEL', 'NEV', 'SCC', 'SEK']\n",
            "Found 6 classes: ['ACK', 'BCC', 'MEL', 'NEV', 'SCC', 'SEK']\n",
            "Model has 12850246 parameters\n",
            "Trainable parameters: 12850246\n",
            "\n",
            "--- Starting Training ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100 | Time: 77.47s | Train Loss: 1.6639, Train Acc: 33.27% | Val Loss: 1.5029, Val Acc: 35.47%\n",
            "\n",
            "Model saved to ./best_model_pad_ufes.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/100 | Time: 77.45s | Train Loss: 1.5394, Train Acc: 35.63% | Val Loss: 1.4876, Val Acc: 33.72%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/100 | Time: 77.08s | Train Loss: 1.5424, Train Acc: 34.45% | Val Loss: 1.5223, Val Acc: 35.17%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4/100 | Time: 76.42s | Train Loss: 1.5133, Train Acc: 36.26% | Val Loss: 1.4681, Val Acc: 34.30%\n",
            "\n",
            "Reducing learning rate from 1.00e-03 to 2.00e-04\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5/100 | Time: 76.30s | Train Loss: 1.4997, Train Acc: 36.07% | Val Loss: 1.4497, Val Acc: 35.47%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6/100 | Time: 75.07s | Train Loss: 1.4695, Train Acc: 36.13% | Val Loss: 1.4708, Val Acc: 35.17%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7/100 | Time: 75.34s | Train Loss: 1.4640, Train Acc: 36.38% | Val Loss: 1.4472, Val Acc: 37.21%\n",
            "\n",
            "Model saved to ./best_model_pad_ufes.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8/100 | Time: 82.09s | Train Loss: 1.4509, Train Acc: 37.56% | Val Loss: 1.3754, Val Acc: 40.70%\n",
            "\n",
            "Model saved to ./best_model_pad_ufes.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9/100 | Time: 80.45s | Train Loss: 1.4388, Train Acc: 39.43% | Val Loss: 1.4078, Val Acc: 40.99%\n",
            "\n",
            "Model saved to ./best_model_pad_ufes.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10/100 | Time: 79.48s | Train Loss: 1.4272, Train Acc: 38.68% | Val Loss: 1.3685, Val Acc: 43.31%\n",
            "\n",
            "Model saved to ./best_model_pad_ufes.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 11/100 | Time: 77.37s | Train Loss: 1.4319, Train Acc: 40.30% | Val Loss: 1.3670, Val Acc: 44.48%\n",
            "\n",
            "Model saved to ./best_model_pad_ufes.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 12/100 | Time: 76.66s | Train Loss: 1.4207, Train Acc: 39.37% | Val Loss: 1.5521, Val Acc: 32.85%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 13/100 | Time: 76.76s | Train Loss: 1.4090, Train Acc: 40.42% | Val Loss: 1.3502, Val Acc: 45.64%\n",
            "\n",
            "Model saved to ./best_model_pad_ufes.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 14/100 | Time: 76.74s | Train Loss: 1.3926, Train Acc: 42.85% | Val Loss: 1.3164, Val Acc: 45.93%\n",
            "\n",
            "Model saved to ./best_model_pad_ufes.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 15/100 | Time: 77.18s | Train Loss: 1.4157, Train Acc: 40.86% | Val Loss: 1.3898, Val Acc: 41.86%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 16/100 | Time: 76.65s | Train Loss: 1.3981, Train Acc: 43.16% | Val Loss: 1.3656, Val Acc: 49.71%\n",
            "\n",
            "Model saved to ./best_model_pad_ufes.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 17/100 | Time: 76.13s | Train Loss: 1.3736, Train Acc: 44.53% | Val Loss: 1.2802, Val Acc: 48.84%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 18/100 | Time: 75.74s | Train Loss: 1.3732, Train Acc: 43.53% | Val Loss: 1.3200, Val Acc: 44.19%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 19/100 | Time: 76.29s | Train Loss: 1.3735, Train Acc: 42.54% | Val Loss: 1.3070, Val Acc: 49.13%\n",
            "\n",
            "Reducing learning rate from 2.00e-04 to 4.00e-05\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 20/100 | Time: 77.47s | Train Loss: 1.3436, Train Acc: 44.59% | Val Loss: 1.2741, Val Acc: 52.03%\n",
            "\n",
            "Model saved to ./best_model_pad_ufes.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 21/100 | Time: 76.09s | Train Loss: 1.2882, Train Acc: 48.07% | Val Loss: 1.2549, Val Acc: 50.87%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 22/100 | Time: 75.68s | Train Loss: 1.2946, Train Acc: 46.39% | Val Loss: 1.2661, Val Acc: 52.62%\n",
            "\n",
            "Model saved to ./best_model_pad_ufes.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 23/100 | Time: 76.08s | Train Loss: 1.2710, Train Acc: 48.76% | Val Loss: 1.2534, Val Acc: 51.45%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 24/100 | Time: 77.15s | Train Loss: 1.2740, Train Acc: 49.38% | Val Loss: 1.2565, Val Acc: 50.87%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 25/100 | Time: 77.39s | Train Loss: 1.2646, Train Acc: 50.50% | Val Loss: 1.2367, Val Acc: 51.16%\n",
            "\n",
            "Reducing learning rate from 4.00e-05 to 8.00e-06\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 26/100 | Time: 76.63s | Train Loss: 1.2577, Train Acc: 48.38% | Val Loss: 1.2429, Val Acc: 51.45%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 27/100 | Time: 77.18s | Train Loss: 1.2381, Train Acc: 48.88% | Val Loss: 1.2415, Val Acc: 52.91%\n",
            "\n",
            "Model saved to ./best_model_pad_ufes.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 28/100 | Time: 76.95s | Train Loss: 1.2594, Train Acc: 48.26% | Val Loss: 1.2357, Val Acc: 52.03%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 29/100 | Time: 76.70s | Train Loss: 1.2377, Train Acc: 50.25% | Val Loss: 1.2221, Val Acc: 53.20%\n",
            "\n",
            "Model saved to ./best_model_pad_ufes.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 30/100 | Time: 76.82s | Train Loss: 1.2497, Train Acc: 50.25% | Val Loss: 1.2293, Val Acc: 52.03%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 31/100 | Time: 76.87s | Train Loss: 1.2426, Train Acc: 49.13% | Val Loss: 1.2174, Val Acc: 53.20%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 32/100 | Time: 77.01s | Train Loss: 1.2424, Train Acc: 49.63% | Val Loss: 1.2154, Val Acc: 52.91%\n",
            "\n",
            "Reducing learning rate from 8.00e-06 to 1.60e-06\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 33/100 | Time: 76.47s | Train Loss: 1.2404, Train Acc: 50.37% | Val Loss: 1.2179, Val Acc: 52.33%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 34/100 | Time: 76.06s | Train Loss: 1.2498, Train Acc: 49.63% | Val Loss: 1.2157, Val Acc: 52.33%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 35/100 | Time: 76.38s | Train Loss: 1.2399, Train Acc: 50.12% | Val Loss: 1.2205, Val Acc: 51.74%\n",
            "\n",
            "Reducing learning rate from 1.60e-06 to 3.20e-07\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 36/100 | Time: 74.73s | Train Loss: 1.2189, Train Acc: 50.56% | Val Loss: 1.2184, Val Acc: 53.20%\n",
            "\n",
            "Early stopping at epoch 36\n",
            "Restored model weights from the end of the best epoch: 29\n",
            "--- Training Finished ---\n",
            "\n",
            "--- Starting Comprehensive Evaluation on Test Set ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating: 100%|██████████| 6/6 [00:12<00:00,  2.02s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Top-K Accuracy Results ---\n",
            "Top-1 Accuracy: 50.58%\n",
            "Top-3 Accuracy: 88.44%\n",
            "Top-5 Accuracy: 98.84%\n",
            "\n",
            "--- Additional Performance Metrics ---\n",
            "Macro Average Precision: 0.3497\n",
            "Macro Average Recall: 0.3002\n",
            "Macro Average F1-Score: 0.3041\n",
            "Weighted Average Precision: 0.4719\n",
            "Weighted Average Recall: 0.5058\n",
            "Weighted Average F1-Score: 0.4731\n",
            "\n",
            "--- Model Confidence & Calibration Metrics ---\n",
            "Average Prediction Confidence: 0.5087\n",
            "Confidence Standard Deviation: 0.0957\n",
            "Average Prediction Entropy: 1.2184\n",
            "Expected Calibration Error: 0.0320\n",
            "\n",
            "Final Test Loss: 1.2696\n",
            "===============================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hybrid Transformer on ResNet-18 (RoPE)"
      ],
      "metadata": {
        "id": "3eZHP_Z3qoux"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "import torchvision.transforms as transforms\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "import os\n",
        "import requests\n",
        "import zipfile\n",
        "import shutil\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "import sys\n",
        "import math\n",
        "import numpy as np\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "\n",
        "# --- Configuration Parameters ---\n",
        "CONFIG = {\n",
        "    \"data_path\": \"./pad-ufes-20/\",\n",
        "    \"dataset_url\": \"https://www.kaggle.com/api/v1/datasets/download/maxjen/pad-ufes-20\",  # Kaggle API URL\n",
        "    \"batch_size\": 64,  # Reduced due to smaller dataset\n",
        "    \"num_epochs\": 100,\n",
        "    \"learning_rate\": 0.001,\n",
        "    \"weight_decay\": 1e-4,\n",
        "    \"split_seed\": 42,\n",
        "    \"train_split\": 0.70,\n",
        "    \"val_split\": 0.15,\n",
        "    \"test_split\": 0.15,\n",
        "    \"num_classes\": 6,  # PAD-UFES-20 has 6 diagnostic classes\n",
        "    \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
        "    \"checkpoint_path\": \"./best_model_pad_ufes.pth\",\n",
        "}\n",
        "\n",
        "# --- Transformer Configuration ---\n",
        "TRANSFORMER_CONFIG = {\n",
        "    \"embedding_dim\": 256,\n",
        "    \"nhead\": 8,\n",
        "    \"num_encoder_layers\": 3,\n",
        "    \"dim_feedforward\": 512,\n",
        "    \"dropout\": 0.1,\n",
        "}\n",
        "\n",
        "# ==========================================\n",
        "# Callback Classes\n",
        "# ==========================================\n",
        "\n",
        "class Callback:\n",
        "    \"\"\"Base callback class\"\"\"\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        pass\n",
        "\n",
        "    def on_training_end(self):\n",
        "        pass\n",
        "\n",
        "class ReduceLROnPlateau(Callback):\n",
        "    \"\"\"Reduce learning rate when a metric has stopped improving\"\"\"\n",
        "    def __init__(self, optimizer, monitor='val_accuracy', factor=0.2, patience=3, min_lr=1e-7, verbose=1):\n",
        "        self.optimizer = optimizer\n",
        "        self.monitor = monitor\n",
        "        self.factor = factor\n",
        "        self.patience = patience\n",
        "        self.min_lr = min_lr\n",
        "        self.verbose = verbose\n",
        "        self.wait = 0\n",
        "        self.best = None\n",
        "        self.mode = 'min' if 'loss' in monitor else 'max'\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        if logs is None:\n",
        "            return\n",
        "\n",
        "        current = logs.get(self.monitor)\n",
        "        if current is None:\n",
        "            return\n",
        "\n",
        "        if self.best is None:\n",
        "            self.best = current\n",
        "            return\n",
        "\n",
        "        if self.mode == 'min':\n",
        "            improved = current < self.best\n",
        "        else:\n",
        "            improved = current > self.best\n",
        "\n",
        "        if improved:\n",
        "            self.best = current\n",
        "            self.wait = 0\n",
        "        else:\n",
        "            self.wait += 1\n",
        "            if self.wait >= self.patience:\n",
        "                old_lr = self.optimizer.param_groups[0]['lr']\n",
        "                new_lr = max(old_lr * self.factor, self.min_lr)\n",
        "                if new_lr != old_lr:\n",
        "                    self.optimizer.param_groups[0]['lr'] = new_lr\n",
        "                    if self.verbose:\n",
        "                        print(f\"\\nReducing learning rate from {old_lr:.2e} to {new_lr:.2e}\")\n",
        "                    self.wait = 0\n",
        "\n",
        "class EarlyStopping(Callback):\n",
        "    \"\"\"Stop training when a monitored metric has stopped improving\"\"\"\n",
        "    def __init__(self, monitor='val_accuracy', patience=7, restore_best_weights=True, verbose=1):\n",
        "        self.monitor = monitor\n",
        "        self.patience = patience\n",
        "        self.restore_best_weights = restore_best_weights\n",
        "        self.verbose = verbose\n",
        "        self.wait = 0\n",
        "        self.best = None\n",
        "        self.best_weights = None\n",
        "        self.mode = 'min' if 'loss' in monitor else 'max'\n",
        "        self.stopped_epoch = 0\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None, model=None):\n",
        "        if logs is None:\n",
        "            return False\n",
        "\n",
        "        current = logs.get(self.monitor)\n",
        "        if current is None:\n",
        "            return False\n",
        "\n",
        "        if self.best is None:\n",
        "            self.best = current\n",
        "            if model is not None and self.restore_best_weights:\n",
        "                self.best_weights = model.state_dict().copy()\n",
        "            return False\n",
        "\n",
        "        if self.mode == 'min':\n",
        "            improved = current < self.best\n",
        "        else:\n",
        "            improved = current > self.best\n",
        "\n",
        "        if improved:\n",
        "            self.best = current\n",
        "            self.wait = 0\n",
        "            if model is not None and self.restore_best_weights:\n",
        "                self.best_weights = model.state_dict().copy()\n",
        "        else:\n",
        "            self.wait += 1\n",
        "            if self.wait >= self.patience:\n",
        "                self.stopped_epoch = epoch\n",
        "                if self.verbose:\n",
        "                    print(f\"\\nEarly stopping at epoch {epoch + 1}\")\n",
        "                return True\n",
        "        return False\n",
        "\n",
        "    def on_training_end(self, model=None):\n",
        "        if self.stopped_epoch > 0 and self.verbose:\n",
        "            print(f\"Restored model weights from the end of the best epoch: {self.stopped_epoch + 1 - self.patience}\")\n",
        "        if model is not None and self.restore_best_weights and self.best_weights is not None:\n",
        "            model.load_state_dict(self.best_weights)\n",
        "\n",
        "class ModelCheckpoint(Callback):\n",
        "    \"\"\"Save the model after every epoch\"\"\"\n",
        "    def __init__(self, filepath, save_best_only=True, monitor='val_accuracy', verbose=1):\n",
        "        self.filepath = filepath\n",
        "        self.save_best_only = save_best_only\n",
        "        self.monitor = monitor\n",
        "        self.verbose = verbose\n",
        "        self.best = None\n",
        "        self.mode = 'min' if 'loss' in monitor else 'max'\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None, model=None):\n",
        "        if logs is None or model is None:\n",
        "            return\n",
        "\n",
        "        current = logs.get(self.monitor)\n",
        "        if current is None:\n",
        "            return\n",
        "\n",
        "        if not self.save_best_only:\n",
        "            filepath = self.filepath.replace('.pth', f'_epoch_{epoch + 1}.pth')\n",
        "            torch.save(model.state_dict(), filepath)\n",
        "            if self.verbose:\n",
        "                print(f\"\\nModel saved to {filepath}\")\n",
        "            return\n",
        "\n",
        "        if self.best is None:\n",
        "            self.best = current\n",
        "            torch.save(model.state_dict(), self.filepath)\n",
        "            if self.verbose:\n",
        "                print(f\"\\nModel saved to {self.filepath}\")\n",
        "            return\n",
        "\n",
        "        if self.mode == 'min':\n",
        "            improved = current < self.best\n",
        "        else:\n",
        "            improved = current > self.best\n",
        "\n",
        "        if improved:\n",
        "            self.best = current\n",
        "            torch.save(model.state_dict(), self.filepath)\n",
        "            if self.verbose:\n",
        "                print(f\"\\nModel saved to {self.filepath}\")\n",
        "\n",
        "# ==========================================\n",
        "# Data Preprocessing\n",
        "# ==========================================\n",
        "\n",
        "def download_and_extract_pad_ufes(url, dest_path):\n",
        "    \"\"\"Downloads and extracts PAD-UFES-20 dataset from Kaggle with progress and robust error handling.\"\"\"\n",
        "    if os.path.exists(dest_path):\n",
        "        print(\"Dataset directory already exists.\")\n",
        "        return True\n",
        "\n",
        "    zip_path = dest_path.rstrip('/') + \".zip\"\n",
        "    extract_to_dir = os.path.abspath(os.path.join(dest_path, os.pardir))\n",
        "\n",
        "    print(f\"Downloading PAD-UFES-20 dataset from Kaggle...\")\n",
        "    try:\n",
        "        # Add headers to mimic browser request\n",
        "        headers = {\n",
        "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
        "        }\n",
        "\n",
        "        with requests.get(url, stream=True, timeout=60, headers=headers) as r:\n",
        "            r.raise_for_status()\n",
        "            total_size = int(r.headers.get('content-length', 0))\n",
        "            with open(zip_path, 'wb') as f, tqdm(\n",
        "                total=total_size, unit='iB', unit_scale=True, desc=\"PAD-UFES-20\"\n",
        "            ) as progress_bar:\n",
        "                for chunk in r.iter_content(chunk_size=8192):\n",
        "                    f.write(chunk)\n",
        "                    progress_bar.update(len(chunk))\n",
        "\n",
        "        print(\"Extracting...\")\n",
        "        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "            zip_ref.extractall(extract_to_dir)\n",
        "\n",
        "        # Check if extraction created nested folders and fix if needed\n",
        "        extracted_folders = [f for f in os.listdir(extract_to_dir) if os.path.isdir(os.path.join(extract_to_dir, f))]\n",
        "        if 'pad-ufes-20' in extracted_folders:\n",
        "            # Dataset extracted correctly\n",
        "            pass\n",
        "        else:\n",
        "            # Look for the actual dataset folder\n",
        "            for folder in extracted_folders:\n",
        "                folder_path = os.path.join(extract_to_dir, folder)\n",
        "                if os.path.exists(os.path.join(folder_path, 'metadata.csv')):\n",
        "                    # This is our dataset folder, rename it\n",
        "                    os.rename(folder_path, dest_path)\n",
        "                    break\n",
        "\n",
        "        return True\n",
        "\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"\\nError downloading file: {e}\", file=sys.stderr)\n",
        "        print(\"Please download manually from: https://www.kaggle.com/datasets/maxjen/pad-ufes-20\", file=sys.stderr)\n",
        "        return False\n",
        "\n",
        "    finally:\n",
        "        if os.path.exists(zip_path):\n",
        "            os.remove(zip_path)\n",
        "\n",
        "class PADUFESDataset(Dataset):\n",
        "    \"\"\"Custom dataset class for PAD-UFES-20.\"\"\"\n",
        "    def __init__(self, data_dir, metadata_df, transform=None):\n",
        "        self.data_dir = data_dir\n",
        "        self.metadata_df = metadata_df.reset_index(drop=True)\n",
        "        self.transform = transform\n",
        "\n",
        "        # Create label mapping\n",
        "        self.classes = sorted(metadata_df['diagnostic'].unique())\n",
        "        self.class_to_idx = {cls_name: idx for idx, cls_name in enumerate(self.classes)}\n",
        "\n",
        "        print(f\"Found {len(self.classes)} classes: {self.classes}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.metadata_df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.metadata_df.iloc[idx]\n",
        "\n",
        "        # Handle different possible image path formats\n",
        "        img_name = row['img_id']\n",
        "        if not img_name.endswith('.png'):\n",
        "            img_name += '.png'\n",
        "\n",
        "        # Try different possible folder structures\n",
        "        possible_paths = [\n",
        "            os.path.join(self.data_dir, 'images', img_name),\n",
        "            os.path.join(self.data_dir, 'imgs', img_name),\n",
        "            os.path.join(self.data_dir, img_name),\n",
        "        ]\n",
        "\n",
        "        img_path = None\n",
        "        for path in possible_paths:\n",
        "            if os.path.exists(path):\n",
        "                img_path = path\n",
        "                break\n",
        "\n",
        "        if img_path is None:\n",
        "            # Last resort: search for the file\n",
        "            for root, dirs, files in os.walk(self.data_dir):\n",
        "                if img_name in files:\n",
        "                    img_path = os.path.join(root, img_name)\n",
        "                    break\n",
        "\n",
        "        if img_path is None:\n",
        "            print(f\"Warning: Image {img_name} not found, using black placeholder\")\n",
        "            image = Image.new('RGB', (224, 224), color='black')\n",
        "        else:\n",
        "            try:\n",
        "                image = Image.open(img_path).convert('RGB')\n",
        "            except Exception as e:\n",
        "                print(f\"Error loading image {img_path}: {e}\")\n",
        "                image = Image.new('RGB', (224, 224), color='black')\n",
        "\n",
        "        label = self.class_to_idx[row['diagnostic']]\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, label\n",
        "\n",
        "def prepare_pad_ufes_data(data_path):\n",
        "    \"\"\"Prepare PAD-UFES-20 data by reading metadata and organizing structure.\"\"\"\n",
        "\n",
        "    # Try different possible metadata file locations\n",
        "    possible_metadata_paths = [\n",
        "        os.path.join(data_path, 'metadata.csv'),\n",
        "        os.path.join(data_path, 'PAD-UFES-20_metadata.csv'),\n",
        "    ]\n",
        "\n",
        "    metadata_path = None\n",
        "    for path in possible_metadata_paths:\n",
        "        if os.path.exists(path):\n",
        "            metadata_path = path\n",
        "            break\n",
        "\n",
        "    if metadata_path is None:\n",
        "        # Search for any CSV file that might be the metadata\n",
        "        for root, dirs, files in os.walk(data_path):\n",
        "            for file in files:\n",
        "                if file.endswith('.csv') and 'metadata' in file.lower():\n",
        "                    metadata_path = os.path.join(root, file)\n",
        "                    break\n",
        "            if metadata_path:\n",
        "                break\n",
        "\n",
        "    if metadata_path is None:\n",
        "        print(f\"Metadata file not found in {data_path}\")\n",
        "        return None\n",
        "\n",
        "    print(f\"Loading metadata from: {metadata_path}\")\n",
        "    try:\n",
        "        metadata_df = pd.read_csv(metadata_path)\n",
        "    except Exception as e:\n",
        "        print(f\"Error reading metadata file: {e}\")\n",
        "        return None\n",
        "\n",
        "    # Print dataset info\n",
        "    print(f\"Total samples: {len(metadata_df)}\")\n",
        "    print(f\"Diagnostic classes: {metadata_df['diagnostic'].value_counts()}\")\n",
        "\n",
        "    # Verify dataset integrity\n",
        "    expected_samples = 2298\n",
        "    expected_classes = 6\n",
        "\n",
        "    actual_classes = len(metadata_df['diagnostic'].unique())\n",
        "    if len(metadata_df) != expected_samples:\n",
        "        print(f\"⚠️  Warning: Expected {expected_samples} samples, got {len(metadata_df)}\")\n",
        "    if actual_classes != expected_classes:\n",
        "        print(f\"⚠️  Warning: Expected {expected_classes} classes, got {actual_classes}\")\n",
        "\n",
        "    print(\"✅ Dataset verification complete\")\n",
        "    return metadata_df\n",
        "\n",
        "def get_dataloaders(config):\n",
        "    \"\"\"Downloads, prepares, and splits the PAD-UFES-20 data, returning DataLoaders.\"\"\"\n",
        "\n",
        "    # Try to download if URL is provided and dataset doesn't exist\n",
        "    if config.get(\"dataset_url\") and not os.path.exists(config[\"data_path\"]):\n",
        "        if not download_and_extract_pad_ufes(config[\"dataset_url\"], config[\"data_path\"]):\n",
        "            print(\"\\nAutomatic download failed. Please download manually:\")\n",
        "            print(\"1. Visit: https://www.kaggle.com/datasets/maxjen/pad-ufes-20\")\n",
        "            print(\"2. Download the dataset\")\n",
        "            print(\"3. Extract to:\", config[\"data_path\"])\n",
        "            return None\n",
        "\n",
        "    # Check if dataset exists\n",
        "    if not os.path.exists(config[\"data_path\"]):\n",
        "        print(f\"\\nDataset not found at {config['data_path']}\")\n",
        "        print(\"Please download the PAD-UFES-20 dataset:\")\n",
        "        print(\"1. Visit: https://www.kaggle.com/datasets/maxjen/pad-ufes-20\")\n",
        "        print(\"2. Download the dataset\")\n",
        "        print(\"3. Extract to:\", config[\"data_path\"])\n",
        "        print(\"4. Ensure the structure includes metadata.csv and image files\")\n",
        "        return None\n",
        "\n",
        "    # Prepare metadata\n",
        "    metadata_df = prepare_pad_ufes_data(config[\"data_path\"])\n",
        "    if metadata_df is None:\n",
        "        return None\n",
        "\n",
        "    # Define transforms\n",
        "    train_transform = transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.RandomHorizontalFlip(p=0.5),\n",
        "        transforms.RandomRotation(degrees=10),\n",
        "        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
        "        transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "    ])\n",
        "\n",
        "    val_test_transform = transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "    ])\n",
        "\n",
        "    # Create full dataset\n",
        "    full_dataset = PADUFESDataset(config[\"data_path\"], metadata_df, transform=None)\n",
        "\n",
        "    # Ensure reproducible splits\n",
        "    torch.manual_seed(config[\"split_seed\"])\n",
        "    np.random.seed(config[\"split_seed\"])\n",
        "\n",
        "    total_size = len(full_dataset)\n",
        "    train_size = int(config[\"train_split\"] * total_size)\n",
        "    val_size = int(config[\"val_split\"] * total_size)\n",
        "    test_size = int(config[\"test_split\"] * total_size)\n",
        "\n",
        "    # Adjust sizes to ensure they sum to total_size\n",
        "    actual_total = train_size + val_size + test_size\n",
        "    if actual_total != total_size:\n",
        "        test_size += (total_size - actual_total)\n",
        "\n",
        "    print(f\"Dataset splits - Train: {train_size}, Val: {val_size}, Test: {test_size}, Total: {total_size}\")\n",
        "\n",
        "    # Create indices for splitting\n",
        "    indices = list(range(total_size))\n",
        "    np.random.shuffle(indices)\n",
        "\n",
        "    train_indices = indices[:train_size]\n",
        "    val_indices = indices[train_size:train_size + val_size]\n",
        "    test_indices = indices[train_size + val_size:]\n",
        "\n",
        "    # Create subset datasets with appropriate transforms\n",
        "    train_dataset = PADUFESDataset(\n",
        "        config[\"data_path\"],\n",
        "        metadata_df.iloc[train_indices],\n",
        "        transform=train_transform\n",
        "    )\n",
        "    val_dataset = PADUFESDataset(\n",
        "        config[\"data_path\"],\n",
        "        metadata_df.iloc[val_indices],\n",
        "        transform=val_test_transform\n",
        "    )\n",
        "    test_dataset = PADUFESDataset(\n",
        "        config[\"data_path\"],\n",
        "        metadata_df.iloc[test_indices],\n",
        "        transform=val_test_transform\n",
        "    )\n",
        "\n",
        "    # Set worker init function for reproducible DataLoader behavior\n",
        "    def worker_init_fn(worker_id):\n",
        "        np.random.seed(config[\"split_seed\"] + worker_id)\n",
        "\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=config[\"batch_size\"],\n",
        "        shuffle=True,\n",
        "        num_workers=2,\n",
        "        pin_memory=True,\n",
        "        worker_init_fn=worker_init_fn,\n",
        "        generator=torch.Generator().manual_seed(config[\"split_seed\"])\n",
        "    )\n",
        "    val_loader = DataLoader(\n",
        "        val_dataset,\n",
        "        batch_size=config[\"batch_size\"],\n",
        "        shuffle=False,\n",
        "        num_workers=2,\n",
        "        pin_memory=True,\n",
        "        worker_init_fn=worker_init_fn\n",
        "    )\n",
        "    test_loader = DataLoader(\n",
        "        test_dataset,\n",
        "        batch_size=config[\"batch_size\"],\n",
        "        shuffle=False,\n",
        "        num_workers=2,\n",
        "        pin_memory=True,\n",
        "        worker_init_fn=worker_init_fn\n",
        "    )\n",
        "\n",
        "    return train_loader, val_loader, test_loader\n",
        "\n",
        "# ==========================================\n",
        "# Model Architecture and Training\n",
        "# ==========================================\n",
        "\n",
        "class RotaryEmbedding(nn.Module):\n",
        "    \"\"\"\n",
        "    The Rotary Positional Embedding (RoPE) module.\n",
        "    This implementation is based on the paper \"RoFormer: Enhanced Transformer with Rotary Position Embedding\".\n",
        "    \"\"\"\n",
        "    def __init__(self, dim, base=10000):\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "        self.base = base\n",
        "        # Create inverse frequencies and register as a buffer\n",
        "        inv_freq = 1.0 / (self.base ** (torch.arange(0, dim, 2).float() / dim))\n",
        "        self.register_buffer(\"inv_freq\", inv_freq)\n",
        "        self.seq_len_cached = None\n",
        "        self.cos_cached = None\n",
        "        self.sin_cached = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        seq_len = x.shape[1]\n",
        "        # Check if we need to recompute the cache\n",
        "        if seq_len != self.seq_len_cached:\n",
        "            self.seq_len_cached = seq_len\n",
        "            t = torch.arange(seq_len, device=x.device).type_as(self.inv_freq)\n",
        "            freqs = torch.einsum(\"i,j->ij\", t, self.inv_freq)\n",
        "            emb = torch.cat((freqs, freqs), dim=-1).to(x.device)\n",
        "            self.cos_cached = emb.cos()[:, None, :]\n",
        "            self.sin_cached = emb.sin()[:, None, :]\n",
        "\n",
        "        # Apply the rotation\n",
        "        return self.cos_cached, self.sin_cached\n",
        "\n",
        "def rotate_half(x):\n",
        "    x1, x2 = x[..., : x.shape[-1] // 2], x[..., x.shape[-1] // 2 :]\n",
        "    return torch.cat((-x2, x1), dim=x1.ndim - 1)\n",
        "\n",
        "def apply_rotary_pos_emb(q, k, cos, sin):\n",
        "    return (q * cos) + (rotate_half(q) * sin), (k * cos) + (rotate_half(k) * sin)\n",
        "\n",
        "class MultiHeadAttentionWithRoPE(nn.Module):\n",
        "    \"\"\"Custom Multi-Head Attention with RoPE support\"\"\"\n",
        "    def __init__(self, d_model, nhead, dropout=0.1, batch_first=True):\n",
        "        super().__init__()\n",
        "        assert d_model % nhead == 0\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.nhead = nhead\n",
        "        self.d_k = d_model // nhead\n",
        "        self.batch_first = batch_first  # Add this attribute\n",
        "\n",
        "        self.w_q = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.w_k = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.w_v = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.w_o = nn.Linear(d_model, d_model)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.rotary_emb = RotaryEmbedding(self.d_k)\n",
        "\n",
        "    def forward(self, query, key, value, key_padding_mask=None, attn_mask=None, need_weights=False, is_causal=False):\n",
        "        batch_size, seq_len, _ = query.size()\n",
        "\n",
        "        # Linear transformations\n",
        "        Q = self.w_q(query)  # [batch_size, seq_len, d_model]\n",
        "        K = self.w_k(key)    # [batch_size, seq_len, d_model]\n",
        "        V = self.w_v(value)  # [batch_size, seq_len, d_model]\n",
        "\n",
        "        # Reshape for multi-head attention\n",
        "        Q = Q.view(batch_size, seq_len, self.nhead, self.d_k)  # [batch, seq, heads, d_k]\n",
        "        K = K.view(batch_size, seq_len, self.nhead, self.d_k)\n",
        "        V = V.view(batch_size, seq_len, self.nhead, self.d_k)\n",
        "\n",
        "        # Apply RoPE to Q and K\n",
        "        cos, sin = self.rotary_emb(query)\n",
        "        Q, K = apply_rotary_pos_emb(Q, K, cos, sin)\n",
        "\n",
        "        # Transpose for attention computation: [batch, heads, seq, d_k]\n",
        "        Q = Q.transpose(1, 2)\n",
        "        K = K.transpose(1, 2)\n",
        "        V = V.transpose(1, 2)\n",
        "\n",
        "        # Scaled dot-product attention\n",
        "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
        "\n",
        "        # Apply masks if provided\n",
        "        if attn_mask is not None:\n",
        "            scores = scores.masked_fill(attn_mask == 0, -1e9)\n",
        "\n",
        "        if key_padding_mask is not None:\n",
        "            # key_padding_mask: [batch_size, seq_len], True for padding positions\n",
        "            key_padding_mask = key_padding_mask.unsqueeze(1).unsqueeze(2)  # [batch, 1, 1, seq]\n",
        "            scores = scores.masked_fill(key_padding_mask, -1e9)\n",
        "\n",
        "        attention_weights = torch.softmax(scores, dim=-1)\n",
        "        attention_weights = self.dropout(attention_weights)\n",
        "\n",
        "        # Apply attention to values\n",
        "        attention_output = torch.matmul(attention_weights, V)  # [batch, heads, seq, d_k]\n",
        "\n",
        "        # Concatenate heads\n",
        "        attention_output = attention_output.transpose(1, 2).contiguous().view(\n",
        "            batch_size, seq_len, self.d_model\n",
        "        )\n",
        "\n",
        "        # Final linear transformation\n",
        "        output = self.w_o(attention_output)\n",
        "\n",
        "        if need_weights:\n",
        "            return output, attention_weights.mean(dim=1)  # Average over heads for compatibility\n",
        "        return output\n",
        "\n",
        "class TransformerEncoderLayerWithRoPE(nn.Module):\n",
        "    \"\"\"\n",
        "    A custom Transformer Encoder Layer that incorporates RoPE.\n",
        "    \"\"\"\n",
        "    def __init__(self, d_model, nhead, dim_feedforward, dropout, batch_first=True):\n",
        "        super().__init__()\n",
        "        self.self_attn = MultiHeadAttentionWithRoPE(d_model, nhead, dropout, batch_first)\n",
        "        self.linear1 = nn.Linear(d_model, dim_feedforward)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.linear2 = nn.Linear(dim_feedforward, d_model)\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "        self.activation = nn.ReLU(inplace=True)\n",
        "        self.batch_first = batch_first  # Add this attribute\n",
        "\n",
        "    def forward(self, src, src_mask=None, src_key_padding_mask=None, is_causal=False):\n",
        "        # Self-attention with RoPE\n",
        "        src2 = self.self_attn(src, src, src, key_padding_mask=src_key_padding_mask, attn_mask=src_mask)\n",
        "        src = src + self.dropout1(src2)\n",
        "        src = self.norm1(src)\n",
        "\n",
        "        # Feed Forward\n",
        "        src2 = self.linear2(self.dropout(self.activation(self.linear1(src))))\n",
        "        src = src + self.dropout2(src2)\n",
        "        src = self.norm2(src)\n",
        "        return src\n",
        "\n",
        "class NonResidualBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, stride=1):\n",
        "        super(NonResidualBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        out = self.relu(out)\n",
        "        return out\n",
        "\n",
        "class ResNetTransformer(nn.Module):\n",
        "    def __init__(self, block, layers, num_classes, t_config):\n",
        "        super(ResNetTransformer, self).__init__()\n",
        "        self.in_channels = 64\n",
        "        self.embedding_dim = t_config[\"embedding_dim\"]\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "        self.layer1 = self._make_layer(block, 64, layers[0], stride=1)\n",
        "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n",
        "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n",
        "\n",
        "        self.projections = nn.ModuleList([\n",
        "            self._create_projection(64, self.embedding_dim),\n",
        "            self._create_projection(64, self.embedding_dim),\n",
        "            self._create_projection(128, self.embedding_dim),\n",
        "            self._create_projection(256, self.embedding_dim),\n",
        "            self._create_projection(512, self.embedding_dim)\n",
        "        ])\n",
        "\n",
        "        # Create transformer encoder layers manually\n",
        "        self.transformer_layers = nn.ModuleList([\n",
        "            TransformerEncoderLayerWithRoPE(\n",
        "                d_model=self.embedding_dim,\n",
        "                nhead=t_config[\"nhead\"],\n",
        "                dim_feedforward=t_config[\"dim_feedforward\"],\n",
        "                dropout=t_config[\"dropout\"],\n",
        "                batch_first=True\n",
        "            ) for _ in range(t_config[\"num_encoder_layers\"])\n",
        "        ])\n",
        "\n",
        "        self.classifier = nn.Linear(self.embedding_dim, num_classes)\n",
        "\n",
        "    def _create_projection(self, in_features, out_features):\n",
        "        return nn.Sequential(\n",
        "            nn.AdaptiveAvgPool2d((1, 1)),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(in_features, out_features)\n",
        "        )\n",
        "\n",
        "    def _make_layer(self, block, out_channels, num_blocks, stride):\n",
        "        strides = [stride] + [1] * (num_blocks - 1)\n",
        "        layers = []\n",
        "        current_in_channels = self.in_channels\n",
        "        for s in strides:\n",
        "            layers.append(block(current_in_channels, out_channels, s))\n",
        "            current_in_channels = out_channels\n",
        "        self.in_channels = out_channels\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        features = []\n",
        "        x = self.maxpool(self.relu(self.bn1(self.conv1(x))))\n",
        "        features.append(x)\n",
        "        x = self.layer1(x)\n",
        "        features.append(x)\n",
        "        x = self.layer2(x)\n",
        "        features.append(x)\n",
        "        x = self.layer3(x)\n",
        "        features.append(x)\n",
        "        x = self.layer4(x)\n",
        "        features.append(x)\n",
        "\n",
        "        tokens = [self.projections[i](feat) for i, feat in enumerate(features)]\n",
        "        token_sequence = torch.stack(tokens, dim=1)\n",
        "\n",
        "        # Apply transformer layers manually\n",
        "        for layer in self.transformer_layers:\n",
        "            token_sequence = layer(token_sequence)\n",
        "\n",
        "        aggregated_vector = token_sequence.mean(dim=1)\n",
        "        logits = self.classifier(aggregated_vector)\n",
        "        return logits\n",
        "\n",
        "def evaluate_model(model, data_loader, criterion, device):\n",
        "    \"\"\"Evaluates the model on a given dataset.\"\"\"\n",
        "    model.eval()\n",
        "    total_loss, correct, total = 0.0, 0, 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in data_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            total_loss += loss.item() * images.size(0)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    avg_loss = total_loss / total\n",
        "    accuracy = 100 * correct / total\n",
        "    return avg_loss, accuracy\n",
        "\n",
        "def train_and_validate(model, train_loader, val_loader, config):\n",
        "    \"\"\"Main training loop with callbacks.\"\"\"\n",
        "    device = config[\"device\"]\n",
        "    model.to(device)\n",
        "\n",
        "    # Set seeds for reproducible training\n",
        "    torch.manual_seed(config[\"split_seed\"])\n",
        "    np.random.seed(config[\"split_seed\"])\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(config[\"split_seed\"])\n",
        "        torch.cuda.manual_seed_all(config[\"split_seed\"])\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=config[\"learning_rate\"], weight_decay=config[\"weight_decay\"])\n",
        "\n",
        "    # Initialize callbacks\n",
        "    callbacks = [\n",
        "        ReduceLROnPlateau(\n",
        "            optimizer=optimizer,\n",
        "            monitor='val_accuracy',\n",
        "            factor=0.2,\n",
        "            patience=3,\n",
        "            min_lr=1e-7,\n",
        "            verbose=1\n",
        "        ),\n",
        "        EarlyStopping(\n",
        "            monitor='val_accuracy',\n",
        "            patience=7,\n",
        "            restore_best_weights=True,\n",
        "            verbose=1\n",
        "        ),\n",
        "        ModelCheckpoint(\n",
        "            filepath=config[\"checkpoint_path\"],\n",
        "            save_best_only=True,\n",
        "            monitor='val_accuracy',\n",
        "            verbose=1\n",
        "        )\n",
        "    ]\n",
        "\n",
        "    print(\"\\n--- Starting Training ---\")\n",
        "    early_stop = False\n",
        "\n",
        "    for epoch in range(config[\"num_epochs\"]):\n",
        "        if early_stop:\n",
        "            break\n",
        "\n",
        "        model.train()\n",
        "        start_time = time.time()\n",
        "        running_loss, train_correct, train_total = 0.0, 0, 0\n",
        "\n",
        "        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{config['num_epochs']} [Train]\", leave=False)\n",
        "        for images, labels in progress_bar:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item() * images.size(0)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            train_total += labels.size(0)\n",
        "            train_correct += (predicted == labels).sum().item()\n",
        "            progress_bar.set_postfix(loss=loss.item())\n",
        "\n",
        "        train_loss = running_loss / train_total\n",
        "        train_acc = 100 * train_correct / train_total\n",
        "        val_loss, val_acc = evaluate_model(model, val_loader, criterion, device)\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{config['num_epochs']} | Time: {time.time() - start_time:.2f}s | \"\n",
        "              f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}% | \"\n",
        "              f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%\")\n",
        "\n",
        "        # Prepare logs for callbacks\n",
        "        logs = {\n",
        "            'train_loss': train_loss,\n",
        "            'train_accuracy': train_acc,\n",
        "            'val_loss': val_loss,\n",
        "            'val_accuracy': val_acc\n",
        "        }\n",
        "\n",
        "        # Execute callbacks\n",
        "        for callback in callbacks:\n",
        "            if isinstance(callback, EarlyStopping):\n",
        "                if callback.on_epoch_end(epoch, logs, model):\n",
        "                    early_stop = True\n",
        "                    break\n",
        "            elif isinstance(callback, ModelCheckpoint):\n",
        "                callback.on_epoch_end(epoch, logs, model)\n",
        "            else:\n",
        "                callback.on_epoch_end(epoch, logs)\n",
        "\n",
        "    # Execute callback cleanup\n",
        "    for callback in callbacks:\n",
        "        if isinstance(callback, EarlyStopping):\n",
        "            callback.on_training_end(model)\n",
        "        else:\n",
        "            callback.on_training_end()\n",
        "\n",
        "    print(\"--- Training Finished ---\\n\")\n",
        "    return model\n",
        "\n",
        "# ==========================================\n",
        "# Evaluation\n",
        "# ==========================================\n",
        "\n",
        "def calculate_top_k_accuracy(outputs, labels, k_values=[1, 3, 5]):\n",
        "    \"\"\"Calculate top-k accuracy for given k values.\"\"\"\n",
        "    batch_size = labels.size(0)\n",
        "    _, pred = outputs.topk(max(k_values), 1, True, True)\n",
        "    pred = pred.t()\n",
        "    correct = pred.eq(labels.view(1, -1).expand_as(pred))\n",
        "\n",
        "    top_k_accuracies = {}\n",
        "    for k in k_values:\n",
        "        correct_k = correct[:k].reshape(-1).float().sum(0, keepdim=True)\n",
        "        top_k_accuracies[k] = correct_k.item() / batch_size\n",
        "\n",
        "    return top_k_accuracies\n",
        "\n",
        "def calculate_entropy(probs):\n",
        "    \"\"\"Calculate entropy of probability distributions.\"\"\"\n",
        "    # Add small epsilon to avoid log(0)\n",
        "    epsilon = 1e-8\n",
        "    probs = probs + epsilon\n",
        "    entropy = -torch.sum(probs * torch.log(probs), dim=1)\n",
        "    return entropy\n",
        "\n",
        "def calculate_ece(confidences, accuracies, n_bins=10):\n",
        "    \"\"\"Calculate Expected Calibration Error.\"\"\"\n",
        "    bin_boundaries = torch.linspace(0, 1, n_bins + 1)\n",
        "    bin_lowers = bin_boundaries[:-1]\n",
        "    bin_uppers = bin_boundaries[1:]\n",
        "\n",
        "    ece = 0\n",
        "    for bin_lower, bin_upper in zip(bin_lowers, bin_uppers):\n",
        "        in_bin = (confidences > bin_lower.item()) & (confidences <= bin_upper.item())\n",
        "        prop_in_bin = in_bin.float().mean()\n",
        "\n",
        "        if prop_in_bin.item() > 0:\n",
        "            accuracy_in_bin = accuracies[in_bin].float().mean()\n",
        "            avg_confidence_in_bin = confidences[in_bin].mean()\n",
        "            ece += torch.abs(avg_confidence_in_bin - accuracy_in_bin) * prop_in_bin\n",
        "\n",
        "    return ece.item()\n",
        "\n",
        "def comprehensive_evaluation(model, test_loader, criterion, device):\n",
        "    \"\"\"Comprehensive evaluation with all requested metrics.\"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    all_predictions = []\n",
        "    all_labels = []\n",
        "    all_confidences = []\n",
        "    all_entropies = []\n",
        "    all_top_k_results = {1: [], 3: [], 5: []}\n",
        "    total_loss = 0.0\n",
        "    total_samples = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in tqdm(test_loader, desc=\"Evaluating\"):\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            # Convert outputs to probabilities\n",
        "            probs = torch.softmax(outputs, dim=1)\n",
        "\n",
        "            # Get predictions and confidences\n",
        "            confidences, predictions = torch.max(probs, dim=1)\n",
        "\n",
        "            # Calculate entropy\n",
        "            entropies = calculate_entropy(probs)\n",
        "\n",
        "            # Calculate top-k accuracies\n",
        "            top_k_accs = calculate_top_k_accuracy(outputs, labels, [1, 3, 5])\n",
        "\n",
        "            # Store results\n",
        "            all_predictions.extend(predictions.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "            all_confidences.extend(confidences.cpu().numpy())\n",
        "            all_entropies.extend(entropies.cpu().numpy())\n",
        "\n",
        "            for k in [1, 3, 5]:\n",
        "                all_top_k_results[k].extend([top_k_accs[k]] * labels.size(0))\n",
        "\n",
        "            total_loss += loss.item() * images.size(0)\n",
        "            total_samples += images.size(0)\n",
        "\n",
        "    # Convert to numpy arrays\n",
        "    all_predictions = np.array(all_predictions)\n",
        "    all_labels = np.array(all_labels)\n",
        "    all_confidences = np.array(all_confidences)\n",
        "    all_entropies = np.array(all_entropies)\n",
        "\n",
        "    # Calculate basic metrics\n",
        "    avg_loss = total_loss / total_samples\n",
        "\n",
        "    # Calculate top-k accuracies\n",
        "    top1 = np.mean([pred == label for pred, label in zip(all_predictions, all_labels)])\n",
        "    top3 = np.mean(all_top_k_results[3])\n",
        "    top5 = np.mean(all_top_k_results[5])\n",
        "\n",
        "    # Calculate precision, recall, f1-score\n",
        "    precision_macro, recall_macro, f1_macro, _ = precision_recall_fscore_support(\n",
        "        all_labels, all_predictions, average='macro', zero_division=0\n",
        "    )\n",
        "    precision_weighted, recall_weighted, f1_weighted, _ = precision_recall_fscore_support(\n",
        "        all_labels, all_predictions, average='weighted', zero_division=0\n",
        "    )\n",
        "\n",
        "    # Calculate confidence and calibration metrics\n",
        "    avg_confidence = np.mean(all_confidences)\n",
        "    std_confidence = np.std(all_confidences)\n",
        "    avg_entropy = np.mean(all_entropies)\n",
        "\n",
        "    # Calculate ECE\n",
        "    accuracies = (all_predictions == all_labels).astype(float)\n",
        "    ece = calculate_ece(torch.tensor(all_confidences), torch.tensor(accuracies))\n",
        "\n",
        "    return {\n",
        "        'test_loss': avg_loss,\n",
        "        'top1': top1,\n",
        "        'top3': top3,\n",
        "        'top5': top5,\n",
        "        'precision_macro': precision_macro,\n",
        "        'recall_macro': recall_macro,\n",
        "        'f1_macro': f1_macro,\n",
        "        'precision_weighted': precision_weighted,\n",
        "        'recall_weighted': recall_weighted,\n",
        "        'f1_weighted': f1_weighted,\n",
        "        'avg_confidence': avg_confidence,\n",
        "        'std_confidence': std_confidence,\n",
        "        'avg_entropy': avg_entropy,\n",
        "        'ece': ece\n",
        "    }\n",
        "\n",
        "# ==========================================\n",
        "# Main Execution\n",
        "# ==========================================\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # Set global seeds for full reproducibility\n",
        "    torch.manual_seed(CONFIG[\"split_seed\"])\n",
        "    np.random.seed(CONFIG[\"split_seed\"])\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(CONFIG[\"split_seed\"])\n",
        "        torch.cuda.manual_seed_all(CONFIG[\"split_seed\"])\n",
        "        # Ensure deterministic behavior on CUDA\n",
        "        torch.backends.cudnn.deterministic = True\n",
        "        torch.backends.cudnn.benchmark = False\n",
        "\n",
        "    print(f\"Using device: {CONFIG['device']}\")\n",
        "    print(f\"Reproducibility seed: {CONFIG['split_seed']}\")\n",
        "\n",
        "    # Get data loaders\n",
        "    dataloaders = get_dataloaders(CONFIG)\n",
        "    if dataloaders is None:\n",
        "        print(\"Could not prepare data. Halting execution.\", file=sys.stderr)\n",
        "        sys.exit(1)  # Exit if data preparation failed\n",
        "\n",
        "    train_loader, val_loader, test_loader = dataloaders\n",
        "\n",
        "    # Initialize and train model\n",
        "    model = ResNetTransformer(\n",
        "        block=NonResidualBlock,\n",
        "        layers=[2, 2, 2, 2],\n",
        "        num_classes=CONFIG[\"num_classes\"],\n",
        "        t_config=TRANSFORMER_CONFIG\n",
        "    )\n",
        "    trained_model = train_and_validate(model, train_loader, val_loader, CONFIG)\n",
        "\n",
        "    # Comprehensive evaluation on the test set\n",
        "    print(\"--- Starting Comprehensive Evaluation on Test Set ---\")\n",
        "    results = comprehensive_evaluation(trained_model, test_loader, nn.CrossEntropyLoss(), CONFIG[\"device\"])\n",
        "\n",
        "    # Print all requested metrics\n",
        "    print(\"\\n--- Top-K Accuracy Results ---\")\n",
        "    print(f\"Top-1 Accuracy: {results['top1'] * 100:.2f}%\")\n",
        "    print(f\"Top-3 Accuracy: {results['top3'] * 100:.2f}%\")\n",
        "    print(f\"Top-5 Accuracy: {results['top5'] * 100:.2f}%\")\n",
        "\n",
        "    print(\"\\n--- Additional Performance Metrics ---\")\n",
        "    print(f\"Macro Average Precision: {results['precision_macro']:.4f}\")\n",
        "    print(f\"Macro Average Recall: {results['recall_macro']:.4f}\")\n",
        "    print(f\"Macro Average F1-Score: {results['f1_macro']:.4f}\")\n",
        "    print(f\"Weighted Average Precision: {results['precision_weighted']:.4f}\")\n",
        "    print(f\"Weighted Average Recall: {results['recall_weighted']:.4f}\")\n",
        "    print(f\"Weighted Average F1-Score: {results['f1_weighted']:.4f}\")\n",
        "\n",
        "    print(\"\\n--- Model Confidence & Calibration Metrics ---\")\n",
        "    print(f\"Average Prediction Confidence: {results['avg_confidence']:.4f}\")\n",
        "    print(f\"Confidence Standard Deviation: {results['std_confidence']:.4f}\")\n",
        "    print(f\"Average Prediction Entropy: {results['avg_entropy']:.4f}\")\n",
        "    print(f\"Expected Calibration Error: {results['ece']:.4f}\")\n",
        "\n",
        "    print(f\"\\nFinal Test Loss: {results['test_loss']:.4f}\")\n",
        "    print(\"===============================================\")"
      ],
      "metadata": {
        "id": "tneDfThPqrV5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e6f4aba-3119-4c1a-ceea-185d118fbed7"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Reproducibility seed: 42\n",
            "Downloading PAD-UFES-20 dataset from Kaggle...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "PAD-UFES-20: 100%|██████████| 3.60G/3.60G [02:46<00:00, 21.6MiB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting...\n",
            "Loading metadata from: ./pad-ufes-20/metadata.csv\n",
            "Total samples: 2298\n",
            "Diagnostic classes: diagnostic\n",
            "BCC    845\n",
            "ACK    730\n",
            "NEV    244\n",
            "SEK    235\n",
            "SCC    192\n",
            "MEL     52\n",
            "Name: count, dtype: int64\n",
            "✅ Dataset verification complete\n",
            "Found 6 classes: ['ACK', 'BCC', 'MEL', 'NEV', 'SCC', 'SEK']\n",
            "Dataset splits - Train: 1608, Val: 344, Test: 346, Total: 2298\n",
            "Found 6 classes: ['ACK', 'BCC', 'MEL', 'NEV', 'SCC', 'SEK']\n",
            "Found 6 classes: ['ACK', 'BCC', 'MEL', 'NEV', 'SCC', 'SEK']\n",
            "Found 6 classes: ['ACK', 'BCC', 'MEL', 'NEV', 'SCC', 'SEK']\n",
            "\n",
            "--- Starting Training ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100 | Time: 80.81s | Train Loss: 1.6814, Train Acc: 34.02% | Val Loss: 1.5097, Val Acc: 35.47%\n",
            "\n",
            "Model saved to ./best_model_pad_ufes.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/100 | Time: 80.23s | Train Loss: 1.5323, Train Acc: 36.07% | Val Loss: 1.4779, Val Acc: 33.72%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/100 | Time: 80.09s | Train Loss: 1.5507, Train Acc: 34.08% | Val Loss: 1.4856, Val Acc: 34.30%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4/100 | Time: 78.54s | Train Loss: 1.4979, Train Acc: 38.62% | Val Loss: 1.4487, Val Acc: 34.30%\n",
            "\n",
            "Reducing learning rate from 1.00e-03 to 2.00e-04\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5/100 | Time: 78.14s | Train Loss: 1.4818, Train Acc: 36.32% | Val Loss: 1.4341, Val Acc: 36.63%\n",
            "\n",
            "Model saved to ./best_model_pad_ufes.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6/100 | Time: 78.20s | Train Loss: 1.4515, Train Acc: 39.12% | Val Loss: 1.4280, Val Acc: 40.99%\n",
            "\n",
            "Model saved to ./best_model_pad_ufes.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7/100 | Time: 78.39s | Train Loss: 1.4384, Train Acc: 37.56% | Val Loss: 1.4225, Val Acc: 39.83%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8/100 | Time: 78.41s | Train Loss: 1.4338, Train Acc: 39.93% | Val Loss: 1.3758, Val Acc: 40.99%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9/100 | Time: 77.00s | Train Loss: 1.4260, Train Acc: 40.24% | Val Loss: 1.3882, Val Acc: 43.02%\n",
            "\n",
            "Model saved to ./best_model_pad_ufes.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10/100 | Time: 77.29s | Train Loss: 1.4018, Train Acc: 38.93% | Val Loss: 1.4060, Val Acc: 40.70%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 11/100 | Time: 77.19s | Train Loss: 1.3765, Train Acc: 41.67% | Val Loss: 1.3756, Val Acc: 39.83%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 12/100 | Time: 77.94s | Train Loss: 1.3962, Train Acc: 40.49% | Val Loss: 1.3220, Val Acc: 41.86%\n",
            "\n",
            "Reducing learning rate from 2.00e-04 to 4.00e-05\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 13/100 | Time: 78.84s | Train Loss: 1.3507, Train Acc: 45.02% | Val Loss: 1.3064, Val Acc: 44.77%\n",
            "\n",
            "Model saved to ./best_model_pad_ufes.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 14/100 | Time: 80.43s | Train Loss: 1.3374, Train Acc: 44.78% | Val Loss: 1.2968, Val Acc: 45.93%\n",
            "\n",
            "Model saved to ./best_model_pad_ufes.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 15/100 | Time: 79.68s | Train Loss: 1.3259, Train Acc: 46.02% | Val Loss: 1.3125, Val Acc: 43.60%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 16/100 | Time: 79.21s | Train Loss: 1.3239, Train Acc: 46.02% | Val Loss: 1.3170, Val Acc: 45.64%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 17/100 | Time: 78.74s | Train Loss: 1.3124, Train Acc: 45.83% | Val Loss: 1.2766, Val Acc: 47.09%\n",
            "\n",
            "Model saved to ./best_model_pad_ufes.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 18/100 | Time: 78.53s | Train Loss: 1.3027, Train Acc: 46.95% | Val Loss: 1.2874, Val Acc: 47.97%\n",
            "\n",
            "Model saved to ./best_model_pad_ufes.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 19/100 | Time: 78.44s | Train Loss: 1.2991, Train Acc: 45.77% | Val Loss: 1.2783, Val Acc: 47.38%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 20/100 | Time: 79.51s | Train Loss: 1.3193, Train Acc: 45.77% | Val Loss: 1.2740, Val Acc: 49.71%\n",
            "\n",
            "Model saved to ./best_model_pad_ufes.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 21/100 | Time: 77.97s | Train Loss: 1.2928, Train Acc: 46.83% | Val Loss: 1.2615, Val Acc: 49.71%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 22/100 | Time: 80.13s | Train Loss: 1.2897, Train Acc: 45.40% | Val Loss: 1.2727, Val Acc: 48.26%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 23/100 | Time: 78.33s | Train Loss: 1.3003, Train Acc: 45.71% | Val Loss: 1.2943, Val Acc: 45.35%\n",
            "\n",
            "Reducing learning rate from 4.00e-05 to 8.00e-06\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 24/100 | Time: 78.25s | Train Loss: 1.2738, Train Acc: 49.19% | Val Loss: 1.2720, Val Acc: 49.71%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 25/100 | Time: 78.02s | Train Loss: 1.2863, Train Acc: 47.82% | Val Loss: 1.2654, Val Acc: 52.03%\n",
            "\n",
            "Model saved to ./best_model_pad_ufes.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 26/100 | Time: 77.78s | Train Loss: 1.2813, Train Acc: 48.38% | Val Loss: 1.2646, Val Acc: 52.03%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 27/100 | Time: 77.75s | Train Loss: 1.2676, Train Acc: 47.70% | Val Loss: 1.2543, Val Acc: 49.42%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 28/100 | Time: 77.89s | Train Loss: 1.2707, Train Acc: 47.01% | Val Loss: 1.2594, Val Acc: 51.74%\n",
            "\n",
            "Reducing learning rate from 8.00e-06 to 1.60e-06\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 29/100 | Time: 77.59s | Train Loss: 1.2683, Train Acc: 47.57% | Val Loss: 1.2549, Val Acc: 50.58%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 30/100 | Time: 77.28s | Train Loss: 1.2792, Train Acc: 48.45% | Val Loss: 1.2587, Val Acc: 51.45%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 31/100 | Time: 78.06s | Train Loss: 1.2711, Train Acc: 47.70% | Val Loss: 1.2522, Val Acc: 50.87%\n",
            "\n",
            "Reducing learning rate from 1.60e-06 to 3.20e-07\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 32/100 | Time: 78.72s | Train Loss: 1.2750, Train Acc: 47.76% | Val Loss: 1.2552, Val Acc: 50.58%\n",
            "\n",
            "Early stopping at epoch 32\n",
            "Restored model weights from the end of the best epoch: 25\n",
            "--- Training Finished ---\n",
            "\n",
            "--- Starting Comprehensive Evaluation on Test Set ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating: 100%|██████████| 6/6 [00:12<00:00,  2.01s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Top-K Accuracy Results ---\n",
            "Top-1 Accuracy: 47.69%\n",
            "Top-3 Accuracy: 86.42%\n",
            "Top-5 Accuracy: 98.55%\n",
            "\n",
            "--- Additional Performance Metrics ---\n",
            "Macro Average Precision: 0.4672\n",
            "Macro Average Recall: 0.3132\n",
            "Macro Average F1-Score: 0.3102\n",
            "Weighted Average Precision: 0.4833\n",
            "Weighted Average Recall: 0.4769\n",
            "Weighted Average F1-Score: 0.4309\n",
            "\n",
            "--- Model Confidence & Calibration Metrics ---\n",
            "Average Prediction Confidence: 0.4818\n",
            "Confidence Standard Deviation: 0.0839\n",
            "Average Prediction Entropy: 1.2624\n",
            "Expected Calibration Error: 0.0373\n",
            "\n",
            "Final Test Loss: 1.3184\n",
            "===============================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WrCPgW6PY216"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}